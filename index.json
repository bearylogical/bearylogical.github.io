[{"authors":["admin"],"categories":null,"content":"From Singapore, interests include theatre, dystopian science fiction and the occasional hike. Watch this space for the occasional sharing on my learning!\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"From Singapore, interests include theatre, dystopian science fiction and the occasional hike. Watch this space for the occasional sharing on my learning!","tags":null,"title":"Syamil Maulod","type":"authors"},{"authors":null,"categories":null,"content":"This section contains documentation on some of the projects/scripts that I have worked on.\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/tutorial/","section":"tutorial","summary":"This section contains documentation on some of the projects/scripts that I have worked on.","tags":null,"title":"Overview","type":"docs"},{"authors":[],"categories":["data-science"],"content":" Table of Contents  Data for the Masses: Structured document parsing with Textract on the Singapore Budget 2024  Budget 2024 and its reach The Problem Looking at Public Data - What\u0026rsquo;s Available on the Budget? The Approach The Solution : AWS Textract Result Conclusion  Post-script Disclaimer  References   \nData for the Masses: Structured document parsing with Textract on the Singapore Budget 2024 Budget 2024 and its reach On the 16th of February, Deputy Prime Minister and Minister for Finance Lawrence Wong spoke at length in the Singapore Parliament on the Singapore 2024 Budget. A number of explainers on the budget to detail who gets what and when are readily available 1, 2. At the same time, going into details of the budget would be also beneficial for the more inquisitive individuals who are simply looking to get an insight of the fine print. For example, getting information on SimplyGo debacle with respect to contextualising the costs of the overall Fare Collection and Ticketing system.\nThe Problem While that specific line item can be interesting to some, having to trawl through the reports can be time consuming, especially when looking across a longer time scale. For example, large CAPEX projects take 5-10 years to come to fruition, and tracking such line items can prove to be daunting if one were to look at the overall government expenditure.\nDue to this perceived limitation, I wanted to understand if there was a way in which one can obtain more detailed information about the budget in a more structured approach.\nLooking at Public Data - What\u0026rsquo;s Available on the Budget? In Singapore, (official) structured   public data can be found in a number of key sites. There are two main established sites for data: the first being data.gov.sg which is maintained by Open Government Products Singapore and secondly SingStat which is run by the Department of Statistics Singapore.\nOn the Singapore budget, only top level data is provided - e.g. Overall Expenditure and Revenue. An example of such a dataset is the Government Budget and Fiscal Position, where granularity stops at the Ministerial level and no information of its constitutent expenditure is provided. An example from Data.gov.sg is provided in the screenshot below.\nWhile limiting, as mentioned in the introduction, all is not lost. Line item information is at least published by the Ministry of Finance in PDF-formatted reports similar to a financial report statement. In order to quench my curiosity, another way forward is needed.\nThe Approach In the early stages of this thought exercise, 4 ways of extracting information from the expenditure reports were investigated and evaluated based on 4 aspects (not ranked) : 1. Scalability 2. Maintanability 3. Cost 4. Reliability (Trustworthiness)\n   Solution What Pros Cons     Manual Extraction Crowd source or manually extract information from the reports  Subjective. Time Consuming. Does not scale   Ask ChatGPT Feed the PDF into a Large Language Model (LLM) Conceptually simple Hallucination is possible in the outputs. Can be costly (Cost per token * document length)   Use Public PDF Parsers Use open source parsers (tesseract, tabula, pyPDF) Free. Local Development Possible Not out of the box, configuration necessary   Use a Paid Service Use Document Intelligence Services from Cloud Providers (AWS, GCP, Azure) Production Grade Can be costly. Need to develop in cloud    ChatGPT or more specifically GPT-4 was used in the evaluation and unfortunately did not produce adequately satisfactory results. For example, the JSON output was not easily understandable and there was a risk of model hallucination that would make the data unreliable. While prompt engineering might be useful in ensuring the document is able to be parsed well, it was outside the scope of the investigation. There may be a use case for large scale knowledge management when the LLM can be used to directly query a backend database to produce deeper levels of insight but again, it was out of scope.\nFor manual extraction and open source PDF parsers, it was briefly investigated but did not produce results that were (1) Scalable and (2) Reliable. To this end, the decision was to use a paid solution to parse the reports. While Google and Azure both have document intelligence expertise that can work with extracting information from PDFs, AWS Textract offered the best out-of-the-box functionality for extracting specifically tabular data in a PDF. This would clearly make it much easier for downstream ingestion and analysis tasks.\nThe Solution : AWS Textract From the AWS Textract website\n Amazon Textract is a machine learning (ML) service that automatically extracts text, handwriting, layout elements, and data from scanned documents. It goes beyond simple optical character recognition (OCR) to identify, understand, and extract specific data from documents.\n For simple cases, AWS does offer a user interface to parse documents of less than a few pages (maximum 150 in one batch). However, I needed to process the financial reports across long time horizons and using a web interface would not suit my needs. Thus in my usecase, it was needed to also include other AWS related components such as object storage (S3), a queue service (SQS) and a notification service in order to receive notifications from the Textract service such that I had a scalable document parsing pipeline. To accomplish this, the system architecture as illustrated below was used:\nIn this approach, the document parsing is done in the following manner:\n The PDF document (i.e. the PDF file containing the expenditure information) is uploaded In doing so, this would place an object in the \u0026ldquo;Processing\u0026rdquo; queue This triggers a lambda function that will call the Textract service The textract service sends a notification to the notification service that contains the status of the service for that particular document The object is then placed in a result queue This triggers another lambda function to turn the results into a JSON The JSON file is then pushed to a seperate output bucket which is then available to the user  Result Using this approach,an example is given for the Ministry of Communications and Information (MCI) expenditure estimate. We start of with a basic PDF table:\nAs you can already see, it has a PDF table that needs some parsing into a structured form. We then use AWS Textract to parse and we obtain a result. We then visualize the output of the textract service using the amazon-textract-textractor python library.4 This overlays the result of the extraction service over the document.\nfrom textractor.entities.document import Document document = Document.open(\u0026quot;PATH_TO_YOUR_JSON\u0026quot;) # This is only possible because we linked each page of a PDF to the page object in the document document.tables[0].visualize()  And we get: The various colors correspond to the Table title, section headers and sub-headers! This is already very useful to start before converting this information into a more understandable dataset :)\nConclusion This is of course just the first big step in turning the information in the PDFs into a well-structured dataset. Next steps include: Defining a result parser to ensure that the section and subsection headers are well accounted for, ingesting the information into a database and so on.\nThe key takeaway from this is that we have a way to deal with unstructured official data and this goes a long way in ensuring that as a whole, availability to less structured forms of data can be made available in the near future to the general public.\nPost-script  The total AWS cost for parsing 3 years worth of budget reports is ~30 USD. The final database will eventually be made available to the public free of charge, but don\u0026rsquo;t ask when\u0026hellip;.  Disclaimer I do not have affliations to AWS or any entities mentioned in this write-up. All attributions are provided at a best-effort basis and the use of any information from this page is at your own risk.\nReferences  https://www.mof.gov.sg/singaporebudget/resources/budget-infographics https://www.straitstimes.com/singapore/government-revenue-in-fy2023-better-than-expected-small-budget-surplus-of-08b-expected-for-fy2024 Structured data is one that can be readily interpreted and reused in other analytical software, this is differentiated from unstructured data which requires additional intervention to make the data usable for downstream analytics and visualization tasks. Note that you do not have to use the architecture to process 1-2 files. You might want to use the AWS console for that  ","date":1708858649,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708865249,"objectID":"8d364f3321701f7b5c5016b6140bb1c2","permalink":"/post/parsing-budgetsg-with-aws-textract/","publishdate":"2024-02-25T11:57:29+01:00","relpermalink":"/post/parsing-budgetsg-with-aws-textract/","section":"post","summary":"Using AWS to programatically ingest and parse tabular data in PDFs","tags":["public-data"],"title":"Parsing the Singapore Budget 2024 with AWS Textract","type":"post"},{"authors":[],"categories":["automation"],"content":" Table of Contents  E-Ink Tags and Home Assistant  System diagram Bill of Materials Software Components Used How to get started Example Node-Red flow Special Thanks   \nE-Ink Tags and Home Assistant In this post i will show you (briefly) how i use a combination of home assistant, node red and other software to get the following result:\nSystem diagram Bill of Materials  Raspberry Pi 5 Starter Kit - 4GB → here (€117.45)\n OpenEPaperLink Mini-AP v3 / zigbee\u0026lt;-\u0026gt;wifi gateway + 5 x 1.54\u0026rdquo; Tags+ 5 x 2.9\u0026rdquo; Tags → here (€111.00)\n  Software Components Used  Raspbian OS Docker NodeRed Home Assistant Open EPaper Link (preferably with an AP already setup - see BoM)  How to get started  Use an existing server or get something which can run a server such as raspberry PI an install an OS (Unix/Linux based preferred) Set up Docker on your server with a container management software (i use Portainer) for your convenience You can follow the tutorial in sequence here to get (1) Docker, (2) Portainer, (3) Home assistant, (4) Node Red setup  https://sequr.be/blog/2022/08/home-assistant-container-part-1-install-debian-docker-and-portainer/ https://sequr.be/blog/2022/09/home-assistant-container-part-2-home-assistant-container/ https://sequr.be/blog/2022/09/home-assistant-container-part-5-node-red  Obtain an API key for Google Maps  Follow the google documentation https://developers.google.com/maps → Note that you need to create a developer account with google to get an API key. NB: you might want to scope your key to the Routes API Google offers a 200USD credit on some of their APIs, of which Routes API is one of those. This would mean that you will need to query the API 39,000 times for you to exceed the \u0026ldquo;free\u0026rdquo; tier (or around 1.2k calls a day)  Obtain an API key for the Nederlandse Spoorwagen / Dutch Railways (NS)  You can do so here: https://apiportal.ns.nl/ you must create an account and generate an API key for the Reisinformatie API product  Link Home assistant to your OpenEPaperLink AP using HACS (Home Assistant Community Store)  Follow the instructions here : https://github.com/jonasniesner/open_epaper_link_homeassistant  Ensure that your node-red and home assistant is able to communicate (see 3.) Have fun and start building your own tag displays!  Use the various blocks to come up with your own way of using the API data and transforming the information into something that appears useful on each tag!    Example Node-Red flow I use the flow below to retrieve train information from the NS API and route the information to my tags! This is what i get:\nNB: The graphics are all custom specified, i do not have any plans of releasing a library to aid in this. The possibilities are endless! Have fun putting your own spin on it!\nSpecial Thanks https://sequr.be/ → for getting the software side setup easily! https://github.com/jjwbruijn/OpenEPaperLink → OpenEPaperLink project community https://www.tindie.com/stores/electronics-by-nic/ → Nic for having all the components available, tested and ready for use on Day 1 https://github.com/jonasniesner/open_epaper_link_homeassistant → Home assistant Epaperlink integration by Jonas Niesner https://www.home-assistant.io/ → open source home assistant!\n","date":1704293849,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704293849,"objectID":"595ae4022ade4134923cba86c37eca4b","permalink":"/post/e-ink-display-home-assistant/","publishdate":"2024-01-03T15:57:29+01:00","relpermalink":"/post/e-ink-display-home-assistant/","section":"post","summary":"Using repurposed supermarket tags and home assistant to get cool-looking displays","tags":["home-automation"],"title":"(Repurposed) Supermarket tags and home assistant!","type":"post"},{"authors":[],"categories":["automation"],"content":" Table of Contents  Cat Connectivity Platform  Info   \nCat Connectivity Platform Info What is it? System to retrieve, transform and send data from various sensor modules to target devices for home automation (smart home) purposes. Its no different from any home automation software in the market, i just want to make it personalized for me? why Cat? I like cats.\nWhat kind of functionalities will it have? - Central aggregation of sensor and/or data from various APIs - Control remote sensors via defined triggers - Ability to take in commands from multiple input sources (i.e. Web UI, Mobile UI, telegram, etc.) - Push data to any target device as specified\nCompatible protocols: - ZigBee (3.0) - WiFi (2.4\u0026frasl;5.0 GHz) - Bluetooth - ?\nSpecified targets: - Telegram chat - Display modules - E-ink displays - LCD modules - mobile app - web ui\n","date":1704293849,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704293849,"objectID":"99d3afd0155f10ef64065b1d8505569b","permalink":"/post/ccp-introduction/","publishdate":"2024-01-03T15:57:29+01:00","relpermalink":"/post/ccp-introduction/","section":"post","summary":"Documenting my home automation journey","tags":["home-automation"],"title":"Cat Connectivity Platform - A Primer","type":"post"},{"authors":[],"categories":[],"content":"Table of Contents  Scheduling details   Introduction While working on my SGDI exploratory project , I wanted something more robust over existing methods implemented by SpaceRacoon and HXChua .\nBy robustness, I intended my scraper to do the following:\n Run prediodically on a schedule Multi-threaded / Concurrency  For feature 1, this was necessary to be able to look at the SGDI across time. For example, movement of key senior personnel and reshuffling. While there may be limitations to the method, being able to document some movement would prove to be useful for other reasons not elaborated here.\nFor feature 2, being able to run concurrent scrapers and / or scrape in parallel would be very useful especially when the sitemap of the SGDI would be highly spread out and contain many sub-sites which could take a long duration. Coupled with the first feature, this would allow the scraping framework to be run quicker. A single-threaded scraper can only parse through one ministry at a time, as compared to a multi-threaded scraper which can parse through multiple ministries concurrently.\nWith knowledge of such requirements I narrowed down my choices to two big frameworks. First is the Scrapy library for web scraping. This was intentionally chosen over BeautifulSoup owing to its ability to support processing pipelines, crawl options and overcome some restrictions used to limit web scraping. More importantly, a scrapy task could be run by multiple workers concurrently. This meant that the concurrency requirement could be fulfilled easily without additional Multi-processing implementations which could be tricky.\nThe ability to schedule and monitor the crawling is a common need. While many libraries already offer integration with Scrapy such as ScrapeOps and SpiderMon , I wanted to integrate Scrapy with a larger data pipeline to be able to run other scripts/tasks (which may/may not be related to the webscraping) at the same time.\nFor this reason, I turned to Apache Airflow which is a (very) elaborate workflow scheduler. The webscraping task can be viewed as one workflow, out of others and the notion of task dependencies within workflows would be well documented. By task dependencies, I refer to the sequence of tasks that have to occur as part of the workflow. For example, if Task A precedes Task B and runs concurrently with Task C, Airflow can manage all these while providing very versatile options to retry failed tasks and document their failures.\nOne additional point was that both frameworks are written in Python which already has a rich ecosystem of analytics and data munging tools.\nImplementation In order to ensure that the solution can be run in most environments, containerization was used. A docker-compose file was created to specify the services we would be creating and exposing as web interfaces. The Airflow documentation already has a minimum example for such a use case. I simply added my scrapy service in addition to the one provided by the example.\nFor my scrapy service, I wanted it to be a web service that allows me to interact with my defined scrapy \u0026ldquo;spider\u0026rdquo;. A scrapy spider contains all the code needed to crawl a web target and parse data. I used Scrapyd which is a service daemon to run Scrapy spiders. This allows me to interact with my spider via a HTTP API, specifying stuff such as running it at a specified time, monitoring its progress and seeing what spiders are runnning at any given moment.\nHere is a sample of my docker-compose file:\nservices: sgdi-scraper: build: ./govdb_scraper/ ports: - 6800:6800 healthcheck: test: [\u0026quot;CMD\u0026quot;, \u0026quot;curl\u0026quot;, \u0026quot;--fail\u0026quot;, \u0026quot;http://sgdi-scraper:6800/\u0026quot;] interval: 6000s timeout: 10s retries: 5 ## Airflow stuff removed because its available on their website  Now, some of the airflow services can be overkill if this was a simple scheduler. For example, I could have done the whole scraping task as a cron job. However, I wanted to add other workflows to this project in the future, which would make this redundancy \u0026ldquo;rational\u0026rdquo; (or not?).\nAfter running docker-compose and building the containers, my Airflow and scraping service is finally up!\nScheduling details In Airflow, my scraping task is defined as a Directed Acyclic Graph (DAG) which is Airflow\u0026rsquo;s fancy way of saying a workflow. My DAG is represented in the image below:\n   Four tasks are specified:\n scrapy_sgdi_check_health - Checks whether the Scrapy service is running, as the Scrapy service does abruptly exits sometimes and this check can be used to get the service back up if it is down. scrapy_sgdi_start_scrape - Starts the SGDI spider scrapy_sgdi_check_running - Polls the ScrapyD endpoint to check if the task is actually running. scrapy_sgdi_check_done - Polls the ScrapyD endpoint to check if the task if completed. (Yes we could do it the other way round too.)  As part of the DAG definition, we can also define what to do if the task fails, succeeds as well as its frequency.\nWhen that\u0026rsquo;s all defined, upon running our DAG, we get all these cool status reports out of the box:\n   From the image above, we do see that there are times when the task fails and that\u0026rsquo;s clearly visible. It\u0026rsquo;s extremely helpful especially if you intend to do a \u0026ldquo;run and forget\u0026rdquo; approach. You can revisit the logs and check what and why things failed without writing any additional code!\nConclusion I hope this post is helpful in understanding how to approach the issues around webscraping and offer another take on the problem!\n","date":1647162368,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647162368,"objectID":"f27d04619edaff1a566e0ca95ef20b65","permalink":"/post/sgdi_data/","publishdate":"2022-03-13T10:06:08+01:00","relpermalink":"/post/sgdi_data/","section":"post","summary":"Airflow and Scrapy for feedback-aware scraping","tags":["scraping"],"title":"Automated Web Scraper with Airflow and Scrapy","type":"post"},{"authors":null,"categories":null,"content":"","date":1583412861,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583412861,"objectID":"5f0380ca99deb16da8746c37ca54d43b","permalink":"/project/sgdi-explore/","publishdate":"2020-03-05T20:54:21+08:00","relpermalink":"/project/sgdi-explore/","section":"project","summary":"This is a series of posts to document my exploration into the Singapore Government Directory.","tags":["data visualization","graph visualisation","fun"],"title":"Sgdi Explore","type":"project"},{"authors":[],"categories":["data-science"],"content":" Table of Contents    Introduction Getting Started The Dataset Some Insights  SGDI Data  Distribution of the names in SGDI Dataset by Ministry Distribution of the names in SGDI Dataset by Ministry / Statboard  Comparison of SGDI vs Official Data.gov Data  Distribution of the names in SGDI Dataset by Ministry / Statboard     \nIntroduction The Singapore Government DIrectory is an online directory that facilitates communication between members of the public and the civil service.\nIn short, it is a repository containing a truncated list of names containing appointment positions as well as ministerial departments.\n   Sample screenshot from sgdi with contact information blocked out.   Given that there are approximately 145,000 officers in the Public Service, it would be interesting to visualise the entirety of the Service and the distribution of employees using the SGDI as a proxy.\nGranted, there are plenty of departments that do not have public facing arms (or are hidden under the official secrets act) as well as employees that do not need to be listed. It\u0026rsquo;s still worthwhile to generate conversations on the supposedly massive bureaucracy of the public service.\nGetting Started First, we start every project by looking at how we can acquire the data.\nAs mentioned above, we probably will have to get the data from the SGDI itself through some recursive web crawling. As web crawling on this scale is probably not recommended and may be in violation of the Computer Misuse Act, I\u0026rsquo;ll only touch upon the key outcomes and not elaborate on the how. BUT, an illustration of SGDI, or rather the structure of the government is as follows:\n   The structure of the SGDI   While the SGDI may be governed by restrictive policies, the data we will be using to compare against comes from [data.gov](\u0026lsquo;https://data.gov.sg'). We can do a simply requests loop using Python to extract the necessary data.\nFor completeness, here is a sample code on how to retrieve data from data.gov:\nimport requests uri = 'https://data.gov.sg' resource_start = '/api/action/datastore_search' payload = { 'resource_id' : 'cbcc128f-081d-4a03-8970-9bac1be13a5d' #lookup this id from data.gov } r = requests.get(uri + resource_start, params=payload).json() records = [] while len(r['result']['records']) != 0: records.extend(r['result']['records']) r = requests.get(uri + r['result']['_links']['next']).json()  Basically, this loop does the pagination needed to extract the data we need.\nThe Dataset After crawling SGDI, we have a total of 36391 names across the various stat boards/ministries. We then do some basic data munging to remove the duplicates and clean up the dataset.\nSome Insights After doing so, we start to produce some visualizations using matplotlib to look at how our names are distributed.\nSGDI Data Distribution of the names in SGDI Dataset by Ministry    Distribution of the names in SGDI Dataset by Ministry / Statboard    Comparison of SGDI vs Official Data.gov Data To see how far/near our SGDI dataset is to actual numbers, we compare it against the data.gov 2016 dataset.\nDistribution of the names in SGDI Dataset by Ministry / Statboard    Now, we can see that there are definitely large gaps between the numbers on the SGDI and the ground truth (Data.gov). However, does this mean that the information is useless?\nNot necessarily. The SGDI dataset will typically capture government employees with either a public facing function or in a position of visibility. It can serve as a credentialling tool for employees to verify their identity to literally anyone that requires it.\nAs such, operations-based roles such as front-line medical staff, teachers and military/civil personnel aren\u0026rsquo;t really expected to be on it but will contribute to the official headcount numbers.\nThose in sensitive areas such as the Home Affairs and Defence Ministry are also unlikely to be on it for matters of national security.\nIn the next series of posts, I will be looking at representing the entire SGDI structure in a graph-based network diagram, trying to sieve out the complexity of our government. Furthermore, as we have the names of our public servants, I will attempt to use machine learning methods to label both the ethnicity and gender of the individuals to better understand how our departments are staffed.\nStay tuned!\n","date":1583222249,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583222249,"objectID":"6e23d56ffbc0077cfd6efc6bb866b668","permalink":"/post/sgdi_intro/","publishdate":"2020-03-03T15:57:29+08:00","relpermalink":"/post/sgdi_intro/","section":"post","summary":"Looking at the structure of the Singapore Government","tags":["scraping","public","citizen ds"],"title":"Exploring the SGDI #1","type":"post"},{"authors":null,"categories":null,"content":"What is this? This is a repository of helpful snippets to generate illustrations in python.\nRepo Index  Annotated Heat Maps Polar Plots Redux  Annotated Heatmaps Create annotated heatmaps that have scaled, and unscaled values on each line.\n   Heatmap generated using Seaborn   Sample code here\nif scale: df_copy=df.copy() ss = MinMaxScaler() scaled_features = ss.fit_transform(df) # scaling is done to allow comparisons across features df[df.columns] = scaled_features # Supplementary text in the case where scaling is used sup_text = '\\nValues are normalized based on the minimum/maximum values across the week for each Facility.' # labels in each cell labels = (np.asarray([\u0026quot;Scaled: {0:.2f} \\nOriginal: {1:.0f}\u0026quot;.format(original, scaled) for original, scaled in zip((df.to_numpy()).flatten(), (df_copy.to_numpy()).flatten())] )).reshape(df.shape).T else: sup_text = '' font_title = {'family': 'sans-serif', 'weight': 'bold', 'size': 16, } # We use ax parameter to tell seaborn which subplot to use for this plot fig = plt.figure(figsize=(15,9)) # generate our heatmap ax = sns.heatmap(df.T, cmap=\u0026quot;PuBu\u0026quot;, annot=labels if scale else True, fmt='' if scale else '.0f',cbar=False) ax.tick_params(axis='both', which='both', length=0) ax.set_xlabel('Day of Week') ax.set_ylabel('Facility Name') # Main title ax.text(x=0, y=1.1, s=f'Visualizing Facilities in {monthDict[month]}', fontsize=16, weight='bold', ha='left', va='bottom', transform=ax.transAxes) # Supplementary text just below the main title ax.text(x=0, y=1.05, s='Distribution of Utilization throughout the week (Mondays - Fridays).' + sup_text, fontsize=8, alpha=0.75, ha='left', va='bottom', transform=ax.transAxes) plt.show()  Polar Plots Redux Plotting events across a clock-like dimensional axis (Polar)\n","date":1580688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580688000,"objectID":"619329421b29b63e826b9133ff4bf9eb","permalink":"/tutorial/py-viz/","publishdate":"2020-02-03T00:00:00Z","relpermalink":"/tutorial/py-viz/","section":"tutorial","summary":"What is this? This is a repository of helpful snippets to generate illustrations in python.\nRepo Index  Annotated Heat Maps Polar Plots Redux  Annotated Heatmaps Create annotated heatmaps that have scaled, and unscaled values on each line.\n   Heatmap generated using Seaborn   Sample code here\nif scale: df_copy=df.copy() ss = MinMaxScaler() scaled_features = ss.fit_transform(df) # scaling is done to allow comparisons across features df[df.columns] = scaled_features # Supplementary text in the case where scaling is used sup_text = '\\nValues are normalized based on the minimum/maximum values across the week for each Facility.","tags":null,"title":"Python Visualisations Repo","type":"docs"},{"authors":null,"categories":null,"content":"Carpark Availability Analysis Problem Statement Every Friday in Singapore, close to 260,000 Male Muslims 1 converge on the 75 mosques to perform their weekly obligatory Friday Prayers much like how Christians have their weekend services.\nThe prayers are usually performed during 1-2pm, and in some areas, this can result in congestion as Motorists vie for limited parking spots. This notebook will look at available carpark data of a particular location - Stirling Road - which is situated near the Mujahidin Mosque 2 to understand the congestion timings for preliminary understanding of the congestion.\nOutline In this example, we will look at the following:\n Creating functions in Python Using the Requests Library to call web APIs Reading/Writing data from/to CSVs using Pandas Plotting timeseries data in Pandas Visualising timeseries data using Seaborn and Matplotlib  Import Libraries First, we import libraries needed for this analysis. Below is a table that explains why the libraries are required for our mini project.\n   Library Function     requests Query Data.gov.sg Realtime Carpark API   datetime Create Datetime objects to filter date   json convert request data into json object to apply data transformations   pandas Create DataFrames to store and transform data   numpy required library for Pandas   matplotlib For Data Visualisations    import requests import datetime import json import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns  Get Parking Data To get the parking data, we query the carpark availability from the data.gov.sg api as shown below. The api requires a payload where we input in a datetime string to get the data we want.\nROOT = 'https://api.data.gov.sg/v1' URI = '/transport/carpark-availability' now = datetime.datetime.now() current_dt = now.isoformat() print(current_dt) payload = {'date_time': current_dt} r = requests.get(ROOT + URI, params=payload)  2019-08-06T00:00:11.741964  In the examples below, we will be largely looking at two key parking locations. You may want to obtain parking locations from here to get the carparks you want.\nQ14 : BLK 49 STIRLING RD Q15 : BLK 41/48 STIRLING RD Q16 : BLK 45 AND 50 STIRLING RD Q17 : BLK 52 STIRLING RD\nCreate function to get data template = pd.DataFrame(columns=['total_lots','lot_type','lots_available','carpark_number','update_time','request_time']) loaded_df = template  def get_cp_data(): ## load file loads = pd.read_csv('data.csv') ROOT = 'https://api.data.gov.sg/v1' URI = '/transport/carpark-availability' now = datetime.datetime.now() current_dt = now.isoformat() payload = {'date_time': current_dt} r = requests.get(ROOT + URI, params=payload) filter_cp = ['Q14', 'Q15', 'Q16', 'Q17'] data = json.loads(r.text)['items'][0]['carpark_data'] filtered_data = [carpark for carpark in data if carpark['carpark_number'] in filter_cp] template_dict = { 'total_lots' : 9999, 'lot_type' : 'NA', 'lots_available' : 9999, 'carpark_number' : 'NA', 'update_time' : 'NA', 'request_time' : 'NA' } for item in filtered_data: for info in item['carpark_info']: template_dict['total_lots'] = info['total_lots'] template_dict['lot_type'] = info['lot_type'] template_dict['lots_available'] = info['lots_available'] template_dict['carpark_number'] = item['carpark_number'] template_dict['update_time'] = item['update_datetime'] template_dict['request_time'] = current_dt loads = loads.append(template_dict, ignore_index=True) loads.drop_duplicates(['total_lots','lot_type','lots_available','carpark_number','update_time'],inplace=True) loads.to_csv('data.csv',index=False) return loads  a = get_cp_data()  Get historical CP Data def get_historical_cp_data(time): ## load file loads = pd.read_csv('parking-data.csv') ROOT = 'https://api.data.gov.sg/v1' URI = '/transport/carpark-availability' payload = {'date_time': time} r = requests.get(ROOT + URI, params=payload) filter_cp = ['Q14', 'Q15', 'Q16', 'Q17'] data = json.loads(r.text)['items'][0]['carpark_data'] filtered_data = [carpark for carpark in data if carpark['carpark_number'] in filter_cp] template_dict = { 'total_lots' : 9999, 'lot_type' : 'NA', 'lots_available' : 9999, 'carpark_number' : 'NA', 'update_time' : 'NA', 'request_time' : 'NA' } for item in filtered_data: for info in item['carpark_info']: template_dict['total_lots'] = info['total_lots'] template_dict['lot_type'] = info['lot_type'] template_dict['lots_available'] = info['lots_available'] template_dict['carpark_number'] = item['carpark_number'] template_dict['update_time'] = item['update_datetime'] template_dict['request_time'] = time loads = loads.append(template_dict, ignore_index=True) loads.drop_duplicates(['total_lots','lot_type','lots_available','carpark_number','update_time'],inplace=True) loads.to_csv('parking-data.csv',index=False) return loads  Using the get_historical_cp_data() function, it takes in a datetime object and saves the data into a csv file parking-data.csv. We next use a for loop with time_steps of 10 minute intervals using Python\u0026rsquo;s in-built range function to generate equally spaced intervals from 0 and 600minutes which is fed into the datetime.timedelta function with minutes argument.\nnow = datetime.datetime.now() idx = 0 for time_step in range(0,600,10): req_time_f = datetime.datetime.now() - datetime.timedelta(minutes=time_step) req_time = req_time_f.isoformat() if idx % 20: print(f\u0026quot;{idx} : Getting data for {req_time}\u0026quot;) get_historical_cp_data(req_time) idx += 1  1 : Getting data for 2019-07-26T19:41:37.599284 2 : Getting data for 2019-07-26T19:31:38.851572 3 : Getting data for 2019-07-26T19:21:40.103910 4 : Getting data for 2019-07-26T19:11:41.326232 5 : Getting data for 2019-07-26T19:01:42.631916 6 : Getting data for 2019-07-26T18:51:43.925971 7 : Getting data for 2019-07-26T18:41:45.144224 8 : Getting data for 2019-07-26T18:31:46.334172 9 : Getting data for 2019-07-26T18:21:47.747885  Reading in File hist_data = pd.read_csv('parking-data.csv')  Visualise Data First, we set request_time as the index of our dataframe. Then, we apply a filter such that we are only interested on the data between 9:15 am to 2.30pm.\nhist_data.set_index('request_time',inplace=True)  hist_data.index = hist_data.index.astype('datetime64[ns]')  filter_time = hist_data.between_time('11:15','15:30').sort_values(by='request_time')  filter_time.head(2)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  total_lots lot_type lots_available carpark_number update_time   request_time          2019-07-26 11:22:39.907290 0 H 0 Q16 2019-07-26T08:34:30   2019-07-26 11:22:39.907290 40 C 25 Q17 2019-07-26T08:34:35     We plot out two plots with two axis as we want to see how closely the update time and request time defers. To recap :\n request_time is the input time that we request the API provides. update_time is the time that the data in the API is recorded in data.gov.sg\u0026rsquo;s database.  We convert out update_time table which is in str format to the datetime format\nfilter_time['update_time'] = filter_time['update_time'].astype('datetime64[ns]') filter_time['request_time'] = filter_time.index  Display the data:\nfilter_time.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  total_lots lot_type lots_available carpark_number update_time request_time   request_time           2019-07-26 11:22:39.907290 0 H 0 Q16 2019-07-26 08:34:30 2019-07-26 11:22:39.907290   2019-07-26 11:22:39.907290 40 C 25 Q17 2019-07-26 08:34:35 2019-07-26 11:22:39.907290   2019-07-26 11:22:39.907290 42 Y 0 Q17 2019-07-26 08:34:35 2019-07-26 11:22:39.907290   2019-07-26 11:22:39.907290 155 Y 0 Q16 2019-07-26 08:34:30 2019-07-26 11:22:39.907290   2019-07-26 11:22:39.907290 40 C 15 Q16 2019-07-26 08:34:30 2019-07-26 11:22:39.907290     #Create a subplot with 2 rows and 1 column fig, ax = plt.subplots(2,1,figsize=(15, 10)) #Create a mask to select only lot Q16 and type 'C' (car) mask = (filter_time['2019-07-26']['carpark_number'] == 'Q16') \u0026amp; (filter_time['2019-07-26']['lot_type'] == 'C') #Plot the first chart ax[0].axvline(x=pd.to_datetime('2019-07-26 12:00'), color='r', linestyle='--',clip_on=False) sns.lineplot(x='update_time',y='lots_available',data=filter_time[mask],ax=ax[0],label='lots_available') sns.lineplot(x='update_time',y='total_lots',data=filter_time[mask],ax=ax[0],label='total_lots') #Plot the second chart ax[1].axvline(x=pd.to_datetime('2019-07-26 12:00'), color='r', linestyle='--',clip_on=False) sns.lineplot(x='request_time',y='lots_available',data=filter_time[mask],ax=ax[1],label='lots_available') sns.lineplot(x='request_time',y='total_lots',data=filter_time[mask],ax=ax[1],label='total_lots')     From the visualisations, if we look at how the two charts overlay on each other, there is a time lag between the update_time and request_time. This means that we generally have to be mindful that the request time and update time don\u0026rsquo;t give the same results. For example, when I request carpark availability at time 12 noon, i will get the data for approximately 9:30AM (first chart).\nAlso, if we look at update time, we see that at approximately 12:30pm, on the first chart, carpark availability drops until after 2pm where it picks up again- which is typically at the end of service and people head back to work.\nSummary In this walk through, we went through a brief yet practical demonstration of why and how to extract data from Data.gov.sg APIs. I hope this will be useful for you! Till next time.\nReferences   Singapore Malay Muslim Community in Figures Booklet 2015, Mendaki Singapore   MUIS Mosque Directory - Mujahidin Mosque   ","date":1564963200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564963200,"objectID":"0cd60c2d5cbeb3b6b161a17fc828b8df","permalink":"/tutorial/api-intro/","publishdate":"2019-08-05T00:00:00Z","relpermalink":"/tutorial/api-intro/","section":"tutorial","summary":"Carpark Availability Analysis Problem Statement Every Friday in Singapore, close to 260,000 Male Muslims 1 converge on the 75 mosques to perform their weekly obligatory Friday Prayers much like how Christians have their weekend services.\nThe prayers are usually performed during 1-2pm, and in some areas, this can result in congestion as Motorists vie for limited parking spots. This notebook will look at available carpark data of a particular location - Stirling Road - which is situated near the Mujahidin Mosque 2 to understand the congestion timings for preliminary understanding of the congestion.","tags":null,"title":"Using GovTech WebAPIs ","type":"docs"},{"authors":[],"categories":["data-science"],"content":" This is the forth of a multi part series that details the processes behind FastParliament . You may view the previous post here :\n Table of Contents  Motivation Quick Recap : The State of Our Corpus Introducing Latent Dirichlet Allocation (LDA) Walkthrough - Applying LDA 1. Import Libraries 2. Create Processing Functions 3. Train LDA Model 4. Coherence Scores and Tuning Visualizing our coherence scores over number of topics 5. Visualising our LDA Model 6. Assigning Topic Terms to Topics 7. Appending our Corpus with Tags! 8. Visualising our Topic Distributions   Motivation While topic modeling is not a visible element behind FastParliament, it allows us to derive insights from our corpus that would otherwise not be apparent.\nFor example, you may be curious to know how discussion topics trend against time, or even ask Is there a special time of the month when parliamentarians discuss on certain key topics?\nNow I hope I sparked something in you to best appreciate why this topic modeling thing is interesting!\nQuick Recap : The State of Our Corpus FastParliament has a document store of around 10,000 documents. Now, that may not be much but imagine if you tasked to \u0026ldquo;find out what parliamentarians talk about\u0026rdquo;.\nHow might you begin? Well, you could be adventurous and read all 10,000 documents, look out for certain keywords and then slowly group those like minded documents together. Or\u0026hellip; you could be like this guy:\n   Laziness is the true mother of all invention   Introducing Latent Dirichlet Allocation (LDA) Taken from wikipedia :\n In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word\u0026rsquo;s presence is attributable to one of the document\u0026rsquo;s topics.\n Let\u0026rsquo;s unpack that.\nFirst, let\u0026rsquo;s define the word topic. A topic is simply a distribution of words. The topic MONEY will likely contain words related to money such as dollars, cents and taxes. Take note that there is no semantical meaning behind the words. Topics are merely probabilistic co-occurances of those words. Also, topic naming in itself is a supervised approach. It is supervised in the sense that an actual human has to step in to give meaning to the distribution of words.\nNext, we generally assume that each document is a mixture of topics and it follows that the distribution of topics follows a probabilistic model.\nBeyond this description, I am not going to delve too much in specifics as the equations that define the probabilistic distribution of topic, words and documents can be somewhat unwieldy.\nWalkthrough - Applying LDA In this section I will be showing how LDA was applied for our corpus. This follows very closely to the tutorials suppled by Gensim.\nAdditionally, we also used pyLDAVis to visualise the topic distribution and also show the histogram of words for a particular topic.\n1. Import Libraries import numpy as np import pandas as pd from datetime import date import matplotlib.pyplot as plt import seaborn as sns import pyLDAvis.gensim import pyLDAvis import pymongo from tqdm import tqdm import gensim from gensim.utils import simple_preprocess from nltk.corpus import stopwords from bson.json_util import dumps  2. Create Processing Functions # Create stopwords stop_words = stopwords.words('english') # Convert Sentences to words def sent_to_words(sentences): for sentence in sentences: yield(gensim.utils.simple_preprocess(str(sentence), deacc=True)) # deacc=True removes punctuations # Remove Stopwords def remove_stopwords(texts): return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts] # Generate Bigrams from text def bigrams(words, bi_min=15, tri_min=10): bigram = gensim.models.Phrases(words, min_count = bi_min) bigram_mod = gensim.models.phrases.Phraser(bigram) return bigram_mod # Combine preprocessing techniques and incorporate bigrams to generate corpus, vocabulary(bigram) and dictionary(id2word). def get_corpus(df): words = list(sent_to_words(df.cleaned_join)) words = remove_stopwords(words) bigram_mod = bigrams(words) bigram = [bigram_mod[review] for review in words] id2word = gensim.corpora.Dictionary(bigram) id2word.filter_extremes(no_below=10, no_above=0.35) id2word.compactify() corpus = [id2word.doc2bow(text) for text in bigram] return corpus, id2word, bigram train_corpus, train_id2word, bigram_train = get_corpus(mongo_df)  3. Train LDA Model We use the ldamulticore function to be able to utilise more cores to generate our model. We then save the model so we can reuse it for future.\nimport warnings import logging # This allows for seeing if the model converges. A log file is created. logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) with warnings.catch_warnings(): warnings.simplefilter('ignore') lda_train = gensim.models.ldamulticore.LdaMulticore( corpus=train_corpus, num_topics=10, id2word=train_id2word, chunksize=100, workers=5, # Num. Processing Cores - 1 passes=50, eval_every = 1, per_word_topics=True) lda_train.save('lda_train.model')  4. Coherence Scores and Tuning While there can be a multitude of ways to assess the coherence of a model, $$C_v$$ is used in FastParliament which is a combination of two other coherence measure. More details can be found here .\nA function compute_coherence_values, which is from this great resource by datascience+ is used for the subsequent evaluation of our coherence scores. It is paired with the CoherenceModel in Gensim to obtain the scores.\nfrom gensim.models import CoherenceModel def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3): \u0026quot;\u0026quot;\u0026quot; Compute c_v coherence for various number of topics Parameters: ---------- dictionary : Gensim dictionary corpus : Gensim corpus texts : List of input texts limit : Max num of topics Returns: ------- model_list : List of LDA topic models coherence_values : Coherence values corresponding to the LDA model with respective number of topics \u0026quot;\u0026quot;\u0026quot; coherence_values = [] model_list = [] for num_topics in range(start, limit, step): model = gensim.models.ldamulticore.LdaMulticore( corpus=corpus, num_topics=num_topics, id2word=train_id2word, chunksize=1000, workers=5, # Num. Processing Cores - 1 passes=50, eval_every = 1, per_word_topics=True) model_list.append(model) coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v') coherence_values.append(coherencemodel.get_coherence()) return model_list, coherence_values  As you can see, this function takes in the scores, at spits out a list of scores with respect to the number of topics defined.\nHowever, it should be noted that each time the model instance is called to generate the LDA model, the values are stochastically determined. As such, it may not be a highly accurate method of accessing coherence, but it largely serves our purpose.\n# run model_list, coherence_values = compute_coherence_values(dictionary=train_id2word, corpus=train_corpus, texts=bigram_train, start=5, limit=40, step=2)  Visualizing our coherence scores over number of topics import matplotlib.pyplot as plt %matplotlib inline # Show graph limit=40; start=5; step=2; x = range(start, limit, step) plt.plot(x, coherence_values) plt.xlabel(\u0026quot;Num Topics\u0026quot;) plt.ylabel(\u0026quot;Coherence score\u0026quot;) plt.legend((\u0026quot;coherence_values\u0026quot;), loc='best') plt.show()  5. Visualising our LDA Model We then use pyLDAVis, which is a great port of the LDAvis library from R, to visualize our topics and the words distribution within the topic.\n## Visualise our model pyLDAvis.enable_notebook(sort=True) vis = pyLDAvis.gensim.prepare(lda_train, train_corpus, train_id2word) pyLDAvis.display(vis)  Due to iframe limitations, I\u0026rsquo;m not able to squeeze it in this post, but you may view the visualisation here . However, here is a screenshot:\n6. Assigning Topic Terms to Topics After that is all said and done, we move on to assigning the terms to each topic.\nIn this iteration of modeling, we print out the top 20 words associated to a topic.\nfor idx, topic in lda_train.print_topics(-1, num_words=20): print(\u0026quot;{}. Topic: {} \\n\\t- Key Words: {}\u0026quot;.format(idx+1, idx, (\u0026quot;,\u0026quot;.join(re.sub(r'\\d.\\d+\\*','',topic).replace('\u0026quot;','').split('+')))))  1. Topic: 0 - Key Words: water , nea , public , residents , town_councils , food , ensure , town_council , ministry , areas , year , use , pub , sir , health , management , buildings , resources , measures , would 2. Topic: 1 - Key Words: new , companies , government , industry , help , services , support , sector , businesses , smes , business , technology , continue , develop , local , example , growth , opportunities , development , need 3. Topic: 2 - Key Words: saf , mas , defence , countries , asean , international , us , security , training , year , financial , foreign , region , ns , mindef , banks , china , financial_institutions , continue , investment 4. Topic: 3 - Key Words: students , education , schools , school , community , moe , children , support , programmes , parents , teachers , sports , learning , programme , work , arts , help , year , training , provide 5. Topic: 4 - Key Words: hdb , lta , new , flats , would , time , residents , public_transport , flat , year , one , housing , road , operators , commuters , transport , system , development , first , sir 6. Topic: 5 - Key Words: police , public , officers , cases , security , safety , home_affairs , number , ministry , community , measures , order , would , crime , act , ensure , take , offence , work , home_team 7. Topic: 6 - Key Words: care , children , support , help , patients , health , family , need , families , singaporeans , healthcare , community , medical , services , parents , elderly , scheme , seniors , provide , child 8. Topic: 7 - Key Words: people , government , one , would , singaporeans , us , think , must , need , even , sir , time , said , want , like , first , know , make , good , society 9. Topic: 8 - Key Words: workers , government , help , companies , work , singaporeans , jobs , budget , employers , would , need , year , one , support , scheme , time , sir , economy , employment , new 10. Topic: 9 - Key Words: bill , act , would , law , case , court , legal , amendments , public , section , cases , sir , make , first , members , time , made , person , one , ensure  Accordingly, we can label the 10 topics as follows:\n Topic: 0 - Environment  Key Words: water , nea , public , residents , town_councils , food , ensure , town_council , ministry , areas   Topic: 1 - Business  Key Words: new , companies , government , industry , help , services , support , sector , businesses , smes   Topic: 2 - External Security  Key Words: saf , mas , defence , countries , asean , international , us , security , training , year   Topic: 3 - Education  Key Words: students , education , schools , school , community , moe , children , support , programmes , parents   Topic: 4 - Living  Key Words: hdb , lta , new , flats , would , time , residents , public_transport , flat , year   Topic: 5 - Internal Security  Key Words: police , public , officers , cases , security , safety , home_affairs , number , ministry , community   Topic: 6 - Healthcare  Key Words: care , children , support , help , patients , health , family , need , families , singaporeans   Topic: 7 - Society  Key Words: people , government , one , would , singaporeans , us , think , must , need , even   Topic: 8 - Employment  Key Words: workers , government , help , companies , work , singaporeans , jobs , budget , employers , would   Topic: 9 - Law  Key Words: bill , act , would , law , case , court , legal , amendments , public , section    7. Appending our Corpus with Tags! Given that we now know what topic numbers refer to what topics, we will then assign a topic to each document. The dominant topic (highest probability) will be assigned.\nFor a particular document :\n## Access document at index 1 and obtain list of tuples where each topic is the index of the topic and the score. row = model[train_corpus][1] ## Sort probabilities in descending order row = sorted(row[0], key=lambda x: x[1], reverse=True)  [(7, 0.43655953), (2, 0.17807838), (0, 0.13052306), (9, 0.09896996), (5, 0.058110144), (1, 0.054801296), (4, 0.042660255)]  We then create a function to show us the score of each dominant topic in a document\ndef format_topics_sentences(ldamodel=model, corpus=train_corpus,texts=bigram_train): # https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/ # Init output sent_topics_df = pd.DataFrame() # Get top 3 topic in each document for i, row in enumerate(ldamodel[corpus]): row = sorted(row[0], key=lambda x: x[1], reverse=True) # Get the Dominant topic, Perc Contribution and Keywords for each document for j, (topic_num, prop_topic) in enumerate(row): if j == 0: # =\u0026gt; dominant topic wp = ldamodel.show_topic(topic_num) topic_keywords = \u0026quot;, \u0026quot;.join([word for word, prop in wp]) sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True) else: break sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords'] # Add original text to the end of the output contents = pd.Series(texts) sent_topics_df = pd.concat([sent_topics_df, contents], axis=1) return(sent_topics_df) df_topic_sents_keywords = format_topics_sentences(ldamodel=model, corpus=train_corpus, texts=bigram_train) # Format df_dominant_topic = df_topic_sents_keywords.reset_index() df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text'] # Show df_dominant_topic.head(10)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Document_No Dominant_Topic Topic_Perc_Contrib Keywords Text     0 0 7.0 0.4911 people, government, one, would, singaporeans, ... [proc_text, debate_resumed, proc_text, br_mr, ...   1 1 7.0 0.4366 people, government, one, would, singaporeans, ... [mr_vikram, nair_asked, minister, foreign_affa...   2 2 5.0 0.3280 police, public, officers, cases, security, saf... [assoc_prof, walter_theseira, asked_minister, ...   3 3 0.0 0.2807 water, nea, public, residents, town_councils, ... [ms_irene, quay_siew, ching, asked_minister, h...   4 4 7.0 0.3995 people, government, one, would, singaporeans, ... [mr_lim, biow_chuan, asked_deputy, prime_minis...   5 5 6.0 0.4204 care, children, support, help, patients, healt... [mr_louis, ng_kok, kwang_asked, minister, heal...   6 6 7.0 0.3054 people, government, one, would, singaporeans, ... [mr_png, eng_huat, asked_deputy, prime_ministe...   7 7 8.0 0.2996 workers, government, help, companies, work, si... [mr_louis, ng_kok, kwang_asked, minister_manpo...   8 8 3.0 0.5370 students, education, schools, school, communit... [ms_irene, quay_siew, ching, asked_minister, e...   9 9 7.0 0.4994 people, government, one, would, singaporeans, ... [proc_text, resumption_debate, question, may, ...     By focusing only on adding the dominant topic to our dataframe, we then create a function called retrieve_dominant_topic.\ndef retrieve_dominant_topic(doc, model=model ,ref_dict=lda_dict, threshold=0.1): doc_bow_sample = ref_dict.doc2bow(doc.split()) #input has to be a list of strings highest_prob = sorted(model.get_document_topics(doc_bow_sample),key=lambda x : x[1], reverse=True)[0][1] if highest_prob \u0026gt; threshold: topic_num = sorted(model.get_document_topics(doc_bow_sample),key=lambda x : x[1], reverse=True)[0][0] else: topic_num = 11 # no topic identidied return topic_num  We then use pandas .map() function to map each topic to each row of our document corpus.\n%time mongo_df['dominant_topic'] = mongo_df.cleaned_join.map(lambda x : retrieve_dominant_topic(x))  By defining the dominant_topic_mappings dictionary, we then map the numbers to the actual strings that define the topic id.\ndominant_topic_mappings = { 0 : 'Environment', 1 : 'Business', 2 : 'External Security', 3 : 'Education', 4 : 'Living', 5 : 'Internal Security', 6 : 'Healthcare', 7 : 'Society', 8 : 'Employment', 9 : 'Law', 11 : 'Unidentified' }  mongo_df['dominant_topic'] = mongo_df['dominant_topic'].map(dominant_topic_mappings)  8. Visualising our Topic Distributions With each document having the dominant_topic label, we then moved to visualise it.\nsns.catplot(x='dominant_topic',kind=\u0026quot;count\u0026quot;,data = mongo_df,height=5, aspect=3 )  Now that we have gotten our topics, we then try to answer our initial question :\nHow have topics changed over time?\nIn the example below, I chose Education as a topic. In this visualisation, we are looking at how the topic trends vary across the months of the year. Accordingly, the intensity of the line hue reflects more recent years\ntopic = \u0026quot;Education\u0026quot; plt.figure(figsize=(20,10)) ax = sns.relplot(x=\u0026quot;month\u0026quot;, y=\u0026quot;topic_count\u0026quot;, hue=\u0026quot;year\u0026quot;, kind = \u0026quot;line\u0026quot;, data=topic_groupings_count[topic_groupings_count['dominant_topic'] == topic],aspect=2) plt.title(topic)  plt.figure(figsize=(20,10)) ax = sns.lineplot(x=\u0026quot;year\u0026quot;, y=\u0026quot;topic_count\u0026quot;, hue=\u0026quot;dominant_topic\u0026quot;, data=topic_groupings_count) plt.setp(ax.collections, alpha=0)  Thanks for following through with this entire post! I hope you came away with more understanding on the capabilities of LDA as well as various visualization methods.\nTill next time!\n","date":1564567963,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564567963,"objectID":"3dcc98a0cd4148cd8738c0ad181f1441","permalink":"/post/capstone-topic-modeling/","publishdate":"2019-07-31T18:12:43+08:00","relpermalink":"/post/capstone-topic-modeling/","section":"post","summary":"Generate Insights and Organise Content","tags":["fastparliament"],"title":"Capstone #4 Topic Modeling","type":"post"},{"authors":[],"categories":["data-science"],"content":"  This is the third of a multi part series that details the processes behind FastParliament. You may view the previous post here:\n Table of Contents    Recap of Vector Representations Enter Word2Vec Doc2Vec : An Extension of Word2Vec Applying Doc2Vec for FastParliament Walkthrough of Doc2Vec Process 1. Import Libraries 2. Read in Data from MongoDB 3. Text Preprocessing 4. Instantiating a Doc2Vec GenSim Instance 5. Build a Vocabulary  6. Save Model  7. Load Model 8. Sample Inference 9. Using it in Production Conclusion   \nFor this post, I will be focusing on the content discovery component of FastParliament.\nIf you have been around FastParliament, you may have observed that each time an article is retrieved from the system, a series of articles are also put forth - which are related to the content of the primary article. Usually, to do so in conventional systems would require the use of \u0026ldquo;tags\u0026rdquo; that article has to allow the system to \u0026ldquo;cough\u0026rdquo; out articles with similar tags.\n   Content Discovery   This works well for a small corpus of say, 50 to at most 200 articles. However, the solution obviously does not scale very well, if not at all. Furthermore, the tagging system can be highly arbitrary, where each person can have their own interpretation of what tags are contained in a particular article.\nRecap of Vector Representations In the previous article, sentence vectors were created out of the TF-IDF method where the vector represented weighted frequencies of a particular word in a text.\nWhile that works for the particular use case, semantics are often lost when we are merely looking at counts of a particular word in the document.\nEnter Word2Vec  Most portions of this description are based on the writeup by Gidi Shperber in which he has a good summary on Doc2Vec.\n Word2Vec is a technique to capture contexts between words. There are two key models in word2vec - namely Continuous Bag of Words(CBoW) and Skip-Gram. For CBoW a sliding window is created around a bunch of words or context with the objective of predicting the probability of the target word. In Skip-Gram, the opposite occurs in which we want to predict the context (\u0026ldquo;surrounding words\u0026rdquo;) given a specific word.\nDepending on the case use, one particular model may be preferred over another.\n   Word2Vec Methods Illustration Obtained from Rakuten   The end result, or representations would be somewhat visualised as below:\n   Semantic Representation from word vectors. Obtained from tensorflow docs.   So, then where does that leave us?\nKnowing that there is a way to represent text as vector representations that is context-aware would mean that we can apply the same concept to our Hansard corpus.\nThis would require converting each document to its vector representation in relation to the entire corpus.\nDoc2Vec : An Extension of Word2Vec One key consideration between the Doc2Vec and Word2Vec is that documents and words don\u0026rsquo;t share the same logical structures. The continuous representation that we see in a line of text may not be the same as a line of documents.\nTo get around this problem, an additional vector is fed into either the Doc2Vec version of either the CBoW or the Skip-Gram method. This additional vector is referred to as the document vector which is essentially the \u0026ldquo;ID\u0026rdquo; of the document.\n   Doc2Vec Methods Illustration Obtained from Rakuten   In this case the CBoW now becomes Document Vector Distributed Bag of Words (DV-DBoW) while the Skip-Gram approach is now called Document Vector Distributed Memory (DV-DM) in which the document vector acts as a \u0026ldquo;memory\u0026rdquo; to assist in identifying the context of the particular word in a document. Essentially, it extends the concept of a particular word beyond the sliding window to between various documents.\nApplying Doc2Vec for FastParliament Now, with these concepts in hand, we look at applying the same approaches to our corpus with the objective of generating document to document recommendations to aid in content discovery.\nWalkthrough of Doc2Vec Process In this example, I will be using gensim\u0026rsquo;s doc2vec generator to generate the vectors. I will also extend this further by showing how I operationalized this on FastParliament.\n1. Import Libraries import gensim from gensim.utils import simple_preprocess import pymongo import pandas as pd import numpy as np from bson import ObjectId from sklearn.model_selection import train_test_split  2. Read in Data from MongoDB client = pymongo.MongoClient(\u0026quot;mongodb://localhost:27017/\u0026quot;) db = client[\u0026quot;parliament\u0026quot;] articles = db[\u0026quot;articles\u0026quot;]  mongo_df = pd.DataFrame.from_records(remote_articles.find()) mongo_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    _id article_text chunks cleaned_join dominant_topic html_clean parliament_num parsed_convo persons_involved session_num session_type sitting_date sitting_num src_url title volume_num     0 5d27eca6172d9aa762d4802f \u0026lt;p\u0026gt;[(proc text) Debate resumed. (proc text)]\u0026lt;/... {\"0\": {\"entity\": \"NA\", \"content\": \"[(proc text... [(proc text) Debate resumed. (proc text)]\u0026lt;br/\u0026gt;... Society [[(proc text) Debate resumed. (proc text)], Mr... 13 [{'content': '[(proc text) Debate resumed. (pr... [Mr Leon Perera, Mr K Shanmugam, Assoc Prof Wa... 2 SECOND READING BILLS 2019-05-08 105 https://sprs.parl.gov.sg/search/sprs3topic?rep... PROTECTION FROM ONLINE FALSEHOODS AND MANIPULA... 94   1 5d27eca6172d9aa762d48030 \u0026lt;p class=\"ql-align-justify\"\u0026gt;4 \u0026lt;strong\u0026gt;Mr Vikra... {\"0\": {\"entity\": \"NA\", \"content\": \"Mr Vikram N... Mr Vikram Nair asked the Minister for Foreign ... Society [Mr Vikram Nair asked the Minister for Foreign... 13 [{'content': 'Mr Vikram Nair asked the Ministe... [Dr Vivian Balakrishnan, The Minister for Fore... 2 ORAL ANSWERS 2019-05-08 105 https://sprs.parl.gov.sg/search/sprs3topic?rep... STATE OF BILATERAL RELATIONS WITH MALAYSIA FOL... 94   2 5d27eca6172d9aa762d48031 \u0026lt;p class=\"ql-align-justify\"\u0026gt;8 \u0026lt;strong\u0026gt;Assoc Pr... {\"0\": {\"entity\": \"NA\", \"content\": \"Assoc Prof ... Assoc Prof Walter Theseira asked the Minister ... Internal Security [Assoc Prof Walter Theseira asked the Minister... 13 [{'content': 'Assoc Prof Walter Theseira asked... [Ms Low Yen Ling, Ms Anthea Ong, Assoc Prof Wa... 2 ORAL ANSWERS 2019-05-08 105 https://sprs.parl.gov.sg/search/sprs3topic?rep... COMPANIES WITH MEASURES TO DEAL WITH WORKPLACE... 94   3 5d27eca6172d9aa762d48032 \u0026lt;p\u0026gt;5 \u0026lt;strong\u0026gt;Ms Irene Quay Siew Ching\u0026lt;/strong\u0026gt;... {\"0\": {\"entity\": \"NA\", \"content\": \"Ms Irene Qu... Ms Irene Quay Siew Ching asked the Minister fo... Environment [Ms Irene Quay Siew Ching asked the Minister f... 13 [{'content': 'Ms Irene Quay Siew Ching asked t... [The Senior Minister of State for Health, Dr L... 2 ORAL ANSWERS 2019-05-08 105 https://sprs.parl.gov.sg/search/sprs3topic?rep... REVIEW OF DRUG TESTING STANDARDS IN SINGAPORE ... 94   4 5d27eca6172d9aa762d48033 \u0026lt;p class=\"ql-align-justify\"\u0026gt;2 \u0026lt;strong\u0026gt;Mr Lim B... {\"0\": {\"entity\": \"NA\", \"content\": \"Mr Lim Biow... Mr Lim Biow Chuan asked the Deputy Prime Minis... Employment [Mr Lim Biow Chuan asked the Deputy Prime Mini... 13 [{'content': 'Mr Lim Biow Chuan asked the Depu... [Ms Indranee Rajah, The Second Minister for Fi... 2 ORAL ANSWERS 2019-05-08 105 https://sprs.parl.gov.sg/search/sprs3topic?rep... LIVING IN PRIVATE PROPERTIES BUT WITH NO DECLA... 94     3. Text Preprocessing In read_corpus we iterate through the dataframe above and then use the TaggedDocument function in Gensim to tag a document ID to each document. We can then use this document ID to retrieve additional metadata from our DB after the process is complete.\ndef read_corpus(series_docs, tokens_only=False): for line in series_docs.itertuples(): if tokens_only: yield simple_preprocess(line.cleaned_join) else: # For training data, add tags yield gensim.models.doc2vec.TaggedDocument(simple_preprocess(line.cleaned_join), tags=[str(line._1)])  Create a list of documents with tags:\ncorpus = list(read_corpus(mongo_df,tokens_only=False))  Display a sample tag:\ncorpus[1].tags  ['5d27eca6172d9aa762d48030']  4. Instantiating a Doc2Vec GenSim Instance The vector_size, min_count and epochs are some hyperparameters that can be tuned down the road. I used this as it gave a relatively quick training time. In general, larger vectors result in larger training times and it exponentially scales.\nmodel = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=3, epochs=100, workers=4)  5. Build a Vocabulary model.build_vocab(corpus)  %time model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)  CPU times: user 20min 22s, sys: 6.98 s, total: 20min 29s Wall time: 6min 42s  6. Save Model model.save('doc2vec')  We delete training data to reduce in-memory usage after training is complete.\nmodel.delete_temporary_training_data()  7. Load Model We can load the model at any time, and this is crucial for deploying it to production.\nmodel = gensim.models.doc2vec.Doc2Vec.load('doc2vec')  8. Sample Inference In this portion, I will demonstrate how to use the inference to be able to generate a list of similar documents which will then be tied in to our production model.\nThe infer_vector method is called on the class model that is an instance of our doc2vec saved model which spits out the vectors.\ninference_1 = model.infer_vector(corpus[291].words)  Using this, we generate a list of tuples with the id at index 0 and the score at index 1.\nresults = model.docvecs.most_similar([inference_1]) display(results)  [('5d27eca6172d9aa762d48152', 0.992645263671875), ('5d27eca6172d9aa762d49ef0', 0.9119903445243835), ('5d27eca6172d9aa762d48b2d', 0.907822847366333), ('5d27eca6172d9aa762d48fc3', 0.8947123289108276), ('5d27eca6172d9aa762d492f7', 0.8905215859413147), ('5d27eca6172d9aa762d49304', 0.8838604688644409), ('5d27eca6172d9aa762d48639', 0.8791929483413696), ('5d27eca6172d9aa762d49673', 0.8630969524383545), ('5d27eca6172d9aa762d49aac', 0.8347380757331848), ('5d27eca6172d9aa762d48b5c', 0.8256769180297852)]  We can see that most_similar returns a list of tuples with the document ID and its respective probability.\nUsing this knowledge, we can use this to retrieve content from our database.\nmongo_df[mongo_df['_id'] == ObjectId('5d27eca6172d9aa762d48152')]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    _id article_text chunks cleaned_join dominant_topic html_clean parliament_num parsed_convo persons_involved session_num session_type sitting_date sitting_num src_url title volume_num     291 5d27eca6172d9aa762d48152 \u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;The Chairman\u0026lt;/strong\u0026gt;: Head P, Mini... {\"0\": {\"entity\": \"The Chairman\", \"content\": \" ... The Chairman: Head P, Ministry of Home Affairs... Internal Security [The Chairman: Head P, Ministry of Home Affair... 13 [{'content': 'The Chairman: Head P, Ministry o... [Mrs Josephine Teo, Mr Yee Chia Hsing, Ms Jess... 2 BUDGET 2019-03-01 96 https://sprs.parl.gov.sg/search/sprs3topic?rep... COMMITTEE OF SUPPLY - HEAD P (MINISTRY OF HOME... 94     9. Using it in Production Following that, for our production model, we can create a function that spits out the top 5 similar documents for any document ID that is fed in.\ndef fetch_recommended_document(document_id,mongo_conn,model,n_results=5): \u0026quot;\u0026quot;\u0026quot; Fetch documents from mongoDB based on inference \u0026quot;\u0026quot;\u0026quot; document = mongo_conn.parliament.articles.find_one({'_id': ObjectId(document_id)}) inference = model.infer_vector(document['cleaned_join'].split()) results = model.docvecs.most_similar([inference]) ids = [] for item in results[:n_results]: ids.append(ObjectId(item[0])) recommends = [] for recommend in mongo_conn.parliament.articles.find({\u0026quot;_id\u0026quot; : {\u0026quot;$in\u0026quot; : ids }}): recommends.append({ \u0026quot;_id\u0026quot; : recommend[\u0026quot;_id\u0026quot;], \u0026quot;title\u0026quot; : recommend[\u0026quot;title\u0026quot;], \u0026quot;sitting_date\u0026quot; : recommend[\u0026quot;sitting_date\u0026quot;], \u0026quot;session_type\u0026quot; : recommend[\u0026quot;session_type\u0026quot;] }) return recommends  A sample call:\nfetch_recommended_document('5d27eca6172d9aa762d48b2d',remote_client,model,6)  [{'_id': ObjectId('5d27eca6172d9aa762d48152'), 'title': 'COMMITTEE OF SUPPLY - HEAD P (MINISTRY OF HOME AFFAIRS)', 'sitting_date': datetime.datetime(2019, 3, 1, 0, 0), 'session_type': 'BUDGET'}, {'_id': ObjectId('5d27eca6172d9aa762d48639'), 'title': 'COMMITTEE OF SUPPLY – HEAD P (MINISTRY OF HOME AFFAIRS)', 'sitting_date': datetime.datetime(2018, 3, 2, 0, 0), 'session_type': 'MOTIONS'}, {'_id': ObjectId('5d27eca6172d9aa762d48b2d'), 'title': 'COMMITTEE OF SUPPLY − HEAD P (MINISTRY OF HOME AFFAIRS)', 'sitting_date': datetime.datetime(2017, 3, 3, 0, 0), 'session_type': 'MOTIONS'}, {'_id': ObjectId('5d27eca6172d9aa762d48fc3'), 'title': 'COMMITTEE OF SUPPLY – HEAD P (MINISTRY OF HOME AFFAIRS)', 'sitting_date': datetime.datetime(2016, 4, 6, 0, 0), 'session_type': 'MOTIONS'}, {'_id': ObjectId('5d27eca6172d9aa762d492f7'), 'title': 'HEAD P – MINISTRY OF HOME AFFAIRS (COMMITTEE OF SUPPLY)', 'sitting_date': datetime.datetime(2015, 3, 6, 0, 0), 'session_type': 'MOTIONS'}, {'_id': ObjectId('5d27eca6172d9aa762d49ef0'), 'title': 'COMMITTEE OF SUPPLY Âˆ’ HEAD P (MINISTRY OF HOME AFFAIRS)', 'sitting_date': datetime.datetime(2012, 3, 1, 0, 0), 'session_type': 'BUDGET'}]  Conclusion While this chapter is pretty short, it covered the concept of vectorization and its applications. As you can see, Doc2Vec is a remarkable tool - especially when pared with recommendation systems such that we can now create semantically aware features that can then be used in classification models.\nFor example, we can pair this with user preferences to generate a predictive model to access how likely a particular product would appeal to a certain user.\nIn the next portion of the writeup, I will be touching upon topic modelling that is being used to generate insights on our existing corpus.\n","date":1564559849,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564559849,"objectID":"95e1cef0733b30dd1805f530e78b3ba4","permalink":"/post/capstone-recommender/","publishdate":"2019-07-31T15:57:29+08:00","relpermalink":"/post/capstone-recommender/","section":"post","summary":"Doc2Vec in the Wild","tags":["fastparliament"],"title":"Capstone #3 Content Discovery System","type":"post"},{"authors":[],"categories":["data-science"],"content":"  This is the second of a multi part series that details the processes behind FastParliament. You may view the previous post here:\n Table of Contents    Introduction to Summarization Deconstructing Extractive Summarization Walkthrough of Extractive Summarization Process Import Libraries Extractive Summarization using TextRank  Step 0 : Import in Sample Text  Step 1 : Extract Sentences from sample Step 2 : Vectorizing our Text Step 3 : Build Similarity Matrix Step 4 : Create Similarity Graph Step 5 : Sorting the Scores Step 6 : Joining our Extracted Summary and Putting it together Enter Gensim Conclusion   \nFor this post, I will be focusing on the summarizer component of FastParliament.\n   Text Summarizer in Fast Parliament   Our Parliamentarians tend to speak a fair bit.\nFrom preliminary exploratory analysis on the Hansard, it is probably safe to assume that there would be more, not less content to come in the future. With larger complexities as well as emerging challenges to the governance of the country, there will surely be plenty of things to talk about in the parliament.\nFurthermore, it is commonly agreed that people have limited attention spans. Matters of civil society are competing on the same playing field as the latest Korean drama or the antics of a certain orange-haired President.\n   With those considerations, being able to reduce contents of a text or summarize such that the reader gets the gist of the debate would clearly be advantageous.\nIntroduction to Summarization As a generality, summarization consists of two core areas:\n Abstractive Summary\nIn abstractive summary, the summarized text often contain paraphrased representations of the original text.\n   Abstractive text    Extractive Summary\nConversely, extractive summary preserves the content of the text by selecting phrases/sentences that best convey the information - focus on brevity.\n   Abstractive text     Thus you may have observed that while those two methods are remarkably similar in supposed meaning, they have distinct flavors of summarization.\nAbstractive summarization - one in which \u0026ldquo;new\u0026rdquo; phrases are being created as a result of semantic parsing of the content, is closely linked to deep learning approaches. Current state of the art abstractive models use a combination of Recurrent Neural Networks (RNNs), Attention Mechanisms, Encoder-Decoder techniques to understand a text AND generate reduced length sentences (summaries) from the long-form text. The generative nature of abstractive summaries generally require large computational resources as well as large datasets to be able to train the generator.\nExtractive summarization, on the other hand, is far easier to deploy. The reason being that it is \u0026ldquo;far easier to select text than it is to generate from scratch\u0026rdquo;. Furthermore, to even create \u0026ldquo;understandable\u0026rdquo; summaries from the hansard debates, it may require computational resource and time beyond what would be alloted for the project.\nAs such, extractive summarization techniques were selected as a core component for FastParliament\u0026rsquo;s Summarizer.\nDeconstructing Extractive Summarization In a nutshell below flow outlines the general workflow of extractive summarization.\nInput DocumentInput DocumentSplit Document into SentencesSplit Document into SentencesPerform PreprocessingPerform PreprocessingGenerate Similarity Scores for Each SentenceGenerate Similarity Scores for Each SentenceCompare ScoresCompare ScoresSelect N sentences with highest scores Select N sentences with highest scores \nWalkthrough of Extractive Summarization Process I will go through a simple conceptualization of one implementation of extractive summarization using the TextRank approach first conceptualised by Rada Mihalcea and Paul Tarau in 2004.\nImport Libraries import re import datetime import numpy as np import pandas as pd from datetime import date ### import pymongo ### NLP import gensim import spacy ### Custom Utils from text_utils.metrics import get_chunks_info, get_metric  Extractive Summarization using TextRank Source : https://www.aclweb.org/anthology/W04-3252\nfrom nltk.corpus import stopwords import matplotlib.pyplot as plt import seaborn as sns import pprint import networkx as nx  Step 0 : Import in Sample Text sample_text = mongo_df.iloc[1300].cleaned_join display(sample_text)  'Mr Lim Biow Chuan asked the Minister for National Development (a) whether a traffic impact assessment had been carried out prior to having the construction staging ground located at Marina East; and (b) when will this temporary staging ground be relocated.\u0026lt;br/\u0026gt;Mr Lawrence Wong: The temporary occupation licence (TOL) for the construction staging ground at Marina East was first issued to LTA in 2014, before activities in the area generated significant traffic impact. In 2016, arising from new TOL applications in the East Coast/Marina Bay area, a joint Traffic Impact Assessment (TIA) was carried out for the TOLs in the East Coast/Marina Bay area, as they share the same access road. The Marina East staging ground is segregated from the existing residential area by East Coast Parkway and East Coast Park. Nevertheless, LTA has adopted measures to minimise dis-amenities to road users in the vicinity. For example, queue bays are provided in the staging ground, and throughput has been enhanced so that heavy vehicles do not overflow into public roads. LTA is also working closely with the developers and contractors in the area to develop localised traffic control plans to improve safety and minimise inconvenience to other road users. These include managing the schedules and routes of heavy vehicles to avoid peak hour traffic and residential areas, where possible. There are also signs to alert motorists to slow down and watch out for heavy vehicles.From a land-use perspective, the Marina East staging ground currently supports LTA’s rail and road infrastructure projects. When these projects are completed, we will review whether to continue the use of this staging ground, in connection with the timing of future development plans for the area. '  Step 1 : Extract Sentences from sample In this step, we will be splitting the text into sentences by using python\u0026rsquo;s .split() method. Additionally, we will be doing some pre-processing as part of the process which includes removing stop words as well as stemming.\nThe function process_text takes in a string of text (or in this case, the document) and then outputs a list of post-processed strings where each item on the list is a processed string.\nFurthermore, we will ignore the text at index 0 as this is typically the question statement and we are only interested in responses.\nstop_words = set(stopwords.words(\u0026quot;english\u0026quot;)) def process_text(text,stopwords = None): text = text.replace('\u0026lt;br/\u0026gt;',' ') split_text = text.split('.') sentences = [sentence for sentence in split_text if len(sentence) \u0026gt; 1] process_sentences = [] ## remove stopwords for sentence in sentences: words = sentence.split() processed_words = [word.lower() for word in words if word not in stopwords] process_sentences.append(\u0026quot; \u0026quot;.join(processed_words)) return process_sentences[1:] processed_text = process_text(sample_text,stop_words) display(processed_text)  ['mr lawrence wong: the temporary occupation licence (tol) construction staging ground marina east first issued lta 2014, activities area generated significant traffic impact', 'in 2016, arising new tol applications east coast/marina bay area, joint traffic impact assessment (tia) carried tols east coast/marina bay area, share access road', 'the marina east staging ground segregated existing residential area east coast parkway east coast park', 'nevertheless, lta adopted measures minimise dis-amenities road users vicinity', 'for example, queue bays provided staging ground, throughput enhanced heavy vehicles overflow public roads', 'lta also working closely developers contractors area develop localised traffic control plans improve safety minimise inconvenience road users', 'these include managing schedules routes heavy vehicles avoid peak hour traffic residential areas, possible', 'there also signs alert motorists slow watch heavy vehicles', 'from land-use perspective, marina east staging ground currently supports lta’s rail road infrastructure projects', 'when projects completed, review whether continue use staging ground, connection timing future development plans area']  Step 2 : Vectorizing our Text In this step, the sentences are then converted to Term-Frequency Inverse Document Frequency (TF-IDF). TF-IDF is used in this case over the Bag of Words (BoW) approach to penalise frequently occuring words within the same text. In plain speak, it\u0026rsquo;s similar to assigning weights over the occurances of a certain word. If the word occurs more frequently, then the value is lower. Likewise, rarely accuring words are then given higher values.\nfrom sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer() sent_vectors = vectorizer.fit_transform(processed_text)  To better understand what is happening below the hood, a dataframe containing the vectors mapped to their associated word is shown below.\npd.DataFrame(sent_vectors.todense().tolist(),columns=vectorizer.get_feature_names())   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    2014 2016 access activities adopted alert also amenities applications area ... traffic use users vehicles vicinity watch when whether wong working     0 0.235868 0.00000 0.00000 0.235868 0.000000 0.000000 0.000000 0.000000 0.00000 0.140063 ... 0.155963 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.235868 0.000000   1 0.000000 0.19567 0.19567 0.000000 0.000000 0.000000 0.000000 0.000000 0.19567 0.232386 ... 0.129383 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000   2 0.000000 0.00000 0.00000 0.000000 0.000000 0.000000 0.000000 0.000000 0.00000 0.160048 ... 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000   3 0.000000 0.00000 0.00000 0.000000 0.346693 0.000000 0.000000 0.346693 0.00000 0.000000 ... 0.000000 0.000000 0.294720 0.000000 0.346693 0.000000 0.000000 0.000000 0.000000 0.000000   4 0.000000 0.00000 0.00000 0.000000 0.000000 0.000000 0.000000 0.000000 0.00000 0.000000 ... 0.000000 0.000000 0.000000 0.216402 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000   5 0.000000 0.00000 0.00000 0.000000 0.000000 0.000000 0.222823 0.000000 0.00000 0.155651 ... 0.173319 0.000000 0.222823 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.262117   6 0.000000 0.00000 0.00000 0.000000 0.000000 0.000000 0.000000 0.000000 0.00000 0.000000 ... 0.188798 0.000000 0.000000 0.212354 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000   7 0.000000 0.00000 0.00000 0.000000 0.000000 0.357395 0.303819 0.000000 0.00000 0.000000 ... 0.000000 0.000000 0.000000 0.265806 0.000000 0.357395 0.000000 0.000000 0.000000 0.000000   8 0.000000 0.00000 0.00000 0.000000 0.000000 0.000000 0.000000 0.000000 0.00000 0.000000 ... 0.000000 0.257492 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000   9 0.000000 0.00000 0.00000 0.000000 0.000000 0.000000 0.000000 0.000000 0.00000 0.169831 ... 0.000000 0.243123 0.000000 0.000000 0.000000 0.000000 0.285997 0.285997 0.000000 0.000000    10 rows × 109 columns\n Step 3 : Build Similarity Matrix In this step, a similarity matrix is created by applying a dot product with the sentence vector and its transpose.\nsim_matrix = np.round(np.dot(sent_vectors, sent_vectors.T).A,3)  Again, we display this matrix as part of a correlation table.\ncorr = pd.DataFrame(columns=range(len(processed_text)),index=range(len(processed_text)),data=sim_matrix) display(corr)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    0 1 2 3 4 5 6 7 8 9     0 1.000 0.200 0.224 0.036 0.048 0.076 0.029 0.000 0.144 0.071   1 0.200 1.000 0.374 0.030 0.000 0.081 0.024 0.000 0.130 0.039   2 0.224 0.374 1.000 0.000 0.055 0.025 0.056 0.000 0.200 0.082   3 0.036 0.030 0.000 1.000 0.000 0.211 0.000 0.000 0.092 0.000   4 0.048 0.000 0.055 0.000 1.000 0.000 0.092 0.115 0.062 0.059   5 0.076 0.081 0.025 0.211 0.000 1.000 0.033 0.068 0.069 0.081   6 0.029 0.024 0.056 0.000 0.092 0.033 1.000 0.113 0.000 0.000   7 0.000 0.000 0.000 0.000 0.115 0.068 0.113 1.000 0.000 0.000   8 0.144 0.130 0.200 0.092 0.062 0.069 0.000 0.000 1.000 0.186   9 0.071 0.039 0.082 0.000 0.059 0.081 0.000 0.000 0.186 1.000     And if we wanted to visualise it using seaborn:\n# Generate a mask for the upper triangle mask = np.zeros_like(corr, dtype=np.bool) mask[np.triu_indices_from(mask)] = True # Set up the matplotlib figure f, ax = plt.subplots(figsize=(8, 8)) # Generate a custom diverging colormap cmap = sns.diverging_palette(220, 10, as_cmap=True) sns.heatmap(corr, mask=mask, cmap=cmap,annot=True, vmax=0.4, center=0, square=True, linewidths=.5, cbar_kws={\u0026quot;shrink\u0026quot;: .5})  We can already see how sentences are similar to each other just by comparing their positions in their table. For example, we see that sentence 1 and 2 are very similar.\nprint(f\u0026quot;Sentence 1 : {processed_text[2]}\u0026quot;) print(f\u0026quot;Sentence 2 : {processed_text[3]}\u0026quot;)  Sentence 1 : the marina east staging ground segregated existing residential area east coast parkway east coast park Sentence 2 : nevertheless, lta adopted measures minimise dis-amenities road users vicinity  Step 4 : Create Similarity Graph Using the TextRank concept, a network graph is created such that each vertex is the sentence represented by its index, and the edges are linked to each other by weights computed by the similarity scores.\n The basic idea implemented by a graph-based ranking model is that of “voting”or“recommendation”. When one vertex links to another one,it is basically casting a vote for that other vertex. The higher the number of votes that are cast for a vertex,the higher the importance of the vertex. Moreover, the importance of the vertex casting the vote determines how important the vote itself is,and this information is also taken into account by the ranking model. Hence,the score associated with a vertex is determined based on the votes that are cast for it,and the score of the vertices casting these votes. (Mihalcea \u0026amp; Tarau, 2004)\n To compute this graph-derived score which is refered to as TextRank - similar to Google\u0026rsquo;s PageRank, we import the popular networkx package. With network x, we can visualise the similarity matrix as a graph. And using this, we can see\nsentence_similarity_graph = nx.from_numpy_array(sim_matrix)  pos = nx.circular_layout(sentence_similarity_graph) plt.figure(figsize=(8,8)) plt.figure(1) nx.draw_circular(sentence_similarity_graph, with_labels = True, node_color=\u0026quot;Orange\u0026quot;) edge_labels = nx.get_edge_attributes(sentence_similarity_graph, 'weight') plt.figure(1) nx.draw_networkx_edge_labels(sentence_similarity_graph,pos=pos, edge_labels=edge_labels) plt.show()  scores = nx.pagerank(sentence_similarity_graph) display(scores)  {0: 0.10594604557676923, 1: 0.10711030046429679, 2: 0.11410080866901795, 3: 0.0893971718098718, 4: 0.09467680498110888, 5: 0.1019626373096041, 6: 0.09218045190228014, 7: 0.09190095545516967, 8: 0.10932632179093772, 9: 0.09339850204094387}  Step 5 : Sorting the Scores Once we have obtained the scores, we then look at picking the top n sentences that have the highest scores to best represent the text. In this case, we select the top 3 sentences.\noriginal_sentences = [sent for sent in sample_text.replace('\u0026lt;br/\u0026gt;',' ').split('. ') if len(sent)\u0026gt;1] sorted_sentences = sorted(((scores[idx], sentence) for idx,sentence in enumerate(original_sentences[1:])), reverse=True) display(sorted_sentences[:3])  [(0.11410080866901795, 'The Marina East staging ground is segregated from the existing residential area by East Coast Parkway and East Coast Park'), (0.10932632179093772, 'When these projects are completed, we will review whether to continue the use of this staging ground, in connection with the timing of future development plans for the area'), (0.10711030046429679, 'In 2016, arising from new TOL applications in the East Coast/Marina Bay area, a joint Traffic Impact Assessment (TIA) was carried out for the TOLs in the East Coast/Marina Bay area, as they share the same access road')]  Step 6 : Joining our Extracted Summary and Putting it together In this step, we will display our Question and Summarised response together to give us a feel of what the end product is.\n\u0026quot;. \u0026quot;.join([sent[1] for sent in sorted_sentences[:3]])  'The Marina East staging ground is segregated from the existing residential area by East Coast Parkway and East Coast Park. When these projects are completed, we will review whether to continue the use of this staging ground, in connection with the timing of future development plans for the area. In 2016, arising from new TOL applications in the East Coast/Marina Bay area, a joint Traffic Impact Assessment (TIA) was carried out for the TOLs in the East Coast/Marina Bay area, as they share the same access road'  print(f\u0026quot;Question : {original_sentences[0]}\u0026quot;) print(f\u0026quot;\\nResponse : {'. '.join([sent[1] for sent in sorted_sentences[0:2]])}.\u0026quot;)  Question : Mr Lim Biow Chuan asked the Minister for National Development (a) whether a traffic impact assessment had been carried out prior to having the construction staging ground located at Marina East; and (b) when will this temporary staging ground be relocated Response : The Marina East staging ground is segregated from the existing residential area by East Coast Parkway and East Coast Park. When these projects are completed, we will review whether to continue the use of this staging ground, in connection with the timing of future development plans for the area.  As you may already see, it is somewhat faithful to the original text. At the same time, it is worth noting that this is a rudimentary implementation of TextRank.\nMany variations of TextRank exist which look at various similarity functions and text vectorisation techniques to improve the summarization capability. One such implementation is by Barrios et al, 2016.\nThis implementation incorporated the following changes to the steps: 1. Choosing Longest Common Substring 2. BM25 Ranking model (variation of TF-IDF model using probabilistic )\nWhich you can read more in the paper linked above.\nConsequentially, this variation is baked into the popular Gensim package. As a result, we can call out this function very easily as you can see below.\nEnter Gensim We can use the summarizer function located in the summarization.summarizer module.\nprint(gensim.summarization.summarizer.summarize(sample_text.replace('\u0026lt;br/\u0026gt;',''), ratio=0.3, word_count=None, split=False))  Mr Lim Biow Chuan asked the Minister for National Development (a) whether a traffic impact assessment had been carried out prior to having the construction staging ground located at Marina East; and (b) when will this temporary staging ground be relocated.Mr Lawrence Wong: The temporary occupation licence (TOL) for the construction staging ground at Marina East was first issued to LTA in 2014, before activities in the area generated significant traffic impact. LTA is also working closely with the developers and contractors in the area to develop localised traffic control plans to improve safety and minimise inconvenience to other road users.  It looks pretty good! This algorithm currently drives the summarizer aspect of FastParliament 😁.\nConclusion To recap, we have gone through briefly the key areas of summarization. Following that, we went in-depth at a particular approach of extractive text summarization in which we used the TextRank method to determine sentence importance. Lastly, I showed a variant of text rank that FastParliament uses.\nFor the next part of the writeup, I will be going through the article to article content discovery mechanism that makes use of Doc2Vec. Stay Tuned!\n","date":1564387169,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564387169,"objectID":"ae69ba0da0066747a39be9a8c0c141e2","permalink":"/post/capstone-summarizer/","publishdate":"2019-07-29T15:59:29+08:00","relpermalink":"/post/capstone-summarizer/","section":"post","summary":"Summarizer methods and techniques","tags":["fastparliament"],"title":"Capstone #2 Summarizer","type":"post"},{"authors":null,"categories":null,"content":"Project Page for Fast Parliament View all related posts that touch on the technical aspects of fastparliament below.\n","date":1564058165,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564058165,"objectID":"13f4b92cb34ac5dba5bcd874075c768d","permalink":"/project/capstone/","publishdate":"2019-07-25T20:36:05+08:00","relpermalink":"/project/capstone/","section":"project","summary":"A quicker way to understand parliamentary debates","tags":[],"title":"FastParliament","type":"project"},{"authors":[],"categories":["data-science"],"content":"  This is the first of a multi part series that details the processes behind FastParliament.\n Table of Contents    Motivation Introduction Data Gathering Data Storage Preliminary Data Cleaning   \nMotivation As part of General Assembly\u0026rsquo;s Data Science Immersive 3 month course that I was enrolled in, each student was tasked to produce a capstone project. In a nutshell, the capstone is meant to showcase the various aspects of the data science process that was taught throughout the 3 month course.\nDuring the inital ideation proccess, I mainly focused on projects that:\n Was relatable to the public Had an interactive element built in  From these two criteria, I came up with two initial ideas.\nThe first idea was a computer vision related project. The broader concept was that I wanted to design a system to identify individuals based on their gait. While I didn\u0026rsquo;t end up going through with this project, I have written a short write-up that documented my time with this project as well as some of key learning.\nThe second idea, which was the one that I went ahead with - was to implement and deploy a summarizer for parliamentary speeches. More importantly, it stemmed from my interest to inject some \u0026lsquo;fresh\u0026rsquo; conversations into the parliamentary debates. Accordingly, I wanted to generate new insights on the unstructured parliamentary corpus.\nIntroduction This post forms the first of a multi-part series that aims to provide you, the reader, with a broader understanding of the various processes and approaches that went into my capstone, FastParliament.\nIdeally, I wanted to make the experience of viewing parliamentary debates more interesting , as well as uncover insights into the documents with Natural Language Processing (NLP) methods. Namely, I found three possible areas of improvement when compared to the existing solution:\n Reducing verbosity of existing debates. Understand how certain topics evolve over time. Find a better way to recommend related documents beyond keyword searches.  Also, I acknowledge that this is not the first time something like this has been done. Hansard Browser by a team of SMU post-grads in 2015 has a similar flavor.\nIn my implementation, I wanted to integrate a summarizer, recommender system as well as topic modeling using more recent technologies such as Text Rank, Doc2Vec and LDA - which I will elaborate in the subsequent posts.\nIn this first post, I will touch on:\n Data Gathering Data Storage Architecture \u0026amp; DB normalisation Preliminary Data Cleaning  Data Gathering As part of the objectives of generating insights into the existing parliamentary debates, data had to be collected from the hansard. Interestingly (or not), the Singapore Hansard does not have an easily callable API to get data. While one can request for documents either by sitting or member of parliament, it made data gathering cumbersome if done manually.\nI then had to use a combination of selenium and requests, to get the data.\n While webscraping can be a relatively harmless activity, do be careful to set reasonable request rates to ensure you don\u0026rsquo;t bring down websites unintentionally!      Interestingly, during the scraping process itself I discovered that there were distinct changes in the way the documents were being served to the end user. There were inconsistencies in the document URL and the HTML formatting of the actual document that was being displayed. While it was not very obvious to the end-user, it made scraping tough as various exception handling methods had to be implemented to ensure that the scraping could continue uninterrupted.\nData Storage Next, I had to figure out a way to store the documents that was being collected by the hansard scraper. While a simple approach was to append a pandas dataframe continuously for each document that was successfully scraped, it may not be a scalable approach for a large document corpus like the hansard. Futhermore, organising the dataframe by metadata may be challenging down the road.\nWith these considerations, I then looked at postgreSQL as a solution to my requirements. Thereafter, I devised a schema (below) that seeks to best capture the essence of how I wanted my documents to be managed.\nAs a generality, since I wanted to capture relationships within each document, as well as tie such relationships in to the actual member of parliament - there had to encompass multiple tables with one to many and many to many relationshps to do so.\n   As you can see above, I applied various database normalisation approaches to ensure that the data didn\u0026rsquo;t look like a giant excel table 🤣.\nOnce the database was created, I used SQLAlchemy as a wrapper to allow the scraper to interface with the database - ensuring that the document insertions also captured the various relationships I highlighted above. Object Relational Mapper (ORMs) such as SQLAlchemy abstract out the grittier aspects of database interactions.\nPreliminary Data Cleaning To narrow the scope of the project, I chose to focus on a ten-year period of 2009 to 2019. Once that was done, I wanted to get a sense of how the meta-data of the Hansard was distributed.\n   As you probably guessed, the data coming out doesn\u0026rsquo;t really look very nice. We see plenty of duplicates and certain formatting quirks. I then applied various cleaning techniques as you an see below.\n# Convert to all upper case df.session_type = df.session_type.map(lambda x : x.upper()) # Shorten any clarification XXXX to CLARIFICATION df.session_type = df.session_type.map(lambda x : 'CLARIFICATION' if re.search('CLARIFICATION',x) else x) # Convert ATBP to 'ASSENTS TO BILLS PASSED' df.session_type = df.session_type.map(lambda x : 'ASSENTS TO BILLS PASSED' if re.search('ATBP',x) else x) # remove \\t\\r\\n df.session_type = df.session_type.map(lambda x : re.sub('\\t|\\r\\n','',x)) # Convert all variations of written answers to WRITTEN ANSWERS df.session_type = df.session_type.map(lambda x : 'WRITTEN ANSWERS' if re.search('WRITTEN ANSWER',x) else x) # Convert all variations of oral answers to ORAL ANSWERS df.session_type = df.session_type.map(lambda x : 'ORAL ANSWERS' if re.search('ORAL ANSWER',x) else x) # Clean 'PRESIDENT'S ADDRESS\u0026quot; df.session_type = df.session_type.map(lambda x : \u0026quot;PRESIDENT'S ADDRESS\u0026quot; if re.search(\u0026quot;PRESIDENT'S ADDRESS\u0026quot;,x) else x) # Clean MINISTERIAL STATEMENT df.session_type = df.session_type.map(lambda x : \u0026quot;MINISTERIAL STATEMENTS\u0026quot; if re.search(\u0026quot;MINISTERIAL STATEMENT\u0026quot;,x) else x) # Clean BILLS INTRODUCED df.session_type = df.session_type.map(lambda x : \u0026quot;(BILL INTRODUCED\u0026quot; if re.search(\u0026quot;(BILL|BILL'S) INTRODUCED\u0026quot;,x) else x) # Clean MOTION df.session_type = df.session_type.map(lambda x : \u0026quot;MOTIONS\u0026quot; if re.search(\u0026quot;MOTION\u0026quot;,x) else x)  What comes out, is much better than before:\n   As that was put out of the way, I then focused on the crux of the project - the actual speech contents. What I did not expect, though was that there were multiple ways that the speeches were not being served to the end-user the same way. It appeared that, as time went by, the content administrators changed some of their html markup.\n   That was annoying, but not difficult to address. I then created a parser to convert the html text to the following:\n HTML markup to plain text Divide the text into chunks (each speaker is a chunk)  The end product was below:\n   It used a combination of the fuzzywuzzy library for string matching as well as a chockful of regex functions to be able to match each chunks to the speaker as well as match it to the appropriate political party or role.\nWith the aspects of data gathering, management and cleaning out of the way, I could then proceed to the next step - text summarization, topic modeling and content based recommender.\n","date":1561386945,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561386945,"objectID":"e06d35d3a50aa257960ab20e39ecced5","permalink":"/post/capstone-introduction/","publishdate":"2019-06-24T22:35:45+08:00","relpermalink":"/post/capstone-introduction/","section":"post","summary":"An Introduction to FastParliament","tags":["fastparliament"],"title":"Capstone #1 Introduction","type":"post"},{"authors":[],"categories":["projects","data-science"],"content":"Being an avid redditor (lurker) myself, I\u0026rsquo;ve always wondered how unique certain subreddits are. For the uninitiated, subreddits are equivalent to sub-topics of a message board. As an example, the r/Singapore subreddit would cover all or most discussions about Singapore and can range from the fascinating to the truly \u0026hellip; strange.\nOn the topic of subreddits, i\u0026rsquo;m a sucker for reading into \u0026lsquo;juicy\u0026rsquo; subreddits that have posts spanning interpersonal relationships. It\u0026rsquo;s not uncommon for a random internet stranget to spill their heart out and treat other strangers as their \u0026lsquo;aunt agony\u0026rsquo;. Interestingly, there exists two similar subreddits relationship and confessions.\nWouldn\u0026rsquo;t it be cool if, on the basis of historical posts, we can develop a method to differentiate between r/relationships or r/confessions subreddit? Well, we can - and it is pretty straightforward!\nUsing common approaches in Natural Language Processing (NLP) - an increasingly popular Data Science topic - this post will go through some of the key steps invovled in this process. More importantly, it is to share an easily generalisable methodology that can be used on other subreddits as well.\nA caveat, however, is that this method is constrained to text-based subreddits which only have text in their posts. Posts with images are not going to be used as it is outside of the scope of this post.\n This post assumes some familiarity with Natural Language Processing. Do continue below if you are already familiar with the topic!   Table of Contents  Importing our libraries Data Acquisition Data Cleaning  Displaying our class balances after dropping the rows:   Class Balance Target Encoding  Cleaning Function   Post Cleaning Data Exploration Data Modeling  Train Test Split Model Playground   Pipeline for Logistic Regression Baseline     Model Evaluation \u0026amp; Summary   Importing our libraries import numpy as np import requests import pandas as pd import time import random import regex as re import matplotlib.pyplot as plt from nltk.corpus import stopwords # Import the stop word list from nltk.stem import WordNetLemmatizer from nltk import word_tokenize from sklearn.metrics import classification_report, roc_curve from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB from sklearn.neighbors import KNeighborsClassifier import warnings from psaw import PushshiftAPI # After the imports warnings.filterwarnings(action='ignore')  Data Acquisition Scrap data using the PushShiftAPI to extract more than 1000 posts per subreddit to overcome Reddit\u0026rsquo;s imposed limitation.\n%time api = PushshiftAPI() confessions = pd.DataFrame(list(api.search_submissions(subreddit='confessions', filter=['author','title','subreddit','selftext'], limit=5000))) relationships = pd.DataFrame(list(api.search_submissions(subreddit='relationships', filter=['author','title','subreddit','selftext'], limit=5000))) # store the scrapped data. confessions.to_csv('./data/confessions.csv') relationships.to_csv('./data/relationships.csv')  Data Cleaning We create a filter_columns function that filters out the title, self text and subreddit name (our target)\nWe use the .count() function in our DataFrame object to understand the class balance of our dataset. Ideally, we want the number of entries of type confessions and/or relationships to be the same.\ndef filter_columns(df): columns_to_retain = ['title','selftext','subreddit','author'] return df[columns_to_retain] df_relationships_clean = filter_columns(df_relationships) df_conf_clean = filter_columns(df_confessions) ` display(df_relationships_clean['title'].count()) display(df_conf_clean['title'].count())  Below is a sample of our data:\ndf_relationships_clean.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  title selftext subreddit author     0 Hi I'm here to find my friends without anybody... NaN relationships 0100100001010000   1 My (M31) mind might be broken when i thi k abo... [removed] relationships obviousThrowaway274   2 How do I (26m) apologize to my ex (25f) in a d... Long story short, we broke up 4 months ago and... relationships Throwitallaway73734   3 Do you believe it's better to solve an argumen... [removed] relationships EvenKealed   4 Am i broken? [removed] relationships obviousThrowaway274     df_conf_clean.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  title selftext subreddit author     0 Thought a girl was giving me a quarter and the... So, this was back in 2nd grade. It's a normal ... confessions jessthatrandomperson   1 How can I enjoy my last few days? I am going to die very soon. \\n\\nI am terrifie... confessions throwaway948118   2 I am a narcissistic asshole and I know it and ... I am basically just a manipulative horrible pe... confessions royjorbison   3 I use Reddit as an audience for my puns I can't go ten sentences without thinking of a... confessions anikdylan27   4 I'm sorry for being an asshole last night To the guy I met last night, who's name escape... confessions roodeeMental     Prior to this, we may wish to remove posts that have \u0026lsquo;Moderator\u0026rsquo; as an author to train our model on more \u0026lsquo;authentic\u0026rsquo; posts.\ndf_relationships_clean.loc[:,'author'] = df_relationships_clean.author.map(lambda x : x.lower()) df_conf_clean.loc[:,'author'] = df_conf_clean.author.map(lambda x : x.lower())  df_relationships_clean = df_relationships_clean[~df_relationships_clean.author.str.contains('moderator')] df_conf_clean = df_conf_clean[~df_conf_clean.author.str.contains('moderator')]  df_relationships_clean.isna().sum()  title 0 selftext 16 subreddit 0 author 0 dtype: int64  df_conf_clean.isna().sum()  title 0 selftext 739 subreddit 0 author 0 dtype: int64  We also observe empty selftext in both subreddits. we shall drop rows with empty selftext.\ndf_relationships_clean = df_relationships_clean.dropna(axis=0) df_conf_clean = df_conf_clean.dropna(axis=0)  Ensure only posts with selftext more than 10 words are selected.\ndf_relationships_clean ['selftext_len'] = df_relationships_clean .selftext.map(lambda x: len(x.split())) df_relationships_clean = df_relationships_clean [df_relationships_clean .selftext_len \u0026gt; 10] df_conf_clean['selftext_len'] = df_conf_clean.selftext.map(lambda x: len(x.split())) df_conf_clean = df_conf_clean[df_conf_clean.selftext_len \u0026gt; 10]  Next, we drop our duplicates:\ndf_relationships_clean.drop_duplicates(inplace=True) df_conf_clean.drop_duplicates(inplace=True)  Displaying our class balances after dropping the rows: display(df_relationships_clean.count()) display(df_conf_clean.count())  For relationships:\ntitle 2925 selftext 2925 subreddit 2925 author 2925 selftext_len 2925 dtype: int64  For confessions:\ntitle 3893 selftext 3893 subreddit 3893 author 3893 selftext_len 3893 dtype: int64  Seeing that a value of 2900 is the limiting number, we randomly select 2900 entries from both sets.\nsubset_relationships_clean = df_relationships_clean.sample(n=2900,random_state=666) subset_conf_clean = df_conf_clean.sample(n=2900,random_state=666)  Class Balance # combine both subsets into a DF df_pre = subset_relationships_clean.append(subset_conf_clean,ignore_index=True) df_pre.subreddit.value_counts(normalize=True)  Target Encoding We then perform an encoding of our target : 1 corresponds to posts of type confessions while 0 corresponds to posts of type relationships.\n# create target class columns 0 = relationships, 1 = confessions - encoding df_pre['label'] = df_pre.subreddit.map({'relationships':0,'confessions':1}).astype('int') df_pre.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  title selftext subreddit author selftext_len label     0 I (F18) am questioning the intentions of a ran... My lovely (quite attractive) new boyfriend (M1... relationships boterbabbelaartje 334 0   1 Jealousy My boyfriend(29m) and I(30f) have been togethe... relationships taramarie87 103 0   2 I [28F] wants sex all the time. I've made this... Lately, I've been wanting more sex. To have se... relationships missionblueberry 236 0   3 I [32m] am having issues with jealousy with my... Hooo boy. Here we go. \\n\\nMy wife and I have b... relationships dcsrm 438 0   4 Is my girlfriend into wedgies? My girlfriend (F 21yrs old) and I (M 32yrs old... relationships davidsardinas36 73 0     Cleaning Function Ensure formatting of text by:\n Converting all to lower cases removing groups of words in parentheses remove line breaks removing special characters  We encapsulate this cleaning into the function clean_text\n# convert the stop words to a set. stops = set(stopwords.words('english')) def clean_text(text): #01 convert titles, selftext into lowercase lower_text = text.lower() #02 remove brackets and parenthesis from the title and selftext. no_br_paret_text = re.sub(r'\\(.+?\\)|\\[.+?\\]',' ',str(lower_text)) #03 remove special characters removed_special = re.sub(r'[^0-9a-zA-Z ]+',' ',str(no_br_paret_text)) #04 remove xamp200b remove_xamp200b = re.sub(r'ampx200b',' ',str(removed_special)) #05 remove digits result = re.sub(r'\\d+', '', remove_xamp200b).split() #06 split into individual words meaningful_words = [w for w in result if not w in stops] #07 Join the words back into one string separated by space, # and return the result. return(\u0026quot; \u0026quot;.join(meaningful_words)) df[['title','selftext']] = df_pre[['title','selftext']].applymap(clean_text) df.head()  A sample of our pre-cleaned data:\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  title selftext subreddit author selftext_len label     0 questioning intentions random new girl asked b... lovely new boyfriend told girl cig outside sch... relationships boterbabbelaartje 334 0   1 jealousy boyfriend together almost years two beautiful ... relationships taramarie87 103 0   2 wants sex time made known whose lately wanting sex sex time bit back story rel... relationships missionblueberry 236 0   3 issues jealousy wife stage acting hooo boy go wife married years coming ups down... relationships dcsrm 438 0   4 girlfriend wedgies girlfriend together months used phone look som... relationships davidsardinas36 73 0     Post Cleaning pd.DataFrame(data=zip(df_pre['selftext'],df['selftext']),columns=['pre','post']).head(5)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  pre post     0 My lovely (quite attractive) new boyfriend (M1... lovely new boyfriend told girl cig outside sch...   1 My boyfriend(29m) and I(30f) have been togethe... boyfriend together almost years two beautiful ...   2 Lately, I've been wanting more sex. To have se... lately wanting sex sex time bit back story rel...   3 Hooo boy. Here we go. \\n\\nMy wife and I have b... hooo boy go wife married years coming ups down...   4 My girlfriend (F 21yrs old) and I (M 32yrs old... girlfriend together months used phone look som...     Data Exploration Split title and self text into two classifiers where the output of title_classifier and self_text classifier would provide indication of which subreddit the posts belong to.\n#split titles, and self text into seperate df df_title = df[['title','label']] df_selftext = df[['selftext','label']]  def get_freq_words(sparse_counts, columns): # X_all is a sparse matrix, so sum() returns a 'matrix' datatype ... # which we then convert into a 1-D ndarray for sorting word_counts = np.asarray(sparse_counts.sum(axis=0)).reshape(-1) # argsort() returns smallest first, so we reverse the result largest_count_indices = word_counts.argsort()[::-1] # pretty-print the results! Remember to always ask whether they make sense ... freq_words = pd.Series(word_counts[largest_count_indices], index=columns[largest_count_indices]) return freq_words  # Let's use the CountVectorizer to count words for us for each class # create mask X_1 = df_selftext[df_selftext['label'] == 1] X_0 = df_selftext[df_selftext['label'] == 0] cvt = CountVectorizer(ngram_range=(1,1),stop_words='english') X_1_all = cvt.fit_transform(X_1['selftext']) X_0_all = cvt.fit_transform(X_0['selftext']) columns_1 = np.array(cvt.get_feature_names()) # ndarray (for indexing below) columns_0 = np.array(cvt.get_feature_names())  freq_words_1 = get_freq_words(X_1_all, columns_1) freq_words_0 = get_freq_words(X_0_all, columns_0)  print('Confessions:') display(freq_words_1[:10]) print(\u0026quot;\\n\u0026quot;) print('Relationships:') display(freq_words_0[:10])  Here are some key words that appear in the Confessions data set - which would mean that the words landlord,jeopardise, etc. would make it more than likely for the post to be of confessions class.\nConfessions: landlord 3063 msg 2325 jeopardise 1979 teachings 1721 eyes 1674 pur 1506 user 1438 overworking 1405 generic 1133 lacking 1109 dtype: int64  Same for relationships:\nRelationships: like 6690 time 4761 know 4694 want 4630 really 4235 feel 4000 relationship 3744 said 3245 things 3070 told 2999 dtype: int64  Data Modeling Train Test Split Here, we start with our model development. Before that, we perform a train/test split to ensure that we can validate our model performance.\nX_text = df_selftext['selftext'] y_text = df_selftext['label'] X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(X_text,y_text,stratify=y_text)  Model Playground We create the class LemmaTokenizer to do both lemmatize each word of each entry. I.e. given a list of words, we lemmatize each word.\nFirstly, we try the Naive Bayes model - MultinomialNB as there are multiple nominal features in the form of the various tokens.\nclassifiers = [] vectorizers = [('cvec', CountVectorizer(stop_words='english',tokenizer=LemmaTokenizer())), ('tfvec', TfidfVectorizer(stop_words='english',tokenizer=LemmaTokenizer()))] for vectorizer in vectorizers: bayes_pipe = Pipeline([ (vectorizer), ('mnb', MultinomialNB()) ]) scores = cross_val_score(bayes_pipe, X_text_train, y_text_train,cv=5,verbose=1) b = bayes_pipe.fit(X_text_train, y_text_train) y_pred = b.predict(X_text_test) print(classification_report(y_text_test, y_pred, target_names=['class 0','class 1'])) print('Cross val score for mnb classifier using {} vectorizer is {}'.format(vectorizer[0],scores)) print('Accuracy score for mnb classifier using {} vectorizer is {}'.format(vectorizer[0],bayes_pipe.score(X_text_test, y_text_test)))   precision recall f1-score support class 0 0.77 0.95 0.85 725 class 1 0.93 0.71 0.81 725 accuracy 0.83 1450 macro avg 0.85 0.83 0.83 1450 weighted avg 0.85 0.83 0.83 1450 Cross val score for mnb classifier using cvec vectorizer is [0.80114943 0.80689655 0.86321839 0.81724138 0.79770115] Accuracy score for mnb classifier using cvec vectorizer is 0.8289655172413793 precision recall f1-score support class 0 0.65 0.99 0.78 725 class 1 0.98 0.46 0.63 725 accuracy 0.73 1450 macro avg 0.82 0.73 0.71 1450 weighted avg 0.82 0.73 0.71 1450 Cross val score for mnb classifier using tfvec vectorizer is [0.71149425 0.70689655 0.74712644 0.73448276 0.7045977 ] Accuracy score for mnb classifier using tfvec vectorizer is 0.7282758620689656  Thus the recall scores for multinomial NB with countvectorizer seems to provide higher recall when compared to the tfidf vectorizer.\nIn the meantime, we create a function to encapsulate our evaluation process such that it returns only the false positive rate and true positive rate with a sklearn processing pipeline.\n# store predicted_proba scores for later evaluation under ROC curve def generate_roc(pipeline): b = pipeline.fit(X_text_train, y_text_train) print(f\u0026quot;Train Score:{round(b.score(X_text_train, y_text_train),2)} / Test Score {round(b.score(X_text_test, y_text_test),2)}\u0026quot;) fpr, tpr, _ = roc_curve(y_text_test, b.predict_proba(X_text_test)[:,1],pos_label=1) return [fpr,tpr]  Rewriting the CountVectorizer Naive Bayes and TF-IDF Naive Bayes into their respective pipelines:\ncv_bayes_pipe = Pipeline([ (vectorizers[0]), ('mnb', MultinomialNB()) ])  tfidf_bayes_pipe = Pipeline([ (vectorizers[1]), ('mnb', MultinomialNB()) ])  Pipeline for Logistic Regression Baseline pipe = Pipeline([ ('cvec', CountVectorizer(stop_words='english',tokenizer=LemmaTokenizer())), ('lr', LogisticRegression(solver='saga',max_iter=300)) ])  Obtain hyperparameters for our vectorizer and logistic regressor. We can use a grid search to find the optimal hyperparameters for our pipelines:\npipe_params = { 'cvec__max_features': [2500, 3000, 3500], 'cvec__ngram_range': [(1,1), (1,2)], 'lr__penalty' : ['elasticnet'], 'lr__C' : np.arange(0.1,1,0.1), 'lr__l1_ratio' : np.arange(0,1.1,0.2) } gs = GridSearchCV(pipe, param_grid=pipe_params, cv=5,verbose=1,n_jobs=-1) gs.fit(X_text_train, y_text_train) print(gs.best_score_)  0.9154022988505747  gs.best_params_  {'cvec__max_features': 2500, 'cvec__ngram_range': (1, 1), 'lr__C': 0.1, 'lr__l1_ratio': 1.0, 'lr__penalty': 'elasticnet'}  The best score for our logistic regression pipeline:\ngs.best_estimator_.score(X_text_test,y_text_test)  0.9186206896551724  Using the hyperparameters:\n# try model on title optimal_pipe = Pipeline([ ('cvec', CountVectorizer(tokenizer=LemmaTokenizer(),max_features=2500,ngram_range=(1,1))), ('lr', LogisticRegression(solver='saga',max_iter=300,C=0.1,l1_ratio=1.0,penalty='elasticnet')) ])  X_title = df_title['title'] y_title = df_title['label']  optimal_pipe.fit(X_text_train, y_text_train)  We try the model on our title dataset to obtain the accuracy of the model to classify the subreddit from titles alone.\ny_logr_pred = optimal_pipe.predict(X_text_test) print(classification_report(y_text_test, y_logr_pred, target_names=['class 0','class 1']))   precision recall f1-score support class 0 0.55 1.00 0.71 725 class 1 0.99 0.18 0.31 725 accuracy 0.59 1450 macro avg 0.77 0.59 0.51 1450 weighted avg 0.77 0.59 0.51 1450  Next, we explore the use tfidfvectorizer instead of countvectorizer to account for document similarity\ntfidf_pipe = Pipeline([ ('tfvec', TfidfVectorizer(stop_words='english',tokenizer=LemmaTokenizer())), ('lr', LogisticRegression(solver='saga',max_iter=300)) ]) tfidf_params = { 'tfvec__max_features': [2500, 3000, 3500], 'tfvec__ngram_range': [(1,1), (1,2)], 'lr__penalty' : ['elasticnet'], 'lr__C' : np.arange(0.1,1,0.1), 'lr__l1_ratio' : np.arange(0,1.1,0.2) } gs = GridSearchCV(tfidf_pipe, param_grid=tfidf_params, cv=3,verbose=1,n_jobs=-1) gs.fit(X_text_train, y_text_train) print(gs.best_score_)  0.9183908045977012  It seems that tfidf vectorizer performs best with the logistic regression model.\ntfidf_best_pipe = Pipeline([ ('tfvec', TfidfVectorizer(max_features=3500,ngram_range=(1,1),stop_words='english',tokenizer=LemmaTokenizer())), ('lr', LogisticRegression(solver='saga',max_iter=300,C=0.9,l1_ratio=1.0,penalty='elasticnet')) ])  # test model against test text data and rest of titles y_text_tfidf_pred = gs.best_estimator_.predict(X_text_test) y_title_tfidf_pred = gs.best_estimator_.predict(X_title) print(\u0026quot;Text Report (results based on test data) \\n\u0026quot; + classification_report(y_text_test, y_text_tfidf_pred, target_names=['class 0','class 1'])) print(\u0026quot;Titles (all titles) Report \\n\u0026quot; + classification_report(y_title, y_title_tfidf_pred, target_names=['class 0','class 1']))  Text Report (results based on test data) precision recall f1-score support class 0 0.93 0.91 0.92 725 class 1 0.91 0.94 0.92 725 accuracy 0.92 1450 macro avg 0.92 0.92 0.92 1450 weighted avg 0.92 0.92 0.92 1450 Titles (all titles) Report precision recall f1-score support class 0 0.92 0.17 0.29 2900 class 1 0.54 0.98 0.70 2900 accuracy 0.58 5800 macro avg 0.73 0.58 0.49 5800 weighted avg 0.73 0.58 0.49 5800  While the optimised model with tfidf vectorizer performs remarkably well with high precision and recall, when used with the titles dataset, we can see that that it is somewhat overfit, unable to classify the titles correctly.\n# look at sample predictions pd.DataFrame(data=zip(X_text_test,y_text_test,y_text_tfidf_pred),columns=['text','actual','predicted']).head(5)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  text actual predicted     0 title says watched porn since got nasty furry ... 1 1   1 understand bad bad read lemon fanfic main vide... 1 1   2 lovely quite attractive boyfriend met girl wee... 0 0   3 dated briefly three months never turned someth... 0 0   4 background dating almost years good many fight... 0 0     Model Evaluation \u0026amp; Summary cv_log_roc = generate_roc(optimal_pipe) tfidf_log_roc = generate_roc(tfidf_best_pipe) cv_nb_roc = generate_roc(cv_bayes_pipe) tfidf_nb_roc = generate_roc(tfidf_bayes_pipe) tfidf_knn_roc = generate_roc(knn_best_pipe)  Train Score:0.59 / Test Score 0.59 Train Score:0.93 / Test Score 0.92 Train Score:0.89 / Test Score 0.83 Train Score:0.81 / Test Score 0.73 Train Score:1.0 / Test Score 0.81  # Evaluation roc_data ={ 'cv_nb' : cv_nb_roc, 'tfidf_nb_roc' : tfidf_nb_roc, 'cv_log_roc' : cv_log_roc, 'tfidf_log_roc' : tfidf_log_roc, 'tfidf_knn_roc' : tfidf_knn_roc }  #### Plot figure plt.figure(1,figsize=(10,5)) plt.plot([0, 1], [0, 1], 'k--') for key,roc in roc_data.items(): plt.plot(roc[0], roc[1], label=key) plt.xlabel('Sensitivity') plt.ylabel('1 - Specificity') plt.title('ROC curve') plt.legend(loc='best') plt.savefig(\u0026quot;./img/roc_curve.png\u0026quot;,dpi=300) plt.show()  The crossvectorizer + logistic regression model seems to perform similar to the tfidf vectorizer and logistic regression model. When looking at the accuracy score of all the models, the tfidf+ logistic regression model performs the best with an accuracy of 92% in terms of predicting if the selftext is either an r/confessions or r/relationships post.\n","date":1561385756,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561385756,"objectID":"a1ebe1b9466a74f44d4afaae5d844774","permalink":"/post/ml-subreddit/","publishdate":"2019-06-24T22:15:56+08:00","relpermalink":"/post/ml-subreddit/","section":"post","summary":"Supervised learning + NLP to create a binary classifier","tags":["NLP"],"title":"Predict the subreddit!","type":"post"},{"authors":null,"categories":null,"content":"","date":1560078352,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560078352,"objectID":"c7cd45aee85bea4fadcef5972c27b4e6","permalink":"/project/tracker/","publishdate":"2019-06-09T19:05:52+08:00","relpermalink":"/project/tracker/","section":"project","summary":"This is a series of posts for an end-to-end approach in capturing movement data and working on geo-spatial visualisations","tags":["GIS","geospatial","vis"],"title":"Movement Tracking","type":"project"}]