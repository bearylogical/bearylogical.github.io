<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data-science | Syamil Maulod</title>
    <link>/categories/data-science/</link>
      <atom:link href="/categories/data-science/index.xml" rel="self" type="application/rss+xml" />
    <description>data-science</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© Syamil 2024</copyright><lastBuildDate>Sun, 25 Feb 2024 11:57:29 +0100</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>data-science</title>
      <link>/categories/data-science/</link>
    </image>
    
    <item>
      <title>Parsing the Singapore Budget 2024 with AWS Textract</title>
      <link>/post/parsing-budgetsg-with-aws-textract/</link>
      <pubDate>Sun, 25 Feb 2024 11:57:29 +0100</pubDate>
      <guid>/post/parsing-budgetsg-with-aws-textract/</guid>
      <description>

&lt;p&gt;&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-for-the-masses-structured-document-parsing-with-textract-on-the-singapore-budget-2024&#34;&gt;Data for the Masses: Structured document parsing with Textract on the Singapore Budget 2024&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#budget-2024-and-its-reach&#34;&gt;Budget 2024 and its reach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-problem&#34;&gt;The Problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#looking-at-public-data-what-s-available-on-the-budget&#34;&gt;Looking at Public Data - What&amp;rsquo;s Available on the Budget?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-approach&#34;&gt;The Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-solution-aws-textract&#34;&gt;The Solution : AWS Textract&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#result&#34;&gt;Result&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#post-script&#34;&gt;Post-script&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#disclaimer&#34;&gt;Disclaimer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;&lt;/p&gt;

&lt;h1 id=&#34;data-for-the-masses-structured-document-parsing-with-textract-on-the-singapore-budget-2024&#34;&gt;Data for the Masses: Structured document parsing with Textract on the Singapore Budget 2024&lt;/h1&gt;

&lt;h2 id=&#34;budget-2024-and-its-reach&#34;&gt;Budget 2024 and its reach&lt;/h2&gt;

&lt;p&gt;On the 16th of February, Deputy Prime Minister and Minister for Finance Lawrence Wong &lt;a href=&#34;https://www.mof.gov.sg/docs/librariesprovider3/budget2024/download/pdf/fy2024_budget_statement.pdf&#34;&gt;spoke at length&lt;/a&gt; in the Singapore Parliament on the Singapore 2024 Budget. A number of explainers on the budget to detail who gets what and when are readily available&lt;sup&gt; 1, 2&lt;/sup&gt;. At the same time, going into &lt;a href=&#34;https://www.mof.gov.sg/singaporebudget/revenue-and-expenditure&#34;&gt;details&lt;/a&gt; of the budget would be also beneficial for the more inquisitive individuals who are simply looking to get an insight of the fine print. For example, getting information on SimplyGo &lt;a href=&#34;https://www.straitstimes.com/singapore/transport/5-things-you-need-to-know-about-the-simplygo-saga-so-far&#34;&gt;debacle&lt;/a&gt; with respect to contextualising the costs of the overall Fare Collection and Ticketing system.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;budget-2024-LTA.png&#34; alt=&#34;Expenditure  on Fare Collection and Ticketing System&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-problem&#34;&gt;The Problem&lt;/h2&gt;

&lt;p&gt;While that specific line item can be interesting to some, having to trawl through the reports can be time consuming, especially when looking across a longer time scale. For example, large CAPEX projects take 5-10 years to come to fruition, and tracking such line items can prove to be daunting if one were to look at the overall government expenditure.&lt;/p&gt;

&lt;p&gt;Due to this perceived limitation, I wanted to understand if there was a way in which one can obtain more detailed information about the budget in a more structured approach.&lt;/p&gt;

&lt;h2 id=&#34;looking-at-public-data-what-s-available-on-the-budget&#34;&gt;Looking at Public Data - What&amp;rsquo;s Available on the Budget?&lt;/h2&gt;

&lt;p&gt;In Singapore, (official) structured &lt;sup&gt; &lt;/sup&gt; public data can be found in a number of key sites. There are two main established sites for data: the first being &lt;a href=&#34;https://beta.data.gov.sg/&#34;&gt;data.gov.sg&lt;/a&gt; which is maintained by Open Government Products Singapore and secondly &lt;a href=&#34;https://www.singstat.gov.sg/&#34;&gt;SingStat&lt;/a&gt; which is run by the Department of Statistics Singapore.&lt;/p&gt;

&lt;p&gt;On the Singapore budget, only top level data is provided - e.g. Overall Expenditure and Revenue. An example of such a dataset is the Government Budget and Fiscal Position, where granularity stops at the Ministerial level and no information of its constitutent expenditure is provided. An example from Data.gov.sg is provided in the screenshot below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;datagov.png&#34; alt=&#34;Data.gov.sg Dataset&#34; /&gt;&lt;/p&gt;

&lt;p&gt;While limiting, as mentioned in the introduction, all is not lost. Line item information is at least published by the Ministry of Finance in PDF-formatted reports similar to a financial report statement. In order to quench my curiosity, another way forward is needed.&lt;/p&gt;

&lt;h2 id=&#34;the-approach&#34;&gt;The Approach&lt;/h2&gt;

&lt;p&gt;In the early stages of this thought exercise, 4 ways of extracting information from the expenditure reports were investigated and evaluated based on 4 aspects (not ranked) : 1. Scalability 2. Maintanability 3. Cost 4. Reliability (Trustworthiness)&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Solution&lt;/th&gt;
&lt;th&gt;What&lt;/th&gt;
&lt;th&gt;Pros&lt;/th&gt;
&lt;th&gt;Cons&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Manual Extraction&lt;/td&gt;
&lt;td&gt;Crowd source or manually extract  information from the reports&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Subjective. Time Consuming. Does not scale&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Ask ChatGPT&lt;/td&gt;
&lt;td&gt;Feed the PDF into a Large Language Model (LLM)&lt;/td&gt;
&lt;td&gt;Conceptually simple&lt;/td&gt;
&lt;td&gt;Hallucination is possible in the outputs. Can be costly (Cost per token * document length)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Use Public PDF Parsers&lt;/td&gt;
&lt;td&gt;Use open source parsers  (tesseract, tabula, pyPDF)&lt;/td&gt;
&lt;td&gt;Free. Local Development Possible&lt;/td&gt;
&lt;td&gt;Not out of the box, configuration necessary&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Use a Paid Service&lt;/td&gt;
&lt;td&gt;Use Document Intelligence Services  from Cloud Providers (AWS, GCP, Azure)&lt;/td&gt;
&lt;td&gt;Production Grade&lt;/td&gt;
&lt;td&gt;Can be costly. Need to develop in cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;ChatGPT or more specifically GPT-4 was used in the evaluation and unfortunately did not produce adequately satisfactory results. For example, the JSON output was not easily understandable and there was a risk of model hallucination that would make the data unreliable. While prompt engineering might be useful in ensuring the document is able to be parsed well, it was outside the scope of the investigation. There may be a use case for large scale knowledge management when the LLM can be used to directly query a backend database to produce deeper levels of insight but again, it was out of scope.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;gpt4.png&#34; alt=&#34;Interacting with GPT4&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For manual extraction and open source PDF parsers, it was briefly investigated but did not produce results that were (1) Scalable and (2) Reliable. To this end, the decision was to use a paid solution to parse the reports. While Google and Azure both have document intelligence expertise that can work with extracting information from PDFs, AWS Textract offered the best out-of-the-box functionality for extracting specifically tabular data in a PDF. This would clearly make it &lt;em&gt;much&lt;/em&gt; easier for downstream ingestion and analysis tasks.&lt;/p&gt;

&lt;h2 id=&#34;the-solution-aws-textract&#34;&gt;The Solution : AWS Textract&lt;/h2&gt;

&lt;p&gt;From the &lt;a href=&#34;https://aws.amazon.com/textract/&#34;&gt;AWS Textract website&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Amazon Textract is a machine learning (ML) service that automatically extracts text, handwriting, layout elements, and data from scanned documents. It goes beyond simple optical character recognition (OCR) to identify, understand, and extract specific data from documents.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For simple cases, AWS does offer a &lt;a href=&#34;https://docs.aws.amazon.com/textract/latest/dg/getting-started.html&#34;&gt;user interface&lt;/a&gt; to parse documents of less than a few pages (maximum 150 in one batch). However, I needed to process the financial reports across long time horizons and using a web interface would not suit my needs. Thus in my usecase, it was needed to also include other AWS related components such as object storage (S3), a queue service (SQS) and a notification service in order to receive notifications from the Textract service such that I had a scalable document parsing pipeline. To accomplish this, the system architecture as illustrated below was used:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;textract_architecture.png&#34; alt=&#34;Textract Architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this approach, the document parsing is done in the following manner:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The PDF document (i.e. the PDF file containing the expenditure information) is uploaded&lt;/li&gt;
&lt;li&gt;In doing so, this would place an object in the &amp;ldquo;Processing&amp;rdquo; queue&lt;/li&gt;
&lt;li&gt;This triggers a lambda function that will call the Textract service&lt;/li&gt;
&lt;li&gt;The textract service sends a notification to the notification service that contains the status of the service for that particular document&lt;/li&gt;
&lt;li&gt;The object is then placed in a result queue&lt;/li&gt;
&lt;li&gt;This triggers another lambda function to turn the results into a JSON&lt;/li&gt;
&lt;li&gt;The JSON file is then pushed to a seperate output bucket which is then available to the user&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;

&lt;p&gt;Using this approach,an example is given for the Ministry of Communications and Information (MCI) expenditure estimate. We start of with a basic PDF table:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;mci.png&#34; alt=&#34;MCI Snapshot&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As you can already see, it has a PDF table that needs some parsing into a structured form. We then use AWS Textract to parse and we obtain a result. We then visualize the output of the textract service using the &lt;code&gt;amazon-textract-textractor&lt;/code&gt; &lt;a href=&#34;https://github.com/aws-samples/amazon-textract-textractor/tree/master&#34;&gt;python library&lt;/a&gt;.&lt;sup&gt;4&lt;/sup&gt; This overlays the result of the extraction service over the document.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from textractor.entities.document import Document

document = Document.open(&amp;quot;PATH_TO_YOUR_JSON&amp;quot;)
# This is only possible because we linked each page of a PDF to the page object in the document
document.tables[0].visualize()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we get:
&lt;img src=&#34;mci_textract.png&#34; alt=&#34;MCI Textract&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The various colors correspond to the Table title, section headers and sub-headers! This is already very useful to start before converting this information into a more understandable dataset :)&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This is of course just the first big step in turning the information in the PDFs into a well-structured dataset. Next steps include: Defining a result parser to ensure that the section and subsection headers are well accounted for, ingesting the information into a database and so on.&lt;/p&gt;

&lt;p&gt;The key takeaway from this is that we have a way to deal with unstructured official data and this goes a long way in ensuring that as a whole, availability to less structured forms of data can be made available in the near future to the general public.&lt;/p&gt;

&lt;h3 id=&#34;post-script&#34;&gt;Post-script&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;The total AWS cost for parsing 3 years worth of budget reports is ~30 USD.&lt;/li&gt;
&lt;li&gt;The final database will eventually be made available to the public free of charge, but don&amp;rsquo;t ask when&amp;hellip;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;disclaimer&#34;&gt;Disclaimer&lt;/h3&gt;

&lt;p&gt;I do not have affliations to AWS or any entities mentioned in this write-up. All attributions are provided at a best-effort basis and the use of any information from this page is at your own risk.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mof.gov.sg/singaporebudget/resources/budget-infographics&#34;&gt;https://www.mof.gov.sg/singaporebudget/resources/budget-infographics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.straitstimes.com/singapore/government-revenue-in-fy2023-better-than-expected-small-budget-surplus-of-08b-expected-for-fy2024&#34;&gt;https://www.straitstimes.com/singapore/government-revenue-in-fy2023-better-than-expected-small-budget-surplus-of-08b-expected-for-fy2024&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Structured data is one that can be readily interpreted and reused in other analytical software, this is differentiated from unstructured data which requires additional intervention to make the data usable for downstream analytics and visualization tasks.&lt;/li&gt;
&lt;li&gt;Note that you &lt;em&gt;do not&lt;/em&gt; have to use the architecture to process 1-2 files. You might want to use the &lt;a href=&#34;https://docs.aws.amazon.com/textract/latest/dg/getting-started.html&#34;&gt;AWS console&lt;/a&gt; for that&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Exploring the SGDI #1</title>
      <link>/post/sgdi_intro/</link>
      <pubDate>Tue, 03 Mar 2020 15:57:29 +0800</pubDate>
      <guid>/post/sgdi_intro/</guid>
      <description>

&lt;p&gt;&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-dataset&#34;&gt;The Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#some-insights&#34;&gt;Some Insights&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#sgdi-data&#34;&gt;SGDI Data&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#distribution-of-the-names-in-sgdi-dataset-by-ministry&#34;&gt;Distribution of the names in SGDI Dataset by Ministry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#distribution-of-the-names-in-sgdi-dataset-by-ministry-statboard&#34;&gt;Distribution of the names in SGDI Dataset by Ministry / Statboard&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comparison-of-sgdi-vs-official-data-gov-data&#34;&gt;Comparison of SGDI vs Official Data.gov Data&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#distribution-of-the-names-in-sgdi-dataset-by-ministry-statboard-1&#34;&gt;Distribution of the names in SGDI Dataset by Ministry / Statboard&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://sgdi.gov.sg&#34;&gt;Singapore Government DIrectory&lt;/a&gt; is an online directory that facilitates communication between members of the public and the civil service.&lt;/p&gt;

&lt;p&gt;In short, it is a repository containing a truncated list of names containing appointment positions as well as ministerial departments.&lt;/p&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/sgdi_intro/sgdi-info_hubf9c3711b7259cff516b1ed067e06dd8_31797_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Sample screenshot from sgdi with contact information blocked out.&#34;&gt;


  &lt;img data-src=&#34;/post/sgdi_intro/sgdi-info_hubf9c3711b7259cff516b1ed067e06dd8_31797_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;673&#34; height=&#34;391&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;em&gt;Sample screenshot from sgdi with contact information blocked out.&lt;/em&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;Given that there are approximately 145,000 officers in the Public Service, it would be interesting to visualise the entirety of the Service and the distribution of employees using the SGDI as a proxy.&lt;/p&gt;

&lt;p&gt;Granted, there are plenty of departments that do not have public facing arms (or are hidden under the official secrets act) as well as employees that do not need to be listed. It&amp;rsquo;s still worthwhile to generate conversations on the supposedly massive bureaucracy of the public service.&lt;/p&gt;

&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;

&lt;p&gt;First, we start every project by looking at how we can acquire the data.&lt;/p&gt;

&lt;p&gt;As mentioned above, we probably will have to get the data from the &lt;a href=&#34;https://sgdi.gov.sg&#34;&gt;SGDI&lt;/a&gt; itself through some recursive web crawling. As web crawling on this scale is probably not recommended and may be in violation of the &lt;a href=&#34;https://sso.agc.gov.sg/Act/CMA1993&#34;&gt;Computer Misuse Act&lt;/a&gt;, I&amp;rsquo;ll only touch upon the key outcomes and not elaborate on the &lt;em&gt;how&lt;/em&gt;. BUT, an illustration of SGDI, or rather the structure of the government is as follows:&lt;/p&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/sgdi_intro/sgdi_structure_hudfcf0bcfb55cbee3207f8f5091d1b8d0_28666_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;The structure of the SGDI&#34;&gt;


  &lt;img data-src=&#34;/post/sgdi_intro/sgdi_structure_hudfcf0bcfb55cbee3207f8f5091d1b8d0_28666_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;711&#34; height=&#34;871&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;em&gt;The structure of the SGDI&lt;/em&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;While the SGDI may be governed by restrictive policies, the data we will be using to compare against comes from [data.gov](&amp;lsquo;&lt;a href=&#34;https://data.gov.sg&#39;&#34;&gt;https://data.gov.sg&#39;&lt;/a&gt;). We can do a simply &lt;code&gt;requests&lt;/code&gt; loop using &lt;code&gt;Python&lt;/code&gt; to extract the necessary data.&lt;/p&gt;

&lt;p&gt;For completeness, here is a sample code on how to retrieve data from data.gov:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests

uri = &#39;https://data.gov.sg&#39;
resource_start = &#39;/api/action/datastore_search&#39;
payload = {
    &#39;resource_id&#39; : &#39;cbcc128f-081d-4a03-8970-9bac1be13a5d&#39; #lookup this id from data.gov
}

r = requests.get(uri + resource_start, params=payload).json()
records = []
while len(r[&#39;result&#39;][&#39;records&#39;]) != 0:
    records.extend(r[&#39;result&#39;][&#39;records&#39;])
    r = requests.get(uri + r[&#39;result&#39;][&#39;_links&#39;][&#39;next&#39;]).json()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Basically, this loop does the pagination needed to extract the data we need.&lt;/p&gt;

&lt;h2 id=&#34;the-dataset&#34;&gt;The Dataset&lt;/h2&gt;

&lt;p&gt;After crawling SGDI, we have a total of &lt;strong&gt;36391&lt;/strong&gt; names across the various stat boards/ministries. We then do some basic data munging to remove the duplicates and clean up the dataset.&lt;/p&gt;

&lt;h2 id=&#34;some-insights&#34;&gt;Some Insights&lt;/h2&gt;

&lt;p&gt;After doing so, we start to produce some visualizations using &lt;code&gt;matplotlib&lt;/code&gt; to look at how our names are distributed.&lt;/p&gt;

&lt;h3 id=&#34;sgdi-data&#34;&gt;SGDI Data&lt;/h3&gt;

&lt;h4 id=&#34;distribution-of-the-names-in-sgdi-dataset-by-ministry&#34;&gt;Distribution of the names in SGDI Dataset by Ministry&lt;/h4&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/sgdi_intro/overall_distribution_sgdi_hubbc85be10975281ab1472f83b6bb7eab_97280_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;/post/sgdi_intro/overall_distribution_sgdi_hubbc85be10975281ab1472f83b6bb7eab_97280_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1457&#34; height=&#34;872&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;


&lt;h4 id=&#34;distribution-of-the-names-in-sgdi-dataset-by-ministry-statboard&#34;&gt;Distribution of the names in SGDI Dataset by Ministry / Statboard&lt;/h4&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/sgdi_intro/overall_distribution_sgdi_breakdown_hu25514a0ecb97fbb9b2b4a01bc4b8c1be_107429_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;/post/sgdi_intro/overall_distribution_sgdi_breakdown_hu25514a0ecb97fbb9b2b4a01bc4b8c1be_107429_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1457&#34; height=&#34;872&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;


&lt;h3 id=&#34;comparison-of-sgdi-vs-official-data-gov-data&#34;&gt;Comparison of SGDI vs Official Data.gov Data&lt;/h3&gt;

&lt;p&gt;To see how far/near our SGDI dataset is to actual numbers, we compare it against the data.gov 2016 dataset.&lt;/p&gt;

&lt;h4 id=&#34;distribution-of-the-names-in-sgdi-dataset-by-ministry-statboard-1&#34;&gt;Distribution of the names in SGDI Dataset by Ministry / Statboard&lt;/h4&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/sgdi_intro/compared_data_gov_distribution_hu00247c27c3c0e2e7f956ff49a84c5273_107481_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;/post/sgdi_intro/compared_data_gov_distribution_hu00247c27c3c0e2e7f956ff49a84c5273_107481_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1400&#34; height=&#34;872&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;


&lt;p&gt;Now, we can see that there are definitely large gaps between the numbers on the SGDI and the ground truth (Data.gov). However, does this mean that the information is useless?&lt;/p&gt;

&lt;p&gt;Not necessarily. The SGDI dataset will typically capture government employees with either a public facing function or in a position of visibility. It can serve as a credentialling tool for employees to verify their identity to literally anyone that requires it.&lt;/p&gt;

&lt;p&gt;As such, operations-based roles such as front-line medical staff, teachers and military/civil personnel aren&amp;rsquo;t really expected to be on it but will contribute to the official headcount numbers.&lt;/p&gt;

&lt;p&gt;Those in sensitive areas such as the Home Affairs and Defence Ministry are also unlikely to be on it for matters of national security.&lt;/p&gt;

&lt;p&gt;In the next series of posts, I will be looking at representing the entire SGDI structure in a graph-based network diagram, trying to sieve out the complexity of our government. Furthermore, as we have the names of our public servants, I will attempt to use machine learning methods to label both the ethnicity and gender of the individuals to better understand how our departments are staffed.&lt;/p&gt;

&lt;p&gt;Stay tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Capstone #4 Topic Modeling</title>
      <link>/post/capstone-topic-modeling/</link>
      <pubDate>Wed, 31 Jul 2019 18:12:43 +0800</pubDate>
      <guid>/post/capstone-topic-modeling/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;This is the forth of a multi part series that details the processes behind 
&lt;a href=&#34;https://fparl.bearylogical.net&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FastParliament&lt;/a&gt;
. You may view the previous post 
&lt;a href=&#34;/post/capstone-recommender/&#34;&gt;here&lt;/a&gt;
:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#motivation&#34;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#quick-recap--the-state-of-our-corpus&#34;&gt;Quick Recap : The State of Our Corpus&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#introducing-latent-dirichlet-allocation-lda&#34;&gt;Introducing Latent Dirichlet Allocation (LDA)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#walkthrough---applying-lda&#34;&gt;Walkthrough - Applying LDA&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#1-import-libraries&#34;&gt;1. Import Libraries&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-create-processing-functions&#34;&gt;2. Create Processing Functions&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-train-lda-model&#34;&gt;3. Train LDA Model&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-coherence-scores-and-tuning&#34;&gt;4. Coherence Scores and Tuning&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#visualizing-our-coherence-scores-over-number-of-topics&#34;&gt;Visualizing our coherence scores over number of topics&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-visualising-our-lda-model&#34;&gt;5. Visualising our LDA Model&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#6-assigning-topic-terms-to-topics&#34;&gt;6. Assigning Topic Terms to Topics&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#7-appending-our-corpus-with-tags&#34;&gt;7. Appending our Corpus with Tags!&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#8-visualising-our-topic-distributions&#34;&gt;8. Visualising our Topic Distributions&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;While topic modeling is not a visible element behind FastParliament, it allows us to derive insights from our corpus that would otherwise not be apparent.&lt;/p&gt;
&lt;p&gt;For example, you may be curious to know how discussion topics trend against time, or even ask &lt;em&gt;Is there a special time of the month&lt;/em&gt; when parliamentarians discuss on certain key topics?&lt;/p&gt;
&lt;p&gt;Now I hope I sparked something in you to best appreciate &lt;em&gt;why&lt;/em&gt; this topic modeling thing is interesting!&lt;/p&gt;
&lt;h2 id=&#34;quick-recap--the-state-of-our-corpus&#34;&gt;Quick Recap : The State of Our Corpus&lt;/h2&gt;
&lt;p&gt;FastParliament has a document store of around 10,000 documents. Now, that may not be much but imagine if you tasked to &amp;ldquo;find out what parliamentarians talk about&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;How might you begin? Well, you could be adventurous and read all 10,000 documents, look out for certain keywords and then slowly group those like minded documents together. Or&amp;hellip; you could be like this guy:&lt;/p&gt;















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://qph.fs.quoracdn.net/main-qimg-027c13f7ea7c66680eb164c586133d53-c&#34; data-caption=&#34;Laziness is the true mother of all invention&#34;&gt;


  &lt;img src=&#34;https://qph.fs.quoracdn.net/main-qimg-027c13f7ea7c66680eb164c586133d53-c&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Laziness is the true mother of all invention
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;introducing-latent-dirichlet-allocation-lda&#34;&gt;Introducing Latent Dirichlet Allocation (LDA)&lt;/h2&gt;
&lt;p&gt;Taken from 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wikipedia&lt;/a&gt;
:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word&amp;rsquo;s presence is attributable to one of the document&amp;rsquo;s topics.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s unpack that.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s define the word topic. A topic is simply a distribution of words. The topic MONEY will likely contain words related to money such as dollars, cents and taxes. Take note that there is no semantical meaning behind the words. Topics are merely  probabilistic co-occurances of those words. Also, topic naming in itself is a supervised approach. It is supervised in the sense that an actual human has to step in to give meaning to the distribution of words.&lt;/p&gt;
&lt;p&gt;Next, we generally assume that each document is a mixture of topics and it follows that the distribution of topics follows a probabilistic model.&lt;/p&gt;
&lt;p&gt;Beyond this description, I am not going to delve too much in specifics as the equations that define the probabilistic distribution of topic, words and documents can be somewhat unwieldy.&lt;/p&gt;
&lt;h2 id=&#34;walkthrough---applying-lda&#34;&gt;Walkthrough - Applying LDA&lt;/h2&gt;
&lt;p&gt;In this section I will be showing how LDA was applied for our corpus. This follows very closely to the tutorials suppled by Gensim.&lt;/p&gt;
&lt;p&gt;Additionally, we also used pyLDAVis to visualise the topic distribution and also show the histogram of words for a particular topic.&lt;/p&gt;
&lt;h2 id=&#34;1-import-libraries&#34;&gt;1. Import Libraries&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd
from datetime import date 

import matplotlib.pyplot as plt
import seaborn as sns

import pyLDAvis.gensim
import pyLDAvis
import pymongo 

from tqdm import tqdm

import gensim
from gensim.utils import simple_preprocess
from nltk.corpus import stopwords

from bson.json_util import dumps
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;2-create-processing-functions&#34;&gt;2. Create Processing Functions&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create stopwords
stop_words = stopwords.words(&#39;english&#39;)

# Convert Sentences to words
def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

# Remove Stopwords
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

# Generate Bigrams from text
def bigrams(words, bi_min=15, tri_min=10):
    bigram = gensim.models.Phrases(words, min_count = bi_min)
    bigram_mod = gensim.models.phrases.Phraser(bigram)
    return bigram_mod

# Combine preprocessing techniques and incorporate bigrams to generate corpus, vocabulary(bigram) and dictionary(id2word).    
def get_corpus(df):
    words = list(sent_to_words(df.cleaned_join))
    words = remove_stopwords(words)
    bigram_mod = bigrams(words)
    bigram = [bigram_mod[review] for review in words]
    id2word = gensim.corpora.Dictionary(bigram)
    id2word.filter_extremes(no_below=10, no_above=0.35)
    id2word.compactify()
    corpus = [id2word.doc2bow(text) for text in bigram]
    return corpus, id2word, bigram

train_corpus, train_id2word, bigram_train = get_corpus(mongo_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;3-train-lda-model&#34;&gt;3. Train LDA Model&lt;/h2&gt;
&lt;p&gt;We use the &lt;code&gt;ldamulticore&lt;/code&gt; function to be able to utilise more cores to generate our model. We then save the model so we can reuse it for future.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import warnings
import logging # This allows for seeing if the model converges. A log file is created.
logging.basicConfig(filename=&#39;lda_model.log&#39;, format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)

with warnings.catch_warnings():
    warnings.simplefilter(&#39;ignore&#39;)
    lda_train = gensim.models.ldamulticore.LdaMulticore(
                           corpus=train_corpus,
                           num_topics=10,
                           id2word=train_id2word,
                           chunksize=100,
                           workers=5, # Num. Processing Cores - 1
                           passes=50,
                           eval_every = 1,
                           per_word_topics=True)
lda_train.save(&#39;lda_train.model&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;4-coherence-scores-and-tuning&#34;&gt;4. Coherence Scores and Tuning&lt;/h2&gt;
&lt;p&gt;While there can be a multitude of ways to assess the coherence of a model, $$C_v$$ is used in FastParliament which is a combination of two other coherence measure. More details can be found 
&lt;a href=&#34;http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;A function &lt;code&gt;compute_coherence_values&lt;/code&gt;, which is from this 
&lt;a href=&#34;https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;great&lt;/a&gt;
 resource by datascience+ is used for the subsequent evaluation of our coherence scores. It is paired with the &lt;code&gt;CoherenceModel&lt;/code&gt; in Gensim to obtain the scores.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from gensim.models import CoherenceModel

def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    &amp;quot;&amp;quot;&amp;quot;
    Compute c_v coherence for various number of topics

    Parameters:
    ----------
    dictionary : Gensim dictionary
    corpus : Gensim corpus
    texts : List of input texts
    limit : Max num of topics

    Returns:
    -------
    model_list : List of LDA topic models
    coherence_values : Coherence values corresponding to the LDA model with respective number of topics
    &amp;quot;&amp;quot;&amp;quot;
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = gensim.models.ldamulticore.LdaMulticore(
                           corpus=corpus,
                           num_topics=num_topics,
                           id2word=train_id2word,
                           chunksize=1000,
                           workers=5, # Num. Processing Cores - 1
                           passes=50,
                           eval_every = 1,
                           per_word_topics=True)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence=&#39;c_v&#39;)
        coherence_values.append(coherencemodel.get_coherence())

    return model_list, coherence_values

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, this function takes in the scores, at spits out a list of scores with respect to the number of topics defined.&lt;/p&gt;
&lt;p&gt;However, it should be noted that each time the model instance is called to generate the LDA model, the values are &lt;strong&gt;stochastically determined&lt;/strong&gt;. As such, it may not be a highly accurate method of accessing coherence, but it largely serves our purpose.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# run
model_list, coherence_values = compute_coherence_values(dictionary=train_id2word, 
                                                        corpus=train_corpus, texts=bigram_train, start=5, limit=40, step=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;visualizing-our-coherence-scores-over-number-of-topics&#34;&gt;Visualizing our coherence scores over number of topics&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt

%matplotlib inline

# Show graph
limit=40; start=5; step=2;
x = range(start, limit, step)
plt.plot(x, coherence_values)
plt.xlabel(&amp;quot;Num Topics&amp;quot;)
plt.ylabel(&amp;quot;Coherence score&amp;quot;)
plt.legend((&amp;quot;coherence_values&amp;quot;), loc=&#39;best&#39;)
plt.show()


&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;coherence.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;5-visualising-our-lda-model&#34;&gt;5. Visualising our LDA Model&lt;/h2&gt;
&lt;p&gt;We then use pyLDAVis, which is a great port of the LDAvis library from R, to visualize our topics and the words distribution within the topic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;## Visualise our model
pyLDAvis.enable_notebook(sort=True)
vis = pyLDAvis.gensim.prepare(lda_train, train_corpus, train_id2word)
pyLDAvis.display(vis)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Due to iframe limitations, I&amp;rsquo;m not able to squeeze it in this post, but you may view the visualisation 
&lt;a href=&#34;/post/capstone-topic-modeling/lda.html&#34;&gt;here&lt;/a&gt;
. However, here is a screenshot:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ldavis.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;6-assigning-topic-terms-to-topics&#34;&gt;6. Assigning Topic Terms to Topics&lt;/h2&gt;
&lt;p&gt;After that is all said and done, we move on to assigning the terms to each topic.&lt;/p&gt;
&lt;p&gt;In this iteration of modeling, we print out the top 20 words associated to a topic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for idx, topic in lda_train.print_topics(-1, num_words=20):
    print(&amp;quot;{}. Topic: {}  \n\t- Key Words: {}&amp;quot;.format(idx+1, idx, (&amp;quot;,&amp;quot;.join(re.sub(r&#39;\d.\d+\*&#39;,&#39;&#39;,topic).replace(&#39;&amp;quot;&#39;,&#39;&#39;).split(&#39;+&#39;)))))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1. Topic: 0  
    - Key Words: water , nea , public , residents , town_councils , food , ensure , town_council , ministry , areas , year , use , pub , sir , health , management , buildings , resources , measures , would
2. Topic: 1  
    - Key Words: new , companies , government , industry , help , services , support , sector , businesses , smes , business , technology , continue , develop , local , example , growth , opportunities , development , need
3. Topic: 2  
    - Key Words: saf , mas , defence , countries , asean , international , us , security , training , year , financial , foreign , region , ns , mindef , banks , china , financial_institutions , continue , investment
4. Topic: 3  
    - Key Words: students , education , schools , school , community , moe , children , support , programmes , parents , teachers , sports , learning , programme , work , arts , help , year , training , provide
5. Topic: 4  
    - Key Words: hdb , lta , new , flats , would , time , residents , public_transport , flat , year , one , housing , road , operators , commuters , transport , system , development , first , sir
6. Topic: 5  
    - Key Words: police , public , officers , cases , security , safety , home_affairs , number , ministry , community , measures , order , would , crime , act , ensure , take , offence , work , home_team
7. Topic: 6  
    - Key Words: care , children , support , help , patients , health , family , need , families , singaporeans , healthcare , community , medical , services , parents , elderly , scheme , seniors , provide , child
8. Topic: 7  
    - Key Words: people , government , one , would , singaporeans , us , think , must , need , even , sir , time , said , want , like , first , know , make , good , society
9. Topic: 8  
    - Key Words: workers , government , help , companies , work , singaporeans , jobs , budget , employers , would , need , year , one , support , scheme , time , sir , economy , employment , new
10. Topic: 9  
    - Key Words: bill , act , would , law , case , court , legal , amendments , public , section , cases , sir , make , first , members , time , made , person , one , ensure
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Accordingly, we can label the 10 topics as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Topic: 0 - Environment
&lt;ul&gt;
&lt;li&gt;Key Words: water , nea , public , residents , town_councils , food , ensure , town_council , ministry , areas&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Topic: 1 - Business
&lt;ul&gt;
&lt;li&gt;Key Words: new , companies , government , industry , help , services , support , sector , businesses , smes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Topic: 2 - External Security
&lt;ul&gt;
&lt;li&gt;Key Words: saf , mas , defence , countries , asean , international , us , security , training , year&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Topic: 3 - Education
&lt;ul&gt;
&lt;li&gt;Key Words: students , education , schools , school , community , moe , children , support , programmes , parents&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Topic: 4 - Living
&lt;ul&gt;
&lt;li&gt;Key Words: hdb , lta , new , flats , would , time , residents , public_transport , flat , year&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Topic: 5 - Internal Security
&lt;ul&gt;
&lt;li&gt;Key Words: police , public , officers , cases , security , safety , home_affairs , number , ministry , community&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Topic: 6 - Healthcare
&lt;ul&gt;
&lt;li&gt;Key Words: care , children , support , help , patients , health , family , need , families , singaporeans&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Topic: 7 - Society
&lt;ul&gt;
&lt;li&gt;Key Words: people , government , one , would , singaporeans , us , think , must , need , even&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Topic: 8 - Employment
&lt;ul&gt;
&lt;li&gt;Key Words: workers , government , help , companies , work , singaporeans , jobs , budget , employers , would&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Topic: 9 - Law
&lt;ul&gt;
&lt;li&gt;Key Words: bill , act , would , law , case , court , legal , amendments , public , section&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;7-appending-our-corpus-with-tags&#34;&gt;7. Appending our Corpus with Tags!&lt;/h2&gt;
&lt;p&gt;Given that we now know what topic numbers refer to what topics, we will then assign a topic to each document. The dominant topic (highest probability) will be assigned.&lt;/p&gt;
&lt;p&gt;For a particular document :&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;## Access document at index 1 and obtain list of tuples where each topic is the index of the topic and the score.
row = model[train_corpus][1]
## Sort probabilities in descending order
row = sorted(row[0], key=lambda x: x[1], reverse=True)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(7, 0.43655953),
 (2, 0.17807838),
 (0, 0.13052306),
 (9, 0.09896996),
 (5, 0.058110144),
 (1, 0.054801296),
 (4, 0.042660255)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then create a function to show us the score of each dominant topic in a document&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def format_topics_sentences(ldamodel=model, corpus=train_corpus,texts=bigram_train):
    # https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get top 3 topic in each document
    for i, row in enumerate(ldamodel[corpus]):
        row = sorted(row[0], key=lambda x: x[1], reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # =&amp;gt; dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = &amp;quot;, &amp;quot;.join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = [&#39;Dominant_Topic&#39;, &#39;Perc_Contribution&#39;, &#39;Topic_Keywords&#39;]

    # Add original text to the end of the output
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)

df_topic_sents_keywords = format_topics_sentences(ldamodel=model, corpus=train_corpus, texts=bigram_train)

# Format
df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = [&#39;Document_No&#39;, &#39;Dominant_Topic&#39;, &#39;Topic_Perc_Contrib&#39;, &#39;Keywords&#39;, &#39;Text&#39;]

# Show
df_dominant_topic.head(10)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Document_No&lt;/th&gt;
      &lt;th&gt;Dominant_Topic&lt;/th&gt;
      &lt;th&gt;Topic_Perc_Contrib&lt;/th&gt;
      &lt;th&gt;Keywords&lt;/th&gt;
      &lt;th&gt;Text&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;0.4911&lt;/td&gt;
      &lt;td&gt;people, government, one, would, singaporeans, ...&lt;/td&gt;
      &lt;td&gt;[proc_text, debate_resumed, proc_text, br_mr, ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;0.4366&lt;/td&gt;
      &lt;td&gt;people, government, one, would, singaporeans, ...&lt;/td&gt;
      &lt;td&gt;[mr_vikram, nair_asked, minister, foreign_affa...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;0.3280&lt;/td&gt;
      &lt;td&gt;police, public, officers, cases, security, saf...&lt;/td&gt;
      &lt;td&gt;[assoc_prof, walter_theseira, asked_minister, ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.2807&lt;/td&gt;
      &lt;td&gt;water, nea, public, residents, town_councils, ...&lt;/td&gt;
      &lt;td&gt;[ms_irene, quay_siew, ching, asked_minister, h...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;0.3995&lt;/td&gt;
      &lt;td&gt;people, government, one, would, singaporeans, ...&lt;/td&gt;
      &lt;td&gt;[mr_lim, biow_chuan, asked_deputy, prime_minis...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;6.0&lt;/td&gt;
      &lt;td&gt;0.4204&lt;/td&gt;
      &lt;td&gt;care, children, support, help, patients, healt...&lt;/td&gt;
      &lt;td&gt;[mr_louis, ng_kok, kwang_asked, minister, heal...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;0.3054&lt;/td&gt;
      &lt;td&gt;people, government, one, would, singaporeans, ...&lt;/td&gt;
      &lt;td&gt;[mr_png, eng_huat, asked_deputy, prime_ministe...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;8.0&lt;/td&gt;
      &lt;td&gt;0.2996&lt;/td&gt;
      &lt;td&gt;workers, government, help, companies, work, si...&lt;/td&gt;
      &lt;td&gt;[mr_louis, ng_kok, kwang_asked, minister_manpo...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;0.5370&lt;/td&gt;
      &lt;td&gt;students, education, schools, school, communit...&lt;/td&gt;
      &lt;td&gt;[ms_irene, quay_siew, ching, asked_minister, e...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;7.0&lt;/td&gt;
      &lt;td&gt;0.4994&lt;/td&gt;
      &lt;td&gt;people, government, one, would, singaporeans, ...&lt;/td&gt;
      &lt;td&gt;[proc_text, resumption_debate, question, may, ...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;By focusing only on adding the dominant topic to our dataframe, we then create a function called &lt;code&gt;retrieve_dominant_topic&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def retrieve_dominant_topic(doc, model=model ,ref_dict=lda_dict, threshold=0.1):
    doc_bow_sample = ref_dict.doc2bow(doc.split()) #input has to be a list of strings
    highest_prob = sorted(model.get_document_topics(doc_bow_sample),key=lambda x : x[1], reverse=True)[0][1]
    if highest_prob &amp;gt; threshold:
        topic_num = sorted(model.get_document_topics(doc_bow_sample),key=lambda x : x[1], reverse=True)[0][0]
    else:
        topic_num = 11 # no topic identidied
    
    return topic_num 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then use pandas &lt;code&gt;.map()&lt;/code&gt; function to map each topic to each row of our document corpus.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%time mongo_df[&#39;dominant_topic&#39;] = mongo_df.cleaned_join.map(lambda x : retrieve_dominant_topic(x))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By defining the &lt;code&gt;dominant_topic_mappings&lt;/code&gt; dictionary, we then map the numbers to the actual strings that define the topic id.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dominant_topic_mappings = {
    0 : &#39;Environment&#39;,
    1 : &#39;Business&#39;,
    2 : &#39;External Security&#39;,
    3 : &#39;Education&#39;,
    4 : &#39;Living&#39;,
    5 : &#39;Internal Security&#39;,
    6 : &#39;Healthcare&#39;,
    7 : &#39;Society&#39;,
    8 : &#39;Employment&#39;,
    9 : &#39;Law&#39;,
    11 : &#39;Unidentified&#39;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mongo_df[&#39;dominant_topic&#39;] = mongo_df[&#39;dominant_topic&#39;].map(dominant_topic_mappings)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;8-visualising-our-topic-distributions&#34;&gt;8. Visualising our Topic Distributions&lt;/h2&gt;
&lt;p&gt;With each document having the &lt;code&gt;dominant_topic&lt;/code&gt; label, we then moved to visualise it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
sns.catplot(x=&#39;dominant_topic&#39;,kind=&amp;quot;count&amp;quot;,data = mongo_df,height=5, aspect=3 )

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;topic_distrib.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now that we have gotten our topics, we then try to answer our initial question :&lt;/p&gt;
&lt;p&gt;&lt;em&gt;How have topics changed over time?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the example below, I chose &lt;code&gt;Education&lt;/code&gt; as a topic. In this visualisation, we are looking at how the topic trends vary across the months of the year. Accordingly, the intensity of the line hue reflects more recent years&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;topic = &amp;quot;Education&amp;quot;

plt.figure(figsize=(20,10))
ax = sns.relplot(x=&amp;quot;month&amp;quot;, y=&amp;quot;topic_count&amp;quot;, hue=&amp;quot;year&amp;quot;, kind = &amp;quot;line&amp;quot;, data=topic_groupings_count[topic_groupings_count[&#39;dominant_topic&#39;] == topic],aspect=2)
plt.title(topic)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;education.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(20,10))
ax = sns.lineplot(x=&amp;quot;year&amp;quot;, y=&amp;quot;topic_count&amp;quot;, hue=&amp;quot;dominant_topic&amp;quot;,  data=topic_groupings_count)
plt.setp(ax.collections, alpha=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;topic_plot.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Thanks for following through with this entire post! I hope you came away with more understanding on the capabilities of LDA as well as various visualization methods.&lt;/p&gt;
&lt;p&gt;Till next time!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Capstone #3 Content Discovery System</title>
      <link>/post/capstone-recommender/</link>
      <pubDate>Wed, 31 Jul 2019 15:57:29 +0800</pubDate>
      <guid>/post/capstone-recommender/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;This is the third of a multi part series that details the processes behind &lt;a href=&#34;https://fparl.bearylogical.net&#34;&gt;FastParliament&lt;/a&gt;. You may view the previous post &lt;a href=&#34;/post/capstone-summarizer/&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#recap-of-vector-representations&#34;&gt;Recap of Vector Representations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#enter-word2vec&#34;&gt;Enter Word2Vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#doc2vec-an-extension-of-word2vec&#34;&gt;Doc2Vec : An Extension of Word2Vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#applying-doc2vec-for-fastparliament&#34;&gt;Applying Doc2Vec for FastParliament&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#walkthrough-of-doc2vec-process&#34;&gt;Walkthrough of Doc2Vec Process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#1-import-libraries&#34;&gt;1. Import Libraries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2-read-in-data-from-mongodb&#34;&gt;2. Read in Data from MongoDB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#3-text-preprocessing&#34;&gt;3. Text Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#4-instantiating-a-doc2vec-gensim-instance&#34;&gt;4. Instantiating a Doc2Vec GenSim Instance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#5-build-a-vocabulary&#34;&gt;5. Build a Vocabulary&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#6-save-model&#34;&gt;6. Save Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#7-load-model&#34;&gt;7. Load Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#8-sample-inference&#34;&gt;8. Sample Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#9-using-it-in-production&#34;&gt;9. Using it in Production&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;&lt;/p&gt;

&lt;p&gt;For this post, I will be focusing on the content discovery component of &lt;a href=&#34;https://fparl.bearylogical.net/#summary&#34;&gt;FastParliament&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you have been around FastParliament, you may have observed that each time an article is retrieved from the system, a series of articles are also put forth - which are related to the content of the primary article. Usually, to do so in conventional systems would require the use of &amp;ldquo;tags&amp;rdquo; that article has to allow the system to &amp;ldquo;cough&amp;rdquo; out articles with similar tags.&lt;/p&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/capstone-recommender/recommender_hu8e3f2182230b05f4e85ddab698b6aa12_188796_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Content Discovery&#34;&gt;


  &lt;img data-src=&#34;/post/capstone-recommender/recommender_hu8e3f2182230b05f4e85ddab698b6aa12_188796_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1256&#34; height=&#34;735&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Content Discovery
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;This works well for a small corpus of say, 50 to at most 200 articles. However, the solution obviously does not scale very well, if not at all. Furthermore, the tagging system can be highly arbitrary, where each person can have their own interpretation of what tags are contained in a particular article.&lt;/p&gt;

&lt;h2 id=&#34;recap-of-vector-representations&#34;&gt;Recap of Vector Representations&lt;/h2&gt;

&lt;p&gt;In the &lt;a href=&#34;/post/capstone-summarizer/&#34;&gt;previous&lt;/a&gt; article, sentence vectors were created out of the TF-IDF method where the vector represented weighted frequencies of a particular word in a text.&lt;/p&gt;

&lt;p&gt;While that works for the particular use case, semantics are often lost when we are merely looking at counts of a particular word in the document.&lt;/p&gt;

&lt;h2 id=&#34;enter-word2vec&#34;&gt;Enter Word2Vec&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Most portions of this description are based on the writeup by &lt;a href=&#34;https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e&#34;&gt;Gidi Shperber&lt;/a&gt; in which he has a good summary on Doc2Vec.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Word2Vec is a technique to capture contexts between words. There are two key models in word2vec - namely Continuous Bag of Words(CBoW) and Skip-Gram. For CBoW a sliding window is created around a bunch of words or context with the objective of predicting the probability of the target word. In Skip-Gram, the opposite occurs in which we want to predict the context (&amp;ldquo;surrounding words&amp;rdquo;) given a specific word.&lt;/p&gt;

&lt;p&gt;Depending on the case use, one particular model may be preferred over another.&lt;/p&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/capstone-recommender/word2vec_hu77da82a9bad4a5fb714f538ac9e021ef_121775_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Word2Vec Methods Illustration Obtained from Rakuten&#34;&gt;


  &lt;img data-src=&#34;/post/capstone-recommender/word2vec_hu77da82a9bad4a5fb714f538ac9e021ef_121775_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;626&#34; height=&#34;468&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Word2Vec Methods Illustration Obtained from &lt;a href=&#34;https://www.slideshare.net/rakutentech/distributed-representationbased-recommender-systems-in-ecommerce&#34;&gt;Rakuten&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;The end result, or representations would be somewhat visualised as below:&lt;/p&gt;
















&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.tensorflow.org/images/linear-relationships.png&#34; data-caption=&#34;Semantic Representation from word vectors. Obtained from tensorflow docs.&#34;&gt;


  &lt;img src=&#34;https://www.tensorflow.org/images/linear-relationships.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Semantic Representation from word vectors. Obtained from &lt;a href=&#34;https://www.tensorflow.org/tutorials/representation/word2vec&#34;&gt;tensorflow docs&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;So, then where does that leave us?&lt;/p&gt;

&lt;p&gt;Knowing that there is a way to represent text as vector representations that is context-aware would mean that we can apply the same concept to our Hansard corpus.&lt;/p&gt;

&lt;p&gt;This would require converting each document to its vector representation in relation to the entire corpus.&lt;/p&gt;

&lt;h2 id=&#34;doc2vec-an-extension-of-word2vec&#34;&gt;Doc2Vec : An Extension of Word2Vec&lt;/h2&gt;

&lt;p&gt;One key consideration between the Doc2Vec and Word2Vec is that documents and words don&amp;rsquo;t share the same logical structures. The continuous representation that we see in a line of text may not be the same as a line of documents.&lt;/p&gt;

&lt;p&gt;To get around this problem, an additional vector is fed into either the Doc2Vec version of either the CBoW or the Skip-Gram method. This additional vector is referred to as the document vector which is essentially the &amp;ldquo;ID&amp;rdquo; of the document.&lt;/p&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/capstone-recommender/doc2vec_hua699b4048c861c4a5b8c1eb5814ea50a_144611_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Doc2Vec Methods Illustration Obtained from Rakuten&#34;&gt;


  &lt;img data-src=&#34;/post/capstone-recommender/doc2vec_hua699b4048c861c4a5b8c1eb5814ea50a_144611_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;620&#34; height=&#34;470&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Doc2Vec Methods Illustration Obtained from &lt;a href=&#34;https://www.slideshare.net/rakutentech/distributed-representationbased-recommender-systems-in-ecommerce&#34;&gt;Rakuten&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;In this case the CBoW now becomes Document Vector Distributed Bag of Words (DV-DBoW) while the Skip-Gram approach is now called Document Vector Distributed Memory (DV-DM) in which the document vector acts as a &amp;ldquo;memory&amp;rdquo; to assist in identifying the context of the particular word in a document. Essentially, it extends the concept of a particular word beyond the sliding window to between various documents.&lt;/p&gt;

&lt;h2 id=&#34;applying-doc2vec-for-fastparliament&#34;&gt;Applying Doc2Vec for FastParliament&lt;/h2&gt;

&lt;p&gt;Now, with these concepts in hand, we look at applying the same approaches to our corpus with the objective of generating document to document recommendations to aid in content discovery.&lt;/p&gt;

&lt;h2 id=&#34;walkthrough-of-doc2vec-process&#34;&gt;Walkthrough of Doc2Vec Process&lt;/h2&gt;

&lt;p&gt;In this example, I will be using gensim&amp;rsquo;s doc2vec generator to generate the vectors. I will also extend this further by showing how I operationalized this on FastParliament.&lt;/p&gt;

&lt;h2 id=&#34;1-import-libraries&#34;&gt;1. Import Libraries&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gensim
from gensim.utils import simple_preprocess
import pymongo
import pandas as pd
import numpy as np

from bson import ObjectId
from sklearn.model_selection import train_test_split
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-read-in-data-from-mongodb&#34;&gt;2. Read in Data from MongoDB&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;client = pymongo.MongoClient(&amp;quot;mongodb://localhost:27017/&amp;quot;)

db = client[&amp;quot;parliament&amp;quot;]
articles = db[&amp;quot;articles&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mongo_df = pd.DataFrame.from_records(remote_articles.find())
mongo_df.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;_id&lt;/th&gt;
      &lt;th&gt;article_text&lt;/th&gt;
      &lt;th&gt;chunks&lt;/th&gt;
      &lt;th&gt;cleaned_join&lt;/th&gt;
      &lt;th&gt;dominant_topic&lt;/th&gt;
      &lt;th&gt;html_clean&lt;/th&gt;
      &lt;th&gt;parliament_num&lt;/th&gt;
      &lt;th&gt;parsed_convo&lt;/th&gt;
      &lt;th&gt;persons_involved&lt;/th&gt;
      &lt;th&gt;session_num&lt;/th&gt;
      &lt;th&gt;session_type&lt;/th&gt;
      &lt;th&gt;sitting_date&lt;/th&gt;
      &lt;th&gt;sitting_num&lt;/th&gt;
      &lt;th&gt;src_url&lt;/th&gt;
      &lt;th&gt;title&lt;/th&gt;
      &lt;th&gt;volume_num&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;5d27eca6172d9aa762d4802f&lt;/td&gt;
      &lt;td&gt;&amp;lt;p&amp;gt;[(proc text) Debate resumed. (proc text)]&amp;lt;/...&lt;/td&gt;
      &lt;td&gt;{&#34;0&#34;: {&#34;entity&#34;: &#34;NA&#34;, &#34;content&#34;: &#34;[(proc text...&lt;/td&gt;
      &lt;td&gt;[(proc text) Debate resumed. (proc text)]&amp;lt;br/&amp;gt;...&lt;/td&gt;
      &lt;td&gt;Society&lt;/td&gt;
      &lt;td&gt;[[(proc text) Debate resumed. (proc text)], Mr...&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;[{&#39;content&#39;: &#39;[(proc text) Debate resumed. (pr...&lt;/td&gt;
      &lt;td&gt;[Mr Leon Perera, Mr K Shanmugam, Assoc Prof Wa...&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;SECOND READING BILLS&lt;/td&gt;
      &lt;td&gt;2019-05-08&lt;/td&gt;
      &lt;td&gt;105&lt;/td&gt;
      &lt;td&gt;https://sprs.parl.gov.sg/search/sprs3topic?rep...&lt;/td&gt;
      &lt;td&gt;PROTECTION FROM ONLINE FALSEHOODS AND MANIPULA...&lt;/td&gt;
      &lt;td&gt;94&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;5d27eca6172d9aa762d48030&lt;/td&gt;
      &lt;td&gt;&amp;lt;p class=&#34;ql-align-justify&#34;&amp;gt;4 &amp;lt;strong&amp;gt;Mr Vikra...&lt;/td&gt;
      &lt;td&gt;{&#34;0&#34;: {&#34;entity&#34;: &#34;NA&#34;, &#34;content&#34;: &#34;Mr Vikram N...&lt;/td&gt;
      &lt;td&gt;Mr Vikram Nair asked the Minister for Foreign ...&lt;/td&gt;
      &lt;td&gt;Society&lt;/td&gt;
      &lt;td&gt;[Mr Vikram Nair asked the Minister for Foreign...&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;[{&#39;content&#39;: &#39;Mr Vikram Nair asked the Ministe...&lt;/td&gt;
      &lt;td&gt;[Dr Vivian Balakrishnan, The Minister for Fore...&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;ORAL ANSWERS&lt;/td&gt;
      &lt;td&gt;2019-05-08&lt;/td&gt;
      &lt;td&gt;105&lt;/td&gt;
      &lt;td&gt;https://sprs.parl.gov.sg/search/sprs3topic?rep...&lt;/td&gt;
      &lt;td&gt;STATE OF BILATERAL RELATIONS WITH MALAYSIA FOL...&lt;/td&gt;
      &lt;td&gt;94&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;5d27eca6172d9aa762d48031&lt;/td&gt;
      &lt;td&gt;&amp;lt;p class=&#34;ql-align-justify&#34;&amp;gt;8 &amp;lt;strong&amp;gt;Assoc Pr...&lt;/td&gt;
      &lt;td&gt;{&#34;0&#34;: {&#34;entity&#34;: &#34;NA&#34;, &#34;content&#34;: &#34;Assoc Prof ...&lt;/td&gt;
      &lt;td&gt;Assoc Prof Walter Theseira asked the Minister ...&lt;/td&gt;
      &lt;td&gt;Internal Security&lt;/td&gt;
      &lt;td&gt;[Assoc Prof Walter Theseira asked the Minister...&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;[{&#39;content&#39;: &#39;Assoc Prof Walter Theseira asked...&lt;/td&gt;
      &lt;td&gt;[Ms Low Yen Ling, Ms Anthea Ong, Assoc Prof Wa...&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;ORAL ANSWERS&lt;/td&gt;
      &lt;td&gt;2019-05-08&lt;/td&gt;
      &lt;td&gt;105&lt;/td&gt;
      &lt;td&gt;https://sprs.parl.gov.sg/search/sprs3topic?rep...&lt;/td&gt;
      &lt;td&gt;COMPANIES WITH MEASURES TO DEAL WITH WORKPLACE...&lt;/td&gt;
      &lt;td&gt;94&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;5d27eca6172d9aa762d48032&lt;/td&gt;
      &lt;td&gt;&amp;lt;p&amp;gt;5 &amp;lt;strong&amp;gt;Ms Irene Quay Siew Ching&amp;lt;/strong&amp;gt;...&lt;/td&gt;
      &lt;td&gt;{&#34;0&#34;: {&#34;entity&#34;: &#34;NA&#34;, &#34;content&#34;: &#34;Ms Irene Qu...&lt;/td&gt;
      &lt;td&gt;Ms Irene Quay Siew Ching asked the Minister fo...&lt;/td&gt;
      &lt;td&gt;Environment&lt;/td&gt;
      &lt;td&gt;[Ms Irene Quay Siew Ching asked the Minister f...&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;[{&#39;content&#39;: &#39;Ms Irene Quay Siew Ching asked t...&lt;/td&gt;
      &lt;td&gt;[The Senior Minister of State for Health, Dr L...&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;ORAL ANSWERS&lt;/td&gt;
      &lt;td&gt;2019-05-08&lt;/td&gt;
      &lt;td&gt;105&lt;/td&gt;
      &lt;td&gt;https://sprs.parl.gov.sg/search/sprs3topic?rep...&lt;/td&gt;
      &lt;td&gt;REVIEW OF DRUG TESTING STANDARDS IN SINGAPORE ...&lt;/td&gt;
      &lt;td&gt;94&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5d27eca6172d9aa762d48033&lt;/td&gt;
      &lt;td&gt;&amp;lt;p class=&#34;ql-align-justify&#34;&amp;gt;2 &amp;lt;strong&amp;gt;Mr Lim B...&lt;/td&gt;
      &lt;td&gt;{&#34;0&#34;: {&#34;entity&#34;: &#34;NA&#34;, &#34;content&#34;: &#34;Mr Lim Biow...&lt;/td&gt;
      &lt;td&gt;Mr Lim Biow Chuan asked the Deputy Prime Minis...&lt;/td&gt;
      &lt;td&gt;Employment&lt;/td&gt;
      &lt;td&gt;[Mr Lim Biow Chuan asked the Deputy Prime Mini...&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;[{&#39;content&#39;: &#39;Mr Lim Biow Chuan asked the Depu...&lt;/td&gt;
      &lt;td&gt;[Ms Indranee Rajah, The Second Minister for Fi...&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;ORAL ANSWERS&lt;/td&gt;
      &lt;td&gt;2019-05-08&lt;/td&gt;
      &lt;td&gt;105&lt;/td&gt;
      &lt;td&gt;https://sprs.parl.gov.sg/search/sprs3topic?rep...&lt;/td&gt;
      &lt;td&gt;LIVING IN PRIVATE PROPERTIES BUT WITH NO DECLA...&lt;/td&gt;
      &lt;td&gt;94&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h2 id=&#34;3-text-preprocessing&#34;&gt;3. Text Preprocessing&lt;/h2&gt;

&lt;p&gt;In &lt;code&gt;read_corpus&lt;/code&gt; we iterate through the dataframe above and then use the &lt;code&gt;TaggedDocument&lt;/code&gt; function in Gensim to tag a document ID to each document. We can then use this document ID to retrieve additional metadata from our DB after the process is complete.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def read_corpus(series_docs, tokens_only=False):
    for line in series_docs.itertuples():
        if tokens_only:
            yield simple_preprocess(line.cleaned_join)
        else:
            # For training data, add tags
            yield gensim.models.doc2vec.TaggedDocument(simple_preprocess(line.cleaned_join), tags=[str(line._1)])

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a list of documents with tags:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;corpus = list(read_corpus(mongo_df,tokens_only=False))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Display a sample tag:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;corpus[1].tags
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[&#39;5d27eca6172d9aa762d48030&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-instantiating-a-doc2vec-gensim-instance&#34;&gt;4. Instantiating a Doc2Vec GenSim Instance&lt;/h2&gt;

&lt;p&gt;The vector_size, min_count and epochs are some hyperparameters that can be tuned down the road. I used this as it gave a relatively quick training time. In general, larger vectors result in larger training times and it exponentially scales.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=3, epochs=100, workers=4)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-build-a-vocabulary&#34;&gt;5. Build a Vocabulary&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.build_vocab(corpus)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%time model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 20min 22s, sys: 6.98 s, total: 20min 29s
Wall time: 6min 42s
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;6-save-model&#34;&gt;6. Save Model&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.save(&#39;doc2vec&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We delete training data to reduce in-memory usage after training is complete.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.delete_temporary_training_data()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;7-load-model&#34;&gt;7. Load Model&lt;/h2&gt;

&lt;p&gt;We can load the model at any time, and this is crucial for deploying it to production.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = gensim.models.doc2vec.Doc2Vec.load(&#39;doc2vec&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;8-sample-inference&#34;&gt;8. Sample Inference&lt;/h2&gt;

&lt;p&gt;In this portion, I will demonstrate how to use the inference to be able to generate a list of similar documents which will then be tied in to our production model.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;infer_vector&lt;/code&gt; method is called on the class model that is an instance of our doc2vec saved model which spits out the vectors.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;inference_1 = model.infer_vector(corpus[291].words)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using this, we generate a list of tuples with the id at index 0 and the score at index 1.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results = model.docvecs.most_similar([inference_1])
display(results)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[(&#39;5d27eca6172d9aa762d48152&#39;, 0.992645263671875),
 (&#39;5d27eca6172d9aa762d49ef0&#39;, 0.9119903445243835),
 (&#39;5d27eca6172d9aa762d48b2d&#39;, 0.907822847366333),
 (&#39;5d27eca6172d9aa762d48fc3&#39;, 0.8947123289108276),
 (&#39;5d27eca6172d9aa762d492f7&#39;, 0.8905215859413147),
 (&#39;5d27eca6172d9aa762d49304&#39;, 0.8838604688644409),
 (&#39;5d27eca6172d9aa762d48639&#39;, 0.8791929483413696),
 (&#39;5d27eca6172d9aa762d49673&#39;, 0.8630969524383545),
 (&#39;5d27eca6172d9aa762d49aac&#39;, 0.8347380757331848),
 (&#39;5d27eca6172d9aa762d48b5c&#39;, 0.8256769180297852)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can see that &lt;code&gt;most_similar&lt;/code&gt; returns a list of tuples with the document ID and its respective probability.&lt;/p&gt;

&lt;p&gt;Using this knowledge, we can use this to retrieve content from our database.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mongo_df[mongo_df[&#39;_id&#39;] == ObjectId(&#39;5d27eca6172d9aa762d48152&#39;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;_id&lt;/th&gt;
      &lt;th&gt;article_text&lt;/th&gt;
      &lt;th&gt;chunks&lt;/th&gt;
      &lt;th&gt;cleaned_join&lt;/th&gt;
      &lt;th&gt;dominant_topic&lt;/th&gt;
      &lt;th&gt;html_clean&lt;/th&gt;
      &lt;th&gt;parliament_num&lt;/th&gt;
      &lt;th&gt;parsed_convo&lt;/th&gt;
      &lt;th&gt;persons_involved&lt;/th&gt;
      &lt;th&gt;session_num&lt;/th&gt;
      &lt;th&gt;session_type&lt;/th&gt;
      &lt;th&gt;sitting_date&lt;/th&gt;
      &lt;th&gt;sitting_num&lt;/th&gt;
      &lt;th&gt;src_url&lt;/th&gt;
      &lt;th&gt;title&lt;/th&gt;
      &lt;th&gt;volume_num&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;291&lt;/th&gt;
      &lt;td&gt;5d27eca6172d9aa762d48152&lt;/td&gt;
      &lt;td&gt;&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;The Chairman&amp;lt;/strong&amp;gt;: Head P, Mini...&lt;/td&gt;
      &lt;td&gt;{&#34;0&#34;: {&#34;entity&#34;: &#34;The Chairman&#34;, &#34;content&#34;: &#34; ...&lt;/td&gt;
      &lt;td&gt;The Chairman: Head P, Ministry of Home Affairs...&lt;/td&gt;
      &lt;td&gt;Internal Security&lt;/td&gt;
      &lt;td&gt;[The Chairman: Head P, Ministry of Home Affair...&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;[{&#39;content&#39;: &#39;The Chairman: Head P, Ministry o...&lt;/td&gt;
      &lt;td&gt;[Mrs Josephine Teo, Mr Yee Chia Hsing, Ms Jess...&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;BUDGET&lt;/td&gt;
      &lt;td&gt;2019-03-01&lt;/td&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;https://sprs.parl.gov.sg/search/sprs3topic?rep...&lt;/td&gt;
      &lt;td&gt;COMMITTEE OF SUPPLY - HEAD P (MINISTRY OF HOME...&lt;/td&gt;
      &lt;td&gt;94&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h2 id=&#34;9-using-it-in-production&#34;&gt;9. Using it in Production&lt;/h2&gt;

&lt;p&gt;Following that, for our production model, we can create a function that spits out the top 5 similar documents for any document ID that is fed in.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def fetch_recommended_document(document_id,mongo_conn,model,n_results=5):
    &amp;quot;&amp;quot;&amp;quot;
    Fetch documents from mongoDB based on inference
    
    &amp;quot;&amp;quot;&amp;quot;
    document = mongo_conn.parliament.articles.find_one({&#39;_id&#39;: ObjectId(document_id)})
    inference = model.infer_vector(document[&#39;cleaned_join&#39;].split())
    results = model.docvecs.most_similar([inference])
    ids = []
    for item in results[:n_results]:
        ids.append(ObjectId(item[0]))
    
    recommends = []
    for recommend in mongo_conn.parliament.articles.find({&amp;quot;_id&amp;quot; : {&amp;quot;$in&amp;quot; : ids }}):
        recommends.append({
            &amp;quot;_id&amp;quot; : recommend[&amp;quot;_id&amp;quot;],
            &amp;quot;title&amp;quot; : recommend[&amp;quot;title&amp;quot;],
            &amp;quot;sitting_date&amp;quot; : recommend[&amp;quot;sitting_date&amp;quot;],
            &amp;quot;session_type&amp;quot; : recommend[&amp;quot;session_type&amp;quot;]
        })
    return recommends
    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A sample call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fetch_recommended_document(&#39;5d27eca6172d9aa762d48b2d&#39;,remote_client,model,6)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[{&#39;_id&#39;: ObjectId(&#39;5d27eca6172d9aa762d48152&#39;),
  &#39;title&#39;: &#39;COMMITTEE OF SUPPLY - HEAD P (MINISTRY OF HOME AFFAIRS)&#39;,
  &#39;sitting_date&#39;: datetime.datetime(2019, 3, 1, 0, 0),
  &#39;session_type&#39;: &#39;BUDGET&#39;},
 {&#39;_id&#39;: ObjectId(&#39;5d27eca6172d9aa762d48639&#39;),
  &#39;title&#39;: &#39;COMMITTEE OF SUPPLY â HEAD P (MINISTRY OF HOME AFFAIRS)&#39;,
  &#39;sitting_date&#39;: datetime.datetime(2018, 3, 2, 0, 0),
  &#39;session_type&#39;: &#39;MOTIONS&#39;},
 {&#39;_id&#39;: ObjectId(&#39;5d27eca6172d9aa762d48b2d&#39;),
  &#39;title&#39;: &#39;COMMITTEE OF SUPPLY â HEAD P (MINISTRY OF HOME AFFAIRS)&#39;,
  &#39;sitting_date&#39;: datetime.datetime(2017, 3, 3, 0, 0),
  &#39;session_type&#39;: &#39;MOTIONS&#39;},
 {&#39;_id&#39;: ObjectId(&#39;5d27eca6172d9aa762d48fc3&#39;),
  &#39;title&#39;: &#39;COMMITTEE OF SUPPLY â HEAD P (MINISTRY OF HOME AFFAIRS)&#39;,
  &#39;sitting_date&#39;: datetime.datetime(2016, 4, 6, 0, 0),
  &#39;session_type&#39;: &#39;MOTIONS&#39;},
 {&#39;_id&#39;: ObjectId(&#39;5d27eca6172d9aa762d492f7&#39;),
  &#39;title&#39;: &#39;HEAD P â MINISTRY OF HOME AFFAIRS (COMMITTEE OF SUPPLY)&#39;,
  &#39;sitting_date&#39;: datetime.datetime(2015, 3, 6, 0, 0),
  &#39;session_type&#39;: &#39;MOTIONS&#39;},
 {&#39;_id&#39;: ObjectId(&#39;5d27eca6172d9aa762d49ef0&#39;),
  &#39;title&#39;: &#39;COMMITTEE OF SUPPLY ÃËâ HEAD P (MINISTRY OF HOME AFFAIRS)&#39;,
  &#39;sitting_date&#39;: datetime.datetime(2012, 3, 1, 0, 0),
  &#39;session_type&#39;: &#39;BUDGET&#39;}]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;While this chapter is pretty short, it covered the concept of vectorization and its applications. As you can see, Doc2Vec is a remarkable tool - especially when pared with recommendation systems such that we can now create semantically aware features that can then be used in classification models.&lt;/p&gt;

&lt;p&gt;For example, we can pair this with user preferences to generate a predictive model to access how likely a particular product would appeal to a certain user.&lt;/p&gt;

&lt;p&gt;In the next portion of the writeup, I will be touching upon topic modelling that is being used to generate insights on our existing corpus.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Capstone #2 Summarizer</title>
      <link>/post/capstone-summarizer/</link>
      <pubDate>Mon, 29 Jul 2019 15:59:29 +0800</pubDate>
      <guid>/post/capstone-summarizer/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;This is the second of a multi part series that details the processes behind &lt;a href=&#34;https://fparl.bearylogical.net&#34;&gt;FastParliament&lt;/a&gt;. You may view the previous post &lt;a href=&#34;/post/capstone-introduction/&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-to-summarization&#34;&gt;Introduction to Summarization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deconstructing-extractive-summarization&#34;&gt;Deconstructing Extractive Summarization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#walkthrough-of-extractive-summarization-process&#34;&gt;Walkthrough of Extractive Summarization Process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#import-libraries&#34;&gt;Import Libraries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#extractive-summarization-using-textrank&#34;&gt;Extractive Summarization using TextRank&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#step-0-import-in-sample-text&#34;&gt;Step 0 : Import in Sample Text&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-1-extract-sentences-from-sample&#34;&gt;Step 1 : Extract Sentences from sample&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-2-vectorizing-our-text&#34;&gt;Step 2 : Vectorizing our Text&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-3-build-similarity-matrix&#34;&gt;Step 3 : Build Similarity Matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-4-create-similarity-graph&#34;&gt;Step 4 : Create Similarity Graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-5-sorting-the-scores&#34;&gt;Step 5 : Sorting the Scores&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step-6-joining-our-extracted-summary-and-putting-it-together&#34;&gt;Step 6 : Joining our Extracted Summary and Putting it together&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#enter-gensim&#34;&gt;Enter Gensim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;&lt;/p&gt;

&lt;p&gt;For this post, I will be focusing on the summarizer component of &lt;a href=&#34;https://fparl.bearylogical.net/#summary&#34;&gt;FastParliament&lt;/a&gt;.&lt;/p&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/capstone-summarizer/summarizer_hub8df6580f032a69dcf05ed6607939a08_615320_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Text Summarizer in Fast Parliament&#34;&gt;


  &lt;img data-src=&#34;/post/capstone-summarizer/summarizer_hub8df6580f032a69dcf05ed6607939a08_615320_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;2546&#34; height=&#34;1356&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Text Summarizer in Fast Parliament
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;Our Parliamentarians tend to speak a fair bit.&lt;/p&gt;

&lt;p&gt;From preliminary exploratory analysis on the Hansard, it is probably safe to assume that there would be more, not less content to come in the future. With larger complexities as well as emerging challenges to the governance of the country, there will surely be plenty of things to talk about in the parliament.&lt;/p&gt;

&lt;p&gt;Furthermore, it is commonly agreed that people have limited attention spans. Matters of civil society are competing on the same playing field as the latest Korean drama or the antics of a certain orange-haired President.&lt;/p&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/capstone-summarizer/fparl-speech_hu04a2893a2de0ce6edf1eb2b621c99e0b_83269_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;/post/capstone-summarizer/fparl-speech_hu04a2893a2de0ce6edf1eb2b621c99e0b_83269_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1318&#34; height=&#34;964&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;


&lt;p&gt;With those considerations, being able to reduce contents of a text or summarize such that the reader gets the gist of the debate would clearly be advantageous.&lt;/p&gt;

&lt;h2 id=&#34;introduction-to-summarization&#34;&gt;Introduction to Summarization&lt;/h2&gt;

&lt;p&gt;As a generality, summarization consists of two core areas:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Abstractive Summary&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In abstractive summary, the summarized text often contain paraphrased representations of the original text.&lt;/p&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/capstone-summarizer/abstractive_hu4021c375527a1d29dcc9a39a402616d3_114795_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Abstractive text&#34;&gt;


  &lt;img data-src=&#34;/post/capstone-summarizer/abstractive_hu4021c375527a1d29dcc9a39a402616d3_114795_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1124&#34; height=&#34;456&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Abstractive text
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Extractive Summary&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Conversely, extractive summary preserves the content of the text by selecting phrases/sentences that best convey the information - focus on brevity.&lt;/p&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/capstone-summarizer/extractive_hueb03a5dab54226bbfd7deb22915a5636_104319_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Abstractive text&#34;&gt;


  &lt;img data-src=&#34;/post/capstone-summarizer/extractive_hueb03a5dab54226bbfd7deb22915a5636_104319_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1116&#34; height=&#34;446&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Abstractive text
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thus you may have observed that while those two methods are remarkably similar in supposed meaning, they have distinct flavors of summarization.&lt;/p&gt;

&lt;p&gt;Abstractive summarization - one in which &amp;ldquo;new&amp;rdquo; phrases are being created as a result of semantic parsing of the content, is closely linked to deep learning approaches. Current state of the art abstractive models use a combination of Recurrent Neural Networks (RNNs), Attention Mechanisms, Encoder-Decoder techniques to understand a text AND generate reduced length sentences (summaries) from the long-form text. The generative nature of abstractive summaries generally require large computational resources as well as large datasets to be able to train the generator.&lt;/p&gt;

&lt;p&gt;Extractive summarization, on the other hand, is far easier to deploy. The reason being that it is &amp;ldquo;&lt;a href=&#34;http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html?source=post_page---------------------------_&#34;&gt;far easier to select text than it is to generate from scratch&lt;/a&gt;&amp;rdquo;. Furthermore, to even create &amp;ldquo;understandable&amp;rdquo; summaries from the hansard debates, it may require computational resource and time beyond what would be alloted for the project.&lt;/p&gt;

&lt;p&gt;As such, extractive summarization techniques were selected as a core component for FastParliament&amp;rsquo;s Summarizer.&lt;/p&gt;

&lt;h2 id=&#34;deconstructing-extractive-summarization&#34;&gt;Deconstructing Extractive Summarization&lt;/h2&gt;

&lt;p&gt;In a nutshell below flow outlines the general workflow of extractive summarization.&lt;/p&gt;

&lt;p&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34; version=&#34;1.1&#34; width=&#34;529px&#34; viewBox=&#34;-0.5 -0.5 529 141&#34; content=&#34;&amp;lt;mxfile modified=&amp;quot;2019-07-31T00:48:46.830Z&amp;quot; host=&amp;quot;www.draw.io&amp;quot; agent=&amp;quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:68.0) Gecko/20100101 Firefox/68.0&amp;quot; etag=&amp;quot;a4GJHTKjJHTzM-OFi3Pk&amp;quot; version=&amp;quot;11.0.7&amp;quot; type=&amp;quot;device&amp;quot;&amp;gt;&amp;lt;diagram id=&amp;quot;paqVPKDwxPZ-d9Vrmngb&amp;quot; name=&amp;quot;Page-1&amp;quot;&amp;gt;3VhRk5owEP41Pl4ngig+tmhtZ9rOTX1o7zHCAukFloagcr++iQSB0nPsjCfnvTDsl12y+XY/EhjZXrJfCZrFXzEAPrJIsB/Zi5Fl2e5UXTVQVoBD3AqIBAsqaNwAa/YEBiQGLVgAecdRInLJsi7oY5qCLzsYFQJ3XbcQeXfWjEbQA9Y+5X30BwtkXKGuQxr8E7AormceEzOS0NrZAHlMA9y1IHs5sj2BKKu7ZO8B19zVvFRxH58ZPSYmIJXnBBRPUeHi6m7z4P0OHzH9Xr4f302qp2wpL8yCTbKyrBmAQBFiTBQyxghTypcN+kFgkQagpyHKany+IGYKHCvwF0hZmurSQqKCYplwM1rNqSd6dm0GyrEQPpxYkMlfUhGBPOFnHyugOhcwASlKFSeAU8m23Tyo6aHo6NfQrG4M0//ButVj/XOaFVJBC/SLRC/67yo0HGvCdjGTsM7ogYqdEl6Xz5Bx7iFHcYi1Q9cH31d4LgU+Qmtk4zoThxwrsAUhYX+6Bn3OTEDd52UtYGPvGtmMa5+4JZkpeSGWp2+tt+0ze9sZsrftHuvrjLNubxOWKposslYWpL56v1+03cPQ+ne7B9PN1JleqN1fXb/XW+Z1+xv2TP5s3T/oR71zjLXYmycfjLI2UrXeY5A2WlHabMIOVh13QS05Z2ppNqSWnJ6W7kGEKBIF3gvIBCrt5CyNblA/E/Lq9DPu0XhF/ZDb0s/sTP24Q+pn1tPPClIQVILefFjCOBVM6nzWPgq9DakPBKGuS+rHrf3pLcjLHVxegxy/blRe7pnymg8pL7cnLw8TvdqWoG5OOL1z3eDCmfdP1MD1vw6LfNPsNodolaPU761YZQW5dsjr99pFKzGfLchs9uKVuF4hlNn8iDmMtf5m2cs/&amp;lt;/diagram&amp;gt;&amp;lt;/mxfile&amp;gt;&#34; onclick=&#34;(function(svg){var src=window.event.target||window.event.srcElement;while (src!=null&amp;amp;&amp;amp;src.nodeName.toLowerCase()!=&#39;a&#39;){src=src.parentNode;}if(src==null){if(svg.wnd!=null&amp;amp;&amp;amp;!svg.wnd.closed){svg.wnd.focus();}else{var r=function(evt){if(evt.data==&#39;ready&#39;&amp;amp;&amp;amp;evt.source==svg.wnd){svg.wnd.postMessage(decodeURIComponent(svg.getAttribute(&#39;content&#39;)),&#39;*&#39;);window.removeEventListener(&#39;message&#39;,r);}};window.addEventListener(&#39;message&#39;,r);svg.wnd=window.open(&#39;https://www.draw.io/?client=1&amp;amp;lightbox=1&amp;amp;edit=_blank&#39;);}}})(this);&#34; style=&#34;cursor:pointer;max-width:100%;max-height:141px;&#34;&gt;&lt;defs/&gt;&lt;g&gt;&lt;path d=&#34;M 120 30 L 193.63 30&#34; fill=&#34;none&#34; stroke=&#34;#000000&#34; stroke-miterlimit=&#34;10&#34; pointer-events=&#34;none&#34;/&gt;&lt;path d=&#34;M 198.88 30 L 191.88 33.5 L 193.63 30 L 191.88 26.5 Z&#34; fill=&#34;#000000&#34; stroke=&#34;#000000&#34; stroke-miterlimit=&#34;10&#34; pointer-events=&#34;none&#34;/&gt;&lt;rect x=&#34;0&#34; y=&#34;0&#34; width=&#34;120&#34; height=&#34;60&#34; rx=&#34;9&#34; ry=&#34;9&#34; fill=&#34;#f8cecc&#34; stroke=&#34;#b85450&#34; pointer-events=&#34;none&#34;/&gt;&lt;g transform=&#34;translate(17.5,23.5)&#34;&gt;&lt;switch&gt;&lt;foreignObject style=&#34;overflow:visible;&#34; pointer-events=&#34;all&#34; width=&#34;85&#34; height=&#34;12&#34; requiredFeatures=&#34;http://www.w3.org/TR/SVG11/feature#Extensibility&#34;&gt;&lt;div xmlns=&#34;http://www.w3.org/1999/xhtml&#34; style=&#34;display: inline-block; font-size: 12px; font-family: &amp;quot;Helvetica&amp;quot;; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 86px; white-space: nowrap; overflow-wrap: normal; text-align: center;&#34;&gt;&lt;div xmlns=&#34;http://www.w3.org/1999/xhtml&#34; style=&#34;display:inline-block;text-align:inherit;text-decoration:inherit;white-space:normal;&#34;&gt;Input Document&lt;/div&gt;&lt;/div&gt;&lt;/foreignObject&gt;&lt;text x=&#34;43&#34; y=&#34;12&#34; fill=&#34;#000000&#34; text-anchor=&#34;middle&#34; font-size=&#34;12px&#34; font-family=&#34;Helvetica&#34;&gt;Input Document&lt;/text&gt;&lt;/switch&gt;&lt;/g&gt;&lt;path d=&#34;M 320 30 L 373.63 30&#34; fill=&#34;none&#34; stroke=&#34;#000000&#34; stroke-miterlimit=&#34;10&#34; pointer-events=&#34;none&#34;/&gt;&lt;path d=&#34;M 378.88 30 L 371.88 33.5 L 373.63 30 L 371.88 26.5 Z&#34; fill=&#34;#000000&#34; stroke=&#34;#000000&#34; stroke-miterlimit=&#34;10&#34; pointer-events=&#34;none&#34;/&gt;&lt;rect x=&#34;200&#34; y=&#34;0&#34; width=&#34;120&#34; height=&#34;60&#34; rx=&#34;9&#34; ry=&#34;9&#34; fill=&#34;#fff2cc&#34; stroke=&#34;#d6b656&#34; pointer-events=&#34;none&#34;/&gt;&lt;g transform=&#34;translate(201.5,16.5)&#34;&gt;&lt;switch&gt;&lt;foreignObject style=&#34;overflow:visible;&#34; pointer-events=&#34;all&#34; width=&#34;116&#34; height=&#34;27&#34; requiredFeatures=&#34;http://www.w3.org/TR/SVG11/feature#Extensibility&#34;&gt;&lt;div xmlns=&#34;http://www.w3.org/1999/xhtml&#34; style=&#34;display: inline-block; font-size: 12px; font-family: &amp;quot;Helvetica&amp;quot;; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 116px; white-space: nowrap; overflow-wrap: normal; text-align: center;&#34;&gt;&lt;div xmlns=&#34;http://www.w3.org/1999/xhtml&#34; style=&#34;display:inline-block;text-align:inherit;text-decoration:inherit;white-space:normal;&#34;&gt;Split Document into Sentences&lt;/div&gt;&lt;/div&gt;&lt;/foreignObject&gt;&lt;text x=&#34;58&#34; y=&#34;20&#34; fill=&#34;#000000&#34; text-anchor=&#34;middle&#34; font-size=&#34;12px&#34; font-family=&#34;Helvetica&#34;&gt;Split Document into Sentences&lt;/text&gt;&lt;/switch&gt;&lt;/g&gt;&lt;path d=&#34;M 500 30 L 520 30 L 520 110 L 506.37 110&#34; fill=&#34;none&#34; stroke=&#34;#000000&#34; stroke-miterlimit=&#34;10&#34; pointer-events=&#34;none&#34;/&gt;&lt;path d=&#34;M 501.12 110 L 508.12 106.5 L 506.37 110 L 508.12 113.5 Z&#34; fill=&#34;#000000&#34; stroke=&#34;#000000&#34; stroke-miterlimit=&#34;10&#34; pointer-events=&#34;none&#34;/&gt;&lt;rect x=&#34;380&#34; y=&#34;0&#34; width=&#34;120&#34; height=&#34;60&#34; rx=&#34;9&#34; ry=&#34;9&#34; fill=&#34;#fff2cc&#34; stroke=&#34;#d6b656&#34; pointer-events=&#34;none&#34;/&gt;&lt;g transform=&#34;translate(381.5,16.5)&#34;&gt;&lt;switch&gt;&lt;foreignObject style=&#34;overflow:visible;&#34; pointer-events=&#34;all&#34; width=&#34;116&#34; height=&#34;27&#34; requiredFeatures=&#34;http://www.w3.org/TR/SVG11/feature#Extensibility&#34;&gt;&lt;div xmlns=&#34;http://www.w3.org/1999/xhtml&#34; style=&#34;display: inline-block; font-size: 12px; font-family: &amp;quot;Helvetica&amp;quot;; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 116px; white-space: nowrap; overflow-wrap: normal; text-align: center;&#34;&gt;&lt;div xmlns=&#34;http://www.w3.org/1999/xhtml&#34; style=&#34;display:inline-block;text-align:inherit;text-decoration:inherit;white-space:normal;&#34;&gt;Perform Preprocessing&lt;/div&gt;&lt;/div&gt;&lt;/foreignObject&gt;&lt;text x=&#34;58&#34; y=&#34;20&#34; fill=&#34;#000000&#34; text-anchor=&#34;middle&#34; font-size=&#34;12px&#34; font-family=&#34;Helvetica&#34;&gt;Perform Preprocessing&lt;/text&gt;&lt;/switch&gt;&lt;/g&gt;&lt;path d=&#34;M 380 110 L 326.37 110&#34; fill=&#34;none&#34; stroke=&#34;#000000&#34; stroke-miterlimit=&#34;10&#34; pointer-events=&#34;none&#34;/&gt;&lt;path d=&#34;M 321.12 110 L 328.12 106.5 L 326.37 110 L 328.12 113.5 Z&#34; fill=&#34;#000000&#34; stroke=&#34;#000000&#34; stroke-miterlimit=&#34;10&#34; pointer-events=&#34;none&#34;/&gt;&lt;rect x=&#34;380&#34; y=&#34;80&#34; width=&#34;120&#34; height=&#34;60&#34; rx=&#34;9&#34; ry=&#34;9&#34; fill=&#34;#fff2cc&#34; stroke=&#34;#d6b656&#34; pointer-events=&#34;none&#34;/&gt;&lt;g transform=&#34;translate(381.5,89.5)&#34;&gt;&lt;switch&gt;&lt;foreignObject style=&#34;overflow:visible;&#34; pointer-events=&#34;all&#34; width=&#34;116&#34; height=&#34;41&#34; requiredFeatures=&#34;http://www.w3.org/TR/SVG11/feature#Extensibility&#34;&gt;&lt;div xmlns=&#34;http://www.w3.org/1999/xhtml&#34; style=&#34;display: inline-block; font-size: 12px; font-family: &amp;quot;Helvetica&amp;quot;; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 116px; white-space: nowrap; overflow-wrap: normal; text-align: center;&#34;&gt;&lt;div xmlns=&#34;http://www.w3.org/1999/xhtml&#34; style=&#34;display:inline-block;text-align:inherit;text-decoration:inherit;white-space:normal;&#34;&gt;Generate Similarity Scores for Each Sentence&lt;/div&gt;&lt;/div&gt;&lt;/foreignObject&gt;&lt;text x=&#34;58&#34; y=&#34;27&#34; fill=&#34;#000000&#34; text-anchor=&#34;middle&#34; font-size=&#34;12px&#34; font-family=&#34;Helvetica&#34;&gt;Generate Similarity Scores for Each Sentence&lt;/text&gt;&lt;/switch&gt;&lt;/g&gt;&lt;path d=&#34;M 200 110 L 126.37 110&#34; fill=&#34;none&#34; stroke=&#34;#000000&#34; stroke-miterlimit=&#34;10&#34; pointer-events=&#34;none&#34;/&gt;&lt;path d=&#34;M 121.12 110 L 128.12 106.5 L 126.37 110 L 128.12 113.5 Z&#34; fill=&#34;#000000&#34; stroke=&#34;#000000&#34; stroke-miterlimit=&#34;10&#34; pointer-events=&#34;none&#34;/&gt;&lt;rect x=&#34;200&#34; y=&#34;80&#34; width=&#34;120&#34; height=&#34;60&#34; rx=&#34;9&#34; ry=&#34;9&#34; fill=&#34;#fff2cc&#34; stroke=&#34;#d6b656&#34; pointer-events=&#34;none&#34;/&gt;&lt;g transform=&#34;translate(214.5,103.5)&#34;&gt;&lt;switch&gt;&lt;foreignObject style=&#34;overflow:visible;&#34; pointer-events=&#34;all&#34; width=&#34;90&#34; height=&#34;12&#34; requiredFeatures=&#34;http://www.w3.org/TR/SVG11/feature#Extensibility&#34;&gt;&lt;div xmlns=&#34;http://www.w3.org/1999/xhtml&#34; style=&#34;display: inline-block; font-size: 12px; font-family: &amp;quot;Helvetica&amp;quot;; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 91px; white-space: nowrap; overflow-wrap: normal; text-align: center;&#34;&gt;&lt;div xmlns=&#34;http://www.w3.org/1999/xhtml&#34; style=&#34;display:inline-block;text-align:inherit;text-decoration:inherit;white-space:normal;&#34;&gt;Compare Scores&lt;/div&gt;&lt;/div&gt;&lt;/foreignObject&gt;&lt;text x=&#34;45&#34; y=&#34;12&#34; fill=&#34;#000000&#34; text-anchor=&#34;middle&#34; font-size=&#34;12px&#34; font-family=&#34;Helvetica&#34;&gt;Compare Scores&lt;/text&gt;&lt;/switch&gt;&lt;/g&gt;&lt;rect x=&#34;0&#34; y=&#34;80&#34; width=&#34;120&#34; height=&#34;60&#34; rx=&#34;9&#34; ry=&#34;9&#34; fill=&#34;#97d077&#34; stroke=&#34;#d6b656&#34; pointer-events=&#34;none&#34;/&gt;&lt;g transform=&#34;translate(1.5,96.5)&#34;&gt;&lt;switch&gt;&lt;foreignObject style=&#34;overflow:visible;&#34; pointer-events=&#34;all&#34; width=&#34;116&#34; height=&#34;27&#34; requiredFeatures=&#34;http://www.w3.org/TR/SVG11/feature#Extensibility&#34;&gt;&lt;div xmlns=&#34;http://www.w3.org/1999/xhtml&#34; style=&#34;display: inline-block; font-size: 12px; font-family: &amp;quot;Helvetica&amp;quot;; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 116px; white-space: nowrap; overflow-wrap: normal; text-align: center;&#34;&gt;&lt;div xmlns=&#34;http://www.w3.org/1999/xhtml&#34; style=&#34;display:inline-block;text-align:inherit;text-decoration:inherit;white-space:normal;&#34;&gt;Select N sentences with highest scores &lt;/div&gt;&lt;/div&gt;&lt;/foreignObject&gt;&lt;text x=&#34;58&#34; y=&#34;20&#34; fill=&#34;#000000&#34; text-anchor=&#34;middle&#34; font-size=&#34;12px&#34; font-family=&#34;Helvetica&#34;&gt;Select N sentences with highest scores &lt;/text&gt;&lt;/switch&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/p&gt;

&lt;h2 id=&#34;walkthrough-of-extractive-summarization-process&#34;&gt;Walkthrough of Extractive Summarization Process&lt;/h2&gt;

&lt;p&gt;I will go through a simple conceptualization of one implementation of extractive summarization using the &lt;a href=&#34;https://www.aclweb.org/anthology/W04-3252&#34;&gt;TextRank&lt;/a&gt; approach first conceptualised by Rada Mihalcea and Paul Tarau in 2004.&lt;/p&gt;

&lt;h2 id=&#34;import-libraries&#34;&gt;Import Libraries&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import re

import datetime
import numpy as np
import pandas as pd
from datetime import date 

###
import pymongo

### NLP

import gensim
import spacy

### Custom Utils
from text_utils.metrics import get_chunks_info, get_metric

 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;extractive-summarization-using-textrank&#34;&gt;Extractive Summarization using TextRank&lt;/h2&gt;

&lt;p&gt;Source : &lt;a href=&#34;https://www.aclweb.org/anthology/W04-3252&#34;&gt;https://www.aclweb.org/anthology/W04-3252&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from nltk.corpus import stopwords
import matplotlib.pyplot as plt
import seaborn as sns
import pprint

import networkx as nx
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-0-import-in-sample-text&#34;&gt;Step 0 : Import in Sample Text&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sample_text = mongo_df.iloc[1300].cleaned_join
display(sample_text)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&#39;Mr Lim Biow Chuan asked the Minister for National Development (a) whether a traffic impact assessment had been carried out prior to having the construction staging ground located at Marina East; and (b) when will this temporary staging ground be relocated.&amp;lt;br/&amp;gt;Mr Lawrence Wong: The temporary occupation licence (TOL) for the construction staging ground at Marina East was first issued to LTA in 2014, before activities in the area generated significant traffic impact. In 2016, arising from new TOL applications in the East Coast/Marina Bay area, a joint Traffic Impact Assessment (TIA) was carried out for the TOLs in the East Coast/Marina Bay area, as they share the same access road. The Marina East staging ground is segregated from the existing residential area by East Coast Parkway and East Coast Park. Nevertheless, LTA has adopted measures to minimise dis-amenities to road users in the vicinity. For example, queue bays are provided in the staging ground, and throughput has been enhanced so that heavy vehicles do not overflow into public roads. LTA is also working closely with the developers and contractors in the area to develop localised traffic control plans to improve safety and minimise inconvenience to other road users. These include managing the schedules and routes of heavy vehicles to avoid peak hour traffic and residential areas, where possible. There are also signs to alert motorists to slow down and watch out for heavy vehicles.From a land-use perspective, the Marina East staging ground currently supports LTAâs rail and road infrastructure projects. When these projects are completed, we will review whether to continue the use of this staging ground, in connection with the timing of future development plans for the area. &#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-1-extract-sentences-from-sample&#34;&gt;Step 1 : Extract Sentences from sample&lt;/h2&gt;

&lt;p&gt;In this step, we will be splitting the text into sentences by using python&amp;rsquo;s &lt;code&gt;.split()&lt;/code&gt; method. Additionally, we will be doing some pre-processing as part of the process which includes removing stop words as well as stemming.&lt;/p&gt;

&lt;p&gt;The function &lt;code&gt;process_text&lt;/code&gt; takes in a string of text (or in this case, the document) and then outputs a list of post-processed strings where each item on the list is a processed string.&lt;/p&gt;

&lt;p&gt;Furthermore, we will ignore the text at index 0 as this is typically the question statement and we are only interested in responses.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stop_words = set(stopwords.words(&amp;quot;english&amp;quot;))

def process_text(text,stopwords = None):
    text = text.replace(&#39;&amp;lt;br/&amp;gt;&#39;,&#39; &#39;)
    split_text = text.split(&#39;.&#39;)
    sentences = [sentence for sentence in split_text if len(sentence) &amp;gt; 1]
    
    process_sentences = []
    
    ## remove stopwords
    for sentence in sentences:
        words = sentence.split()
        processed_words = [word.lower() for word in words if word not in stopwords]
        process_sentences.append(&amp;quot; &amp;quot;.join(processed_words))
    
    return process_sentences[1:]

processed_text = process_text(sample_text,stop_words)
display(processed_text)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[&#39;mr lawrence wong: the temporary occupation licence (tol) construction staging ground marina east first issued lta 2014, activities area generated significant traffic impact&#39;,
 &#39;in 2016, arising new tol applications east coast/marina bay area, joint traffic impact assessment (tia) carried tols east coast/marina bay area, share access road&#39;,
 &#39;the marina east staging ground segregated existing residential area east coast parkway east coast park&#39;,
 &#39;nevertheless, lta adopted measures minimise dis-amenities road users vicinity&#39;,
 &#39;for example, queue bays provided staging ground, throughput enhanced heavy vehicles overflow public roads&#39;,
 &#39;lta also working closely developers contractors area develop localised traffic control plans improve safety minimise inconvenience road users&#39;,
 &#39;these include managing schedules routes heavy vehicles avoid peak hour traffic residential areas, possible&#39;,
 &#39;there also signs alert motorists slow watch heavy vehicles&#39;,
 &#39;from land-use perspective, marina east staging ground currently supports ltaâs rail road infrastructure projects&#39;,
 &#39;when projects completed, review whether continue use staging ground, connection timing future development plans area&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-2-vectorizing-our-text&#34;&gt;Step 2 : Vectorizing our Text&lt;/h2&gt;

&lt;p&gt;In this step, the sentences are then converted to Term-Frequency Inverse Document Frequency (TF-IDF). TF-IDF is used in this case over the Bag of Words (BoW) approach to penalise frequently occuring words within the same text. In plain speak, it&amp;rsquo;s similar to assigning weights over the occurances of a certain word. If the word occurs more frequently, then the value is lower. Likewise, rarely accuring words are then given higher values.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
sent_vectors = vectorizer.fit_transform(processed_text)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To better understand what is happening below the hood, a dataframe containing the vectors mapped to their associated word is shown below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.DataFrame(sent_vectors.todense().tolist(),columns=vectorizer.get_feature_names())
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;2014&lt;/th&gt;
      &lt;th&gt;2016&lt;/th&gt;
      &lt;th&gt;access&lt;/th&gt;
      &lt;th&gt;activities&lt;/th&gt;
      &lt;th&gt;adopted&lt;/th&gt;
      &lt;th&gt;alert&lt;/th&gt;
      &lt;th&gt;also&lt;/th&gt;
      &lt;th&gt;amenities&lt;/th&gt;
      &lt;th&gt;applications&lt;/th&gt;
      &lt;th&gt;area&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;traffic&lt;/th&gt;
      &lt;th&gt;use&lt;/th&gt;
      &lt;th&gt;users&lt;/th&gt;
      &lt;th&gt;vehicles&lt;/th&gt;
      &lt;th&gt;vicinity&lt;/th&gt;
      &lt;th&gt;watch&lt;/th&gt;
      &lt;th&gt;when&lt;/th&gt;
      &lt;th&gt;whether&lt;/th&gt;
      &lt;th&gt;wong&lt;/th&gt;
      &lt;th&gt;working&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.235868&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.235868&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.140063&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.155963&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.235868&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.19567&lt;/td&gt;
      &lt;td&gt;0.19567&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.19567&lt;/td&gt;
      &lt;td&gt;0.232386&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.129383&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.160048&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.346693&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.346693&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.294720&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.346693&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.216402&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.222823&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.155651&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.173319&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.222823&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.262117&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.188798&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.212354&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.357395&lt;/td&gt;
      &lt;td&gt;0.303819&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.265806&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.357395&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.257492&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.00000&lt;/td&gt;
      &lt;td&gt;0.169831&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.243123&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.285997&lt;/td&gt;
      &lt;td&gt;0.285997&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;10 rows Ã 109 columns&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&#34;step-3-build-similarity-matrix&#34;&gt;Step 3 : Build Similarity Matrix&lt;/h2&gt;

&lt;p&gt;In this step, a similarity matrix is created by applying a dot product with the sentence vector and its transpose.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sim_matrix = np.round(np.dot(sent_vectors, sent_vectors.T).A,3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again, we display this matrix as part of a correlation table.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;corr = pd.DataFrame(columns=range(len(processed_text)),index=range(len(processed_text)),data=sim_matrix)
display(corr)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;th&gt;9&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.000&lt;/td&gt;
      &lt;td&gt;0.200&lt;/td&gt;
      &lt;td&gt;0.224&lt;/td&gt;
      &lt;td&gt;0.036&lt;/td&gt;
      &lt;td&gt;0.048&lt;/td&gt;
      &lt;td&gt;0.076&lt;/td&gt;
      &lt;td&gt;0.029&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.144&lt;/td&gt;
      &lt;td&gt;0.071&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.200&lt;/td&gt;
      &lt;td&gt;1.000&lt;/td&gt;
      &lt;td&gt;0.374&lt;/td&gt;
      &lt;td&gt;0.030&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.081&lt;/td&gt;
      &lt;td&gt;0.024&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.130&lt;/td&gt;
      &lt;td&gt;0.039&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0.224&lt;/td&gt;
      &lt;td&gt;0.374&lt;/td&gt;
      &lt;td&gt;1.000&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.055&lt;/td&gt;
      &lt;td&gt;0.025&lt;/td&gt;
      &lt;td&gt;0.056&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.200&lt;/td&gt;
      &lt;td&gt;0.082&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0.036&lt;/td&gt;
      &lt;td&gt;0.030&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;1.000&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.211&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.092&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.048&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.055&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;1.000&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.092&lt;/td&gt;
      &lt;td&gt;0.115&lt;/td&gt;
      &lt;td&gt;0.062&lt;/td&gt;
      &lt;td&gt;0.059&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;0.076&lt;/td&gt;
      &lt;td&gt;0.081&lt;/td&gt;
      &lt;td&gt;0.025&lt;/td&gt;
      &lt;td&gt;0.211&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;1.000&lt;/td&gt;
      &lt;td&gt;0.033&lt;/td&gt;
      &lt;td&gt;0.068&lt;/td&gt;
      &lt;td&gt;0.069&lt;/td&gt;
      &lt;td&gt;0.081&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;0.029&lt;/td&gt;
      &lt;td&gt;0.024&lt;/td&gt;
      &lt;td&gt;0.056&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.092&lt;/td&gt;
      &lt;td&gt;0.033&lt;/td&gt;
      &lt;td&gt;1.000&lt;/td&gt;
      &lt;td&gt;0.113&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.115&lt;/td&gt;
      &lt;td&gt;0.068&lt;/td&gt;
      &lt;td&gt;0.113&lt;/td&gt;
      &lt;td&gt;1.000&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;0.144&lt;/td&gt;
      &lt;td&gt;0.130&lt;/td&gt;
      &lt;td&gt;0.200&lt;/td&gt;
      &lt;td&gt;0.092&lt;/td&gt;
      &lt;td&gt;0.062&lt;/td&gt;
      &lt;td&gt;0.069&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;1.000&lt;/td&gt;
      &lt;td&gt;0.186&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;0.071&lt;/td&gt;
      &lt;td&gt;0.039&lt;/td&gt;
      &lt;td&gt;0.082&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.059&lt;/td&gt;
      &lt;td&gt;0.081&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
      &lt;td&gt;0.186&lt;/td&gt;
      &lt;td&gt;1.000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;And if we wanted to visualise it using seaborn:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(8, 8))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

sns.heatmap(corr, mask=mask, cmap=cmap,annot=True, vmax=0.4, center=0,
            square=True, linewidths=.5, cbar_kws={&amp;quot;shrink&amp;quot;: .5})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_25_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can already see how sentences are similar to each other just by comparing their positions in their table. For example, we see that sentence 1 and 2 are very similar.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(f&amp;quot;Sentence 1 : {processed_text[2]}&amp;quot;)
print(f&amp;quot;Sentence 2 : {processed_text[3]}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Sentence 1 : the marina east staging ground segregated existing residential area east coast parkway east coast park
Sentence 2 : nevertheless, lta adopted measures minimise dis-amenities road users vicinity
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-4-create-similarity-graph&#34;&gt;Step 4 : Create Similarity Graph&lt;/h2&gt;

&lt;p&gt;Using the TextRank concept, a network graph is created such that each vertex is the sentence represented by its index, and the edges are linked to each other by weights computed by the similarity scores.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The basic idea implemented by a  graph-based ranking model is that of âvotingâorârecommendationâ. When one vertex links to another one,it is basically casting a vote for that other vertex. The higher the number of votes that are cast for a vertex,the higher the importance of the vertex. Moreover, the importance of the vertex casting the vote determines how important the vote itself is,and this information is also taken into account by the ranking model. Hence,the score associated with a vertex is determined based on the votes that are cast for it,and the score of the vertices casting these votes. (&lt;a href=&#34;https://www.aclweb.org/anthology/W04-3252&#34;&gt;Mihalcea &amp;amp; Tarau, 2004&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To compute this graph-derived score which is refered to as &lt;code&gt;TextRank&lt;/code&gt; - similar to Google&amp;rsquo;s PageRank, we import the popular &lt;code&gt;networkx&lt;/code&gt; package. With network x, we can visualise the similarity matrix as a graph. And using this, we can see&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sentence_similarity_graph = nx.from_numpy_array(sim_matrix)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pos = nx.circular_layout(sentence_similarity_graph)
plt.figure(figsize=(8,8))
plt.figure(1)
nx.draw_circular(sentence_similarity_graph, with_labels = True, node_color=&amp;quot;Orange&amp;quot;)
edge_labels = nx.get_edge_attributes(sentence_similarity_graph, &#39;weight&#39;)
plt.figure(1)
nx.draw_networkx_edge_labels(sentence_similarity_graph,pos=pos, edge_labels=edge_labels)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_30_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;scores = nx.pagerank(sentence_similarity_graph)
display(scores)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;{0: 0.10594604557676923,
 1: 0.10711030046429679,
 2: 0.11410080866901795,
 3: 0.0893971718098718,
 4: 0.09467680498110888,
 5: 0.1019626373096041,
 6: 0.09218045190228014,
 7: 0.09190095545516967,
 8: 0.10932632179093772,
 9: 0.09339850204094387}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-5-sorting-the-scores&#34;&gt;Step 5 : Sorting the Scores&lt;/h2&gt;

&lt;p&gt;Once we have obtained the scores, we then look at picking the top n sentences that have the highest scores to best represent the text. In this case, we select the top &lt;strong&gt;3&lt;/strong&gt; sentences.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;original_sentences = [sent for sent in sample_text.replace(&#39;&amp;lt;br/&amp;gt;&#39;,&#39; &#39;).split(&#39;. &#39;) if len(sent)&amp;gt;1]

sorted_sentences = sorted(((scores[idx], sentence) for idx,sentence in enumerate(original_sentences[1:])), reverse=True)
display(sorted_sentences[:3])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[(0.11410080866901795,
  &#39;The Marina East staging ground is segregated from the existing residential area by East Coast Parkway and East Coast Park&#39;),
 (0.10932632179093772,
  &#39;When these projects are completed, we will review whether to continue the use of this staging ground, in connection with the timing of future development plans for the area&#39;),
 (0.10711030046429679,
  &#39;In 2016, arising from new TOL applications in the East Coast/Marina Bay area, a joint Traffic Impact Assessment (TIA) was carried out for the TOLs in the East Coast/Marina Bay area, as they share the same access road&#39;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-6-joining-our-extracted-summary-and-putting-it-together&#34;&gt;Step 6 : Joining our Extracted Summary and Putting it together&lt;/h2&gt;

&lt;p&gt;In this step, we will display our Question and Summarised response together to give us a feel of what the end product is.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;. &amp;quot;.join([sent[1] for sent in sorted_sentences[:3]])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&#39;The Marina East staging ground is segregated from the existing residential area by East Coast Parkway and East Coast Park. When these projects are completed, we will review whether to continue the use of this staging ground, in connection with the timing of future development plans for the area. In 2016, arising from new TOL applications in the East Coast/Marina Bay area, a joint Traffic Impact Assessment (TIA) was carried out for the TOLs in the East Coast/Marina Bay area, as they share the same access road&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(f&amp;quot;Question : {original_sentences[0]}&amp;quot;)
print(f&amp;quot;\nResponse : {&#39;. &#39;.join([sent[1] for sent in sorted_sentences[0:2]])}.&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Question : Mr Lim Biow Chuan asked the Minister for National Development (a) whether a traffic impact assessment had been carried out prior to having the construction staging ground located at Marina East; and (b) when will this temporary staging ground be relocated

Response : The Marina East staging ground is segregated from the existing residential area by East Coast Parkway and East Coast Park. When these projects are completed, we will review whether to continue the use of this staging ground, in connection with the timing of future development plans for the area.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you may already see, it is somewhat faithful to the original text. At the same time, it is worth noting that this is a rudimentary implementation of TextRank.&lt;/p&gt;

&lt;p&gt;Many variations of TextRank exist which look at various similarity functions and text vectorisation techniques to improve the summarization capability. One such implementation is by &lt;a href=&#34;https://arxiv.org/pdf/1602.03606.pdf&#34;&gt;Barrios et al, 2016&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This implementation incorporated the following changes to the steps:
1. Choosing Longest Common Substring
2. BM25 Ranking model (variation of TF-IDF model using probabilistic )&lt;/p&gt;

&lt;p&gt;Which you can read more in the paper linked above.&lt;/p&gt;

&lt;p&gt;Consequentially, this variation is baked into the popular &lt;code&gt;Gensim&lt;/code&gt; package. As a result, we can call out this function very easily as you can see below.&lt;/p&gt;

&lt;h2 id=&#34;enter-gensim&#34;&gt;Enter Gensim&lt;/h2&gt;

&lt;p&gt;We can use the summarizer function located in the &lt;code&gt;summarization.summarizer&lt;/code&gt; module.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(gensim.summarization.summarizer.summarize(sample_text.replace(&#39;&amp;lt;br/&amp;gt;&#39;,&#39;&#39;), ratio=0.3,
                                          word_count=None, split=False))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Mr Lim Biow Chuan asked the Minister for National Development (a) whether a traffic impact assessment had been carried out prior to having the construction staging ground located at Marina East; and (b) when will this temporary staging ground be relocated.Mr Lawrence Wong: The temporary occupation licence (TOL) for the construction staging ground at Marina East was first issued to LTA in 2014, before activities in the area generated significant traffic impact.
LTA is also working closely with the developers and contractors in the area to develop localised traffic control plans to improve safety and minimise inconvenience to other road users.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It looks pretty good! This algorithm currently drives the summarizer aspect of FastParliament ð.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;To recap, we have gone through briefly the key areas of summarization. Following that, we went in-depth at a particular approach of extractive text summarization in which we used the TextRank method to determine sentence importance. Lastly, I showed a variant of text rank that FastParliament uses.&lt;/p&gt;

&lt;p&gt;For the next part of the writeup, I will be going through the article to article content discovery mechanism that makes use of Doc2Vec. Stay Tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Capstone #1 Introduction</title>
      <link>/post/capstone-introduction/</link>
      <pubDate>Mon, 24 Jun 2019 22:35:45 +0800</pubDate>
      <guid>/post/capstone-introduction/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;This is the first of a multi part series that details the processes behind &lt;a href=&#34;https://fparl.bearylogical.net&#34;&gt;FastParliament&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#motivation&#34;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-gathering&#34;&gt;Data Gathering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-storage&#34;&gt;Data Storage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#preliminary-data-cleaning&#34;&gt;Preliminary Data Cleaning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;&lt;/p&gt;

&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;As part of General Assembly&amp;rsquo;s Data Science Immersive 3 month course that I was enrolled in, each student was tasked to produce a capstone project. In a nutshell, the capstone is meant to showcase the various aspects of the data science process that was taught throughout the 3 month course.&lt;/p&gt;

&lt;p&gt;During the inital ideation proccess, I mainly focused on projects that:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Was relatable to the public&lt;/li&gt;
&lt;li&gt;Had an interactive element built in&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;From these two criteria, I came up with two initial ideas.&lt;/p&gt;

&lt;p&gt;The first idea was a computer vision related project. The broader concept was that I wanted to design a system to identify individuals based on their gait. While I didn&amp;rsquo;t end up going through with this project, I have written a &lt;a href=&#34;http://&#34;&gt;short write-up&lt;/a&gt; that documented my time with this project as well as some of key learning.&lt;/p&gt;

&lt;p&gt;The second idea, which was the one that I went ahead with - was to implement and deploy a summarizer for parliamentary speeches. More importantly, it stemmed from my interest to inject some &amp;lsquo;fresh&amp;rsquo; conversations into the parliamentary debates. Accordingly, I wanted to generate new insights on the unstructured parliamentary corpus.&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This post forms the first of a multi-part series that aims to provide you, the reader, with a broader understanding of the various processes and approaches that went into my capstone, FastParliament.&lt;/p&gt;

&lt;p&gt;Ideally, I wanted to make the experience of viewing parliamentary debates more interesting , as well as uncover insights into the documents with Natural Language Processing (NLP) methods. Namely, I found three possible areas of improvement when compared to the existing solution:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Reducing verbosity of existing debates.&lt;/li&gt;
&lt;li&gt;Understand how certain topics evolve over time.&lt;/li&gt;
&lt;li&gt;Find a better way to recommend related documents beyond keyword searches.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Also, I acknowledge that this is not the first time something like this has been done. &lt;a href=&#34;https://wiki.smu.edu.sg/1516t1isss608g1/ISSS608_2015_16T1_Group2_Report&#34;&gt;Hansard Browser&lt;/a&gt; by a team of SMU &lt;a href=&#34;https://mothership.sg/2016/03/heres-how-you-judge-how-hardworking-your-mp-was-in-singapores-12th-parliament/&#34;&gt;post-grads&lt;/a&gt; in 2015 has a similar flavor.&lt;/p&gt;

&lt;p&gt;In my implementation, I wanted to integrate a summarizer, recommender system as well as topic modeling using more recent technologies such as Text Rank, Doc2Vec and LDA - which I will elaborate in the subsequent posts.&lt;/p&gt;

&lt;p&gt;In this first post, I will touch on:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Data Gathering&lt;/li&gt;
&lt;li&gt;Data Storage Architecture &amp;amp; DB normalisation&lt;/li&gt;
&lt;li&gt;Preliminary Data Cleaning&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;data-gathering&#34;&gt;Data Gathering&lt;/h2&gt;

&lt;p&gt;As part of the objectives of generating insights into the existing parliamentary debates, data had to be collected from the &lt;a href=&#34;https://sprs.parl.gov.sg/search/home/&#34;&gt;hansard&lt;/a&gt;. Interestingly (or not), the Singapore Hansard does not have an easily callable API to get data. While one can request for documents either by sitting or member of parliament, it made data gathering cumbersome if done manually.&lt;/p&gt;

&lt;p&gt;I then had to use a combination of selenium and requests, to get the data.&lt;/p&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    While webscraping can be a relatively harmless activity, do be careful to set reasonable request rates to ensure you don&amp;rsquo;t bring down websites unintentionally!
  &lt;/div&gt;
&lt;/div&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/capstone-introduction/scraper_hufed217480b98c9d6d2f6498758c7e534_270577_2000x2000_fit_lanczos.gif&#34; &gt;


  &lt;img data-src=&#34;/post/capstone-introduction/scraper_hufed217480b98c9d6d2f6498758c7e534_270577_2000x2000_fit_lanczos.gif&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;480&#34; height=&#34;270&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;


&lt;p&gt;Interestingly, during the scraping process itself I discovered that there were distinct changes in the way the documents were being served to the end user. There were inconsistencies in the document URL and the HTML formatting of the actual document that was being displayed. While it was not very obvious to the end-user, it made scraping tough as various exception handling methods had to be implemented to ensure that the scraping could continue uninterrupted.&lt;/p&gt;

&lt;h2 id=&#34;data-storage&#34;&gt;Data Storage&lt;/h2&gt;

&lt;p&gt;Next, I had to figure out a way to store the documents that was being collected by the hansard scraper. While a simple approach was to append a pandas dataframe continuously for each document that was successfully scraped, it may not be a scalable approach for a large document corpus like the hansard. Futhermore, organising the dataframe by metadata may be challenging down the road.&lt;/p&gt;

&lt;p&gt;With these considerations, I then looked at postgreSQL as a solution to my requirements. Thereafter, I devised a schema (below) that seeks to best capture the essence of how I wanted my documents to be managed.&lt;/p&gt;

&lt;p&gt;As a generality, since I wanted to capture relationships within each document, as well as tie such relationships in to the actual member of parliament - there had to encompass multiple tables with one to many and many to many relationshps to do so.&lt;/p&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/capstone-introduction/schema_hu1003f2eb9b80365f677b60b9b0a3304f_207997_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;/post/capstone-introduction/schema_hu1003f2eb9b80365f677b60b9b0a3304f_207997_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1134&#34; height=&#34;1116&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;


&lt;p&gt;As you can see above, I applied various database normalisation approaches to ensure that the data didn&amp;rsquo;t look like a giant excel table ð¤£.&lt;/p&gt;

&lt;p&gt;Once the database was created, I used &lt;a href=&#34;https://www.sqlalchemy.org/&#34;&gt;SQLAlchemy&lt;/a&gt; as a wrapper to allow the scraper to interface with the database - ensuring that the document insertions also captured the various relationships I highlighted above. Object Relational Mapper (ORMs) such as SQLAlchemy abstract out the grittier aspects of database interactions.&lt;/p&gt;

&lt;h2 id=&#34;preliminary-data-cleaning&#34;&gt;Preliminary Data Cleaning&lt;/h2&gt;

&lt;p&gt;To narrow the scope of the project, I chose to focus on a ten-year period of 2009 to 2019. Once that was done, I wanted to get a sense of how the meta-data of the Hansard was distributed.&lt;/p&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/capstone-introduction/value_counts_hansard_hub8e4f1ca384b9720597436a522212e58_189850_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;/post/capstone-introduction/value_counts_hansard_hub8e4f1ca384b9720597436a522212e58_189850_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1318&#34; height=&#34;908&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;


&lt;p&gt;As you probably guessed, the data coming out doesn&amp;rsquo;t really look very nice. We see plenty of duplicates and certain formatting quirks. I then applied various cleaning techniques as you an see below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Convert to all upper case
df.session_type = df.session_type.map(lambda x : x.upper())
# Shorten any clarification XXXX to CLARIFICATION
df.session_type = df.session_type.map(lambda x : &#39;CLARIFICATION&#39; if re.search(&#39;CLARIFICATION&#39;,x) else x)
# Convert ATBP to &#39;ASSENTS TO BILLS PASSED&#39;
df.session_type = df.session_type.map(lambda x : &#39;ASSENTS TO BILLS PASSED&#39; if re.search(&#39;ATBP&#39;,x) else x)
# remove \t\r\n
df.session_type = df.session_type.map(lambda x : re.sub(&#39;\t|\r\n&#39;,&#39;&#39;,x))
# Convert all variations of written answers to WRITTEN ANSWERS
df.session_type = df.session_type.map(lambda x : &#39;WRITTEN ANSWERS&#39; if re.search(&#39;WRITTEN ANSWER&#39;,x) else x)
# Convert all variations of oral answers to ORAL ANSWERS
df.session_type = df.session_type.map(lambda x : &#39;ORAL ANSWERS&#39; if re.search(&#39;ORAL ANSWER&#39;,x) else x)
# Clean &#39;PRESIDENT&#39;S ADDRESS&amp;quot;
df.session_type = df.session_type.map(lambda x : &amp;quot;PRESIDENT&#39;S ADDRESS&amp;quot; if re.search(&amp;quot;PRESIDENT&#39;S ADDRESS&amp;quot;,x) else x)
# Clean MINISTERIAL STATEMENT
df.session_type = df.session_type.map(lambda x : &amp;quot;MINISTERIAL STATEMENTS&amp;quot; if re.search(&amp;quot;MINISTERIAL STATEMENT&amp;quot;,x) else x)
# Clean BILLS INTRODUCED
df.session_type = df.session_type.map(lambda x : &amp;quot;(BILL INTRODUCED&amp;quot; if re.search(&amp;quot;(BILL|BILL&#39;S) INTRODUCED&amp;quot;,x) else x)
# Clean MOTION
df.session_type = df.session_type.map(lambda x : &amp;quot;MOTIONS&amp;quot; if re.search(&amp;quot;MOTION&amp;quot;,x) else x)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What comes out, is much better than before:&lt;/p&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/capstone-introduction/value_counts_new_hu7922b0c99fd28886e3ab5f2bb9a21c54_130594_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;/post/capstone-introduction/value_counts_new_hu7922b0c99fd28886e3ab5f2bb9a21c54_130594_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;696&#34; height=&#34;876&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;


&lt;p&gt;As that was put out of the way, I then focused on the crux of the project - the actual speech contents. What I did not expect, though was that there were multiple ways that the speeches were not being served to the end-user the same way. It appeared that, as time went by, the content administrators changed some of their html markup.&lt;/p&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/capstone-introduction/api-headache_hud89ee042b2397da791655ba80cc0530c_328799_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;/post/capstone-introduction/api-headache_hud89ee042b2397da791655ba80cc0530c_328799_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1674&#34; height=&#34;856&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;


&lt;p&gt;That was annoying, but not difficult to address. I then created a parser to convert the html text to the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;HTML markup to plain text&lt;/li&gt;
&lt;li&gt;Divide the text into chunks (each speaker is a chunk)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The end product was below:&lt;/p&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/capstone-introduction/cleaned_hu6aace0f4b0c85c460a09ff28b09693a6_200439_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;/post/capstone-introduction/cleaned_hu6aace0f4b0c85c460a09ff28b09693a6_200439_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1256&#34; height=&#34;660&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;


&lt;p&gt;It used a combination of the &lt;code&gt;fuzzywuzzy&lt;/code&gt; library for string matching as well as a chockful of regex functions to be able to match each chunks to the speaker as well as match it to the appropriate political party or role.&lt;/p&gt;

&lt;p&gt;With the aspects of data gathering, management and cleaning out of the way, I could then proceed to the next step - text summarization, topic modeling and content based recommender.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predict the subreddit!</title>
      <link>/post/ml-subreddit/</link>
      <pubDate>Mon, 24 Jun 2019 22:15:56 +0800</pubDate>
      <guid>/post/ml-subreddit/</guid>
      <description>&lt;p&gt;Being an avid redditor (lurker) myself, I&amp;rsquo;ve always wondered how unique certain subreddits are. For the uninitiated, subreddits are equivalent to sub-topics of a message board. As an example, the r/Singapore subreddit would cover all or most discussions about Singapore and can range from the fascinating to the truly &amp;hellip; strange.&lt;/p&gt;
&lt;p&gt;On the topic of subreddits, i&amp;rsquo;m a sucker for reading into &amp;lsquo;juicy&amp;rsquo; subreddits that have posts spanning interpersonal relationships. It&amp;rsquo;s not uncommon for a random internet stranget to spill their heart out and treat other strangers as their &amp;lsquo;aunt agony&amp;rsquo;. Interestingly, there exists two similar subreddits &lt;code&gt;relationship&lt;/code&gt; and &lt;code&gt;confessions&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Wouldn&amp;rsquo;t it be cool if, on the basis of historical posts, we can develop a method to differentiate between r/relationships or r/confessions subreddit? Well, we can - and it is pretty straightforward!&lt;/p&gt;
&lt;p&gt;Using common approaches in Natural Language Processing (NLP) - an increasingly popular Data Science topic - this post will go through some of the key steps invovled in this process. More importantly, it is to share an easily generalisable methodology that can be used on other subreddits as well.&lt;/p&gt;
&lt;p&gt;A caveat, however, is that this method is constrained to text-based subreddits which only have text in their posts. Posts with images are not going to be used as it is outside of the scope of this post.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This post assumes some familiarity with Natural Language Processing. Do continue below if you are already familiar with the topic!
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#importing-our-libraries&#34;&gt;Importing our libraries&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data-acquisition&#34;&gt;Data Acquisition&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data-cleaning&#34;&gt;Data Cleaning&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#displaying-our-class-balances-after-dropping-the-rows&#34;&gt;Displaying our class balances after dropping the rows:&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#class-balance&#34;&gt;Class Balance&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#target-encoding&#34;&gt;Target Encoding&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#cleaning-function&#34;&gt;Cleaning Function&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#post-cleaning&#34;&gt;Post Cleaning&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data-exploration&#34;&gt;Data Exploration&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data-modeling&#34;&gt;Data Modeling&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#train-test-split&#34;&gt;Train Test Split&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#model-playground&#34;&gt;Model Playground&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#pipeline-for-logistic-regression-baseline&#34;&gt;Pipeline for Logistic Regression Baseline&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#model-evaluation--summary&#34;&gt;Model Evaluation &amp;amp; Summary&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;importing-our-libraries&#34;&gt;Importing our libraries&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import requests
import pandas as pd
import time
import random
import regex as re

import matplotlib.pyplot as plt
    
from nltk.corpus import stopwords # Import the stop word list
from nltk.stem import WordNetLemmatizer 
from nltk import word_tokenize

from sklearn.metrics import classification_report, roc_curve
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB
from sklearn.neighbors import  KNeighborsClassifier 

import warnings
from psaw import PushshiftAPI

# After the imports
warnings.filterwarnings(action=&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-acquisition&#34;&gt;Data Acquisition&lt;/h2&gt;
&lt;p&gt;Scrap data using the PushShiftAPI to extract more than 1000 posts per subreddit to overcome Reddit&amp;rsquo;s imposed limitation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%time

api = PushshiftAPI()
confessions = pd.DataFrame(list(api.search_submissions(subreddit=&#39;confessions&#39;,
                                         filter=[&#39;author&#39;,&#39;title&#39;,&#39;subreddit&#39;,&#39;selftext&#39;],
                                         limit=5000)))
relationships = pd.DataFrame(list(api.search_submissions(subreddit=&#39;relationships&#39;,
                                         filter=[&#39;author&#39;,&#39;title&#39;,&#39;subreddit&#39;,&#39;selftext&#39;],
                                         limit=5000)))

# store the scrapped data.
confessions.to_csv(&#39;./data/confessions.csv&#39;)
relationships.to_csv(&#39;./data/relationships.csv&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-cleaning&#34;&gt;Data Cleaning&lt;/h2&gt;
&lt;p&gt;We create a &lt;code&gt;filter_columns&lt;/code&gt; function that filters out the title, self text and subreddit name (our target)&lt;/p&gt;
&lt;p&gt;We use the &lt;code&gt;.count()&lt;/code&gt; function in our DataFrame object to understand the class balance of our dataset. Ideally, we want the number of entries of type confessions and/or relationships to be the same.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def filter_columns(df):
    columns_to_retain = [&#39;title&#39;,&#39;selftext&#39;,&#39;subreddit&#39;,&#39;author&#39;]
    return df[columns_to_retain]

df_relationships_clean = filter_columns(df_relationships)
df_conf_clean = filter_columns(df_confessions)
`
display(df_relationships_clean[&#39;title&#39;].count())
display(df_conf_clean[&#39;title&#39;].count())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is a sample of our data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_relationships_clean.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;title&lt;/th&gt;
      &lt;th&gt;selftext&lt;/th&gt;
      &lt;th&gt;subreddit&lt;/th&gt;
      &lt;th&gt;author&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Hi I&#39;m here to find my friends without anybody...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;relationships&lt;/td&gt;
      &lt;td&gt;0100100001010000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;My (M31) mind might be broken when i thi k abo...&lt;/td&gt;
      &lt;td&gt;[removed]&lt;/td&gt;
      &lt;td&gt;relationships&lt;/td&gt;
      &lt;td&gt;obviousThrowaway274&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;How do I (26m) apologize to my ex (25f) in a d...&lt;/td&gt;
      &lt;td&gt;Long story short, we broke up 4 months ago and...&lt;/td&gt;
      &lt;td&gt;relationships&lt;/td&gt;
      &lt;td&gt;Throwitallaway73734&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Do you believe it&#39;s better to solve an argumen...&lt;/td&gt;
      &lt;td&gt;[removed]&lt;/td&gt;
      &lt;td&gt;relationships&lt;/td&gt;
      &lt;td&gt;EvenKealed&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Am i broken?&lt;/td&gt;
      &lt;td&gt;[removed]&lt;/td&gt;
      &lt;td&gt;relationships&lt;/td&gt;
      &lt;td&gt;obviousThrowaway274&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_conf_clean.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;title&lt;/th&gt;
      &lt;th&gt;selftext&lt;/th&gt;
      &lt;th&gt;subreddit&lt;/th&gt;
      &lt;th&gt;author&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Thought a girl was giving me a quarter and the...&lt;/td&gt;
      &lt;td&gt;So, this was back in 2nd grade. It&#39;s a normal ...&lt;/td&gt;
      &lt;td&gt;confessions&lt;/td&gt;
      &lt;td&gt;jessthatrandomperson&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;How can I enjoy my last few days?&lt;/td&gt;
      &lt;td&gt;I am going to die very soon. \n\nI am terrifie...&lt;/td&gt;
      &lt;td&gt;confessions&lt;/td&gt;
      &lt;td&gt;throwaway948118&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;I am a narcissistic asshole and I know it and ...&lt;/td&gt;
      &lt;td&gt;I am basically just a manipulative horrible pe...&lt;/td&gt;
      &lt;td&gt;confessions&lt;/td&gt;
      &lt;td&gt;royjorbison&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;I use Reddit as an audience for my puns&lt;/td&gt;
      &lt;td&gt;I can&#39;t go ten sentences without thinking of a...&lt;/td&gt;
      &lt;td&gt;confessions&lt;/td&gt;
      &lt;td&gt;anikdylan27&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;I&#39;m sorry for being an asshole last night&lt;/td&gt;
      &lt;td&gt;To the guy I met last night, who&#39;s name escape...&lt;/td&gt;
      &lt;td&gt;confessions&lt;/td&gt;
      &lt;td&gt;roodeeMental&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Prior to this, we may wish to remove posts that have &amp;lsquo;Moderator&amp;rsquo; as an author to train our model on more &amp;lsquo;authentic&amp;rsquo; posts.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_relationships_clean.loc[:,&#39;author&#39;] = df_relationships_clean.author.map(lambda x : x.lower())
df_conf_clean.loc[:,&#39;author&#39;] = df_conf_clean.author.map(lambda x : x.lower())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_relationships_clean = df_relationships_clean[~df_relationships_clean.author.str.contains(&#39;moderator&#39;)]
df_conf_clean = df_conf_clean[~df_conf_clean.author.str.contains(&#39;moderator&#39;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_relationships_clean.isna().sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;title         0
selftext     16
subreddit     0
author        0
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_conf_clean.isna().sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;title          0
selftext     739
subreddit      0
author         0
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also observe empty selftext in both subreddits. we shall drop rows with empty selftext.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_relationships_clean = df_relationships_clean.dropna(axis=0)
df_conf_clean = df_conf_clean.dropna(axis=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ensure only posts with &lt;code&gt;selftext&lt;/code&gt; more than 10 words are selected.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_relationships_clean [&#39;selftext_len&#39;] = df_relationships_clean .selftext.map(lambda x: len(x.split()))
df_relationships_clean  = df_relationships_clean [df_relationships_clean .selftext_len &amp;gt; 10]
df_conf_clean[&#39;selftext_len&#39;] = df_conf_clean.selftext.map(lambda x: len(x.split()))
df_conf_clean = df_conf_clean[df_conf_clean.selftext_len &amp;gt; 10]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we drop our duplicates:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_relationships_clean.drop_duplicates(inplace=True)
df_conf_clean.drop_duplicates(inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;displaying-our-class-balances-after-dropping-the-rows&#34;&gt;Displaying our class balances after dropping the rows:&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;display(df_relationships_clean.count())
display(df_conf_clean.count())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For &lt;code&gt;relationships&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;title           2925
selftext        2925
subreddit       2925
author          2925
selftext_len    2925
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For &lt;code&gt;confessions&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;title           3893
selftext        3893
subreddit       3893
author          3893
selftext_len    3893
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Seeing that a value of 2900 is the limiting number, we randomly select 2900 entries from both sets.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;subset_relationships_clean = df_relationships_clean.sample(n=2900,random_state=666)
subset_conf_clean = df_conf_clean.sample(n=2900,random_state=666)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;class-balance&#34;&gt;Class Balance&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# combine both subsets into a DF
df_pre = subset_relationships_clean.append(subset_conf_clean,ignore_index=True)
df_pre.subreddit.value_counts(normalize=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;target-encoding&#34;&gt;Target Encoding&lt;/h2&gt;
&lt;p&gt;We then perform an encoding of our target : 1 corresponds to posts of type &lt;code&gt;confessions&lt;/code&gt; while 0 corresponds to posts of type &lt;code&gt;relationships&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create target class columns 0 = relationships, 1 = confessions - encoding

df_pre[&#39;label&#39;] = df_pre.subreddit.map({&#39;relationships&#39;:0,&#39;confessions&#39;:1}).astype(&#39;int&#39;)
df_pre.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;title&lt;/th&gt;
      &lt;th&gt;selftext&lt;/th&gt;
      &lt;th&gt;subreddit&lt;/th&gt;
      &lt;th&gt;author&lt;/th&gt;
      &lt;th&gt;selftext_len&lt;/th&gt;
      &lt;th&gt;label&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;I (F18) am questioning the intentions of a ran...&lt;/td&gt;
      &lt;td&gt;My lovely (quite attractive) new boyfriend (M1...&lt;/td&gt;
      &lt;td&gt;relationships&lt;/td&gt;
      &lt;td&gt;boterbabbelaartje&lt;/td&gt;
      &lt;td&gt;334&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Jealousy&lt;/td&gt;
      &lt;td&gt;My boyfriend(29m) and I(30f) have been togethe...&lt;/td&gt;
      &lt;td&gt;relationships&lt;/td&gt;
      &lt;td&gt;taramarie87&lt;/td&gt;
      &lt;td&gt;103&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;I [28F] wants sex all the time. I&#39;ve made this...&lt;/td&gt;
      &lt;td&gt;Lately, I&#39;ve been wanting more sex. To have se...&lt;/td&gt;
      &lt;td&gt;relationships&lt;/td&gt;
      &lt;td&gt;missionblueberry&lt;/td&gt;
      &lt;td&gt;236&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;I [32m] am having issues with jealousy with my...&lt;/td&gt;
      &lt;td&gt;Hooo boy. Here we go. \n\nMy wife and I have b...&lt;/td&gt;
      &lt;td&gt;relationships&lt;/td&gt;
      &lt;td&gt;dcsrm&lt;/td&gt;
      &lt;td&gt;438&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Is my girlfriend into wedgies?&lt;/td&gt;
      &lt;td&gt;My girlfriend (F 21yrs old) and I (M 32yrs old...&lt;/td&gt;
      &lt;td&gt;relationships&lt;/td&gt;
      &lt;td&gt;davidsardinas36&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;cleaning-function&#34;&gt;Cleaning Function&lt;/h3&gt;
&lt;p&gt;Ensure formatting of text by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Converting all to lower cases&lt;/li&gt;
&lt;li&gt;removing groups of words in parentheses&lt;/li&gt;
&lt;li&gt;remove line breaks&lt;/li&gt;
&lt;li&gt;removing special characters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We encapsulate this cleaning into the function &lt;code&gt;clean_text&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# convert the stop words to a set.
stops = set(stopwords.words(&#39;english&#39;))

def clean_text(text):
    #01 convert titles, selftext into lowercase
    lower_text = text.lower()
    #02 remove brackets and parenthesis from the title and selftext.
    no_br_paret_text = re.sub(r&#39;\(.+?\)|\[.+?\]&#39;,&#39; &#39;,str(lower_text))
    #03 remove special characters
    removed_special = re.sub(r&#39;[^0-9a-zA-Z ]+&#39;,&#39; &#39;,str(no_br_paret_text))
    #04 remove xamp200b
    remove_xamp200b = re.sub(r&#39;ampx200b&#39;,&#39; &#39;,str(removed_special))
    #05 remove digits
    result = re.sub(r&#39;\d+&#39;, &#39;&#39;, remove_xamp200b).split()
    #06 split into individual words
    meaningful_words = [w for w in result if not w in stops]
    
    #07 Join the words back into one string separated by space, 
    # and return the result.
    return(&amp;quot; &amp;quot;.join(meaningful_words))

df[[&#39;title&#39;,&#39;selftext&#39;]] = df_pre[[&#39;title&#39;,&#39;selftext&#39;]].applymap(clean_text)
df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A sample of our pre-cleaned data:&lt;/p&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;title&lt;/th&gt;
      &lt;th&gt;selftext&lt;/th&gt;
      &lt;th&gt;subreddit&lt;/th&gt;
      &lt;th&gt;author&lt;/th&gt;
      &lt;th&gt;selftext_len&lt;/th&gt;
      &lt;th&gt;label&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;questioning intentions random new girl asked b...&lt;/td&gt;
      &lt;td&gt;lovely new boyfriend told girl cig outside sch...&lt;/td&gt;
      &lt;td&gt;relationships&lt;/td&gt;
      &lt;td&gt;boterbabbelaartje&lt;/td&gt;
      &lt;td&gt;334&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;jealousy&lt;/td&gt;
      &lt;td&gt;boyfriend together almost years two beautiful ...&lt;/td&gt;
      &lt;td&gt;relationships&lt;/td&gt;
      &lt;td&gt;taramarie87&lt;/td&gt;
      &lt;td&gt;103&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;wants sex time made known whose&lt;/td&gt;
      &lt;td&gt;lately wanting sex sex time bit back story rel...&lt;/td&gt;
      &lt;td&gt;relationships&lt;/td&gt;
      &lt;td&gt;missionblueberry&lt;/td&gt;
      &lt;td&gt;236&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;issues jealousy wife stage acting&lt;/td&gt;
      &lt;td&gt;hooo boy go wife married years coming ups down...&lt;/td&gt;
      &lt;td&gt;relationships&lt;/td&gt;
      &lt;td&gt;dcsrm&lt;/td&gt;
      &lt;td&gt;438&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;girlfriend wedgies&lt;/td&gt;
      &lt;td&gt;girlfriend together months used phone look som...&lt;/td&gt;
      &lt;td&gt;relationships&lt;/td&gt;
      &lt;td&gt;davidsardinas36&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;post-cleaning&#34;&gt;Post Cleaning&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.DataFrame(data=zip(df_pre[&#39;selftext&#39;],df[&#39;selftext&#39;]),columns=[&#39;pre&#39;,&#39;post&#39;]).head(5)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;pre&lt;/th&gt;
      &lt;th&gt;post&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;My lovely (quite attractive) new boyfriend (M1...&lt;/td&gt;
      &lt;td&gt;lovely new boyfriend told girl cig outside sch...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;My boyfriend(29m) and I(30f) have been togethe...&lt;/td&gt;
      &lt;td&gt;boyfriend together almost years two beautiful ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Lately, I&#39;ve been wanting more sex. To have se...&lt;/td&gt;
      &lt;td&gt;lately wanting sex sex time bit back story rel...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Hooo boy. Here we go. \n\nMy wife and I have b...&lt;/td&gt;
      &lt;td&gt;hooo boy go wife married years coming ups down...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;My girlfriend (F 21yrs old) and I (M 32yrs old...&lt;/td&gt;
      &lt;td&gt;girlfriend together months used phone look som...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;data-exploration&#34;&gt;Data Exploration&lt;/h2&gt;
&lt;p&gt;Split title and self text into two classifiers where the output of title_classifier and self_text classifier would provide indication of which subreddit the posts belong to.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#split titles, and self text into seperate df

df_title = df[[&#39;title&#39;,&#39;label&#39;]]
df_selftext = df[[&#39;selftext&#39;,&#39;label&#39;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_freq_words(sparse_counts, columns):
    # X_all is a sparse matrix, so sum() returns a &#39;matrix&#39; datatype ...
    #   which we then convert into a 1-D ndarray for sorting
    word_counts = np.asarray(sparse_counts.sum(axis=0)).reshape(-1)

    # argsort() returns smallest first, so we reverse the result
    largest_count_indices = word_counts.argsort()[::-1]

    # pretty-print the results! Remember to always ask whether they make sense ...
    freq_words = pd.Series(word_counts[largest_count_indices], 
                           index=columns[largest_count_indices])

    return freq_words
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Let&#39;s use the CountVectorizer to count words for us for each class

# create mask

X_1 = df_selftext[df_selftext[&#39;label&#39;] == 1]
X_0 = df_selftext[df_selftext[&#39;label&#39;] == 0]

cvt      =  CountVectorizer(ngram_range=(1,1),stop_words=&#39;english&#39;)
X_1_all    =  cvt.fit_transform(X_1[&#39;selftext&#39;])
X_0_all    =  cvt.fit_transform(X_0[&#39;selftext&#39;])
columns_1  =  np.array(cvt.get_feature_names())          # ndarray (for indexing below)
columns_0  =  np.array(cvt.get_feature_names())    
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;freq_words_1 = get_freq_words(X_1_all, columns_1)
freq_words_0 = get_freq_words(X_0_all, columns_0)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&#39;Confessions:&#39;)
display(freq_words_1[:10])
print(&amp;quot;\n&amp;quot;)
print(&#39;Relationships:&#39;)
display(freq_words_0[:10])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are some key words that appear in the Confessions data set - which would mean that the words &lt;code&gt;landlord&lt;/code&gt;,&lt;code&gt;jeopardise&lt;/code&gt;, etc. would make it more than likely for the post to be of &lt;code&gt;confessions&lt;/code&gt; class.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Confessions:

landlord       3063
msg            2325
jeopardise     1979
teachings      1721
eyes           1674
pur            1506
user           1438
overworking    1405
generic        1133
lacking        1109
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Same for &lt;code&gt;relationships&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Relationships:

like            6690
time            4761
know            4694
want            4630
really          4235
feel            4000
relationship    3744
said            3245
things          3070
told            2999
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-modeling&#34;&gt;Data Modeling&lt;/h2&gt;
&lt;h3 id=&#34;train-test-split&#34;&gt;Train Test Split&lt;/h3&gt;
&lt;p&gt;Here, we start with our model development. Before that, we perform a train/test split to ensure that we can validate our model performance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_text = df_selftext[&#39;selftext&#39;]
y_text = df_selftext[&#39;label&#39;]

X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(X_text,y_text,stratify=y_text) 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;model-playground&#34;&gt;Model Playground&lt;/h3&gt;
&lt;p&gt;We create the class &lt;code&gt;LemmaTokenizer&lt;/code&gt; to do both lemmatize each word of each entry. I.e. given a list of words, we 
&lt;a href=&#34;https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lemmatize&lt;/a&gt;
 each word.&lt;/p&gt;
&lt;p&gt;Firstly, we try the Naive Bayes model - MultinomialNB as there are multiple nominal features in the form of the various tokens.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;classifiers = []
vectorizers = [(&#39;cvec&#39;, CountVectorizer(stop_words=&#39;english&#39;,tokenizer=LemmaTokenizer())),
              (&#39;tfvec&#39;, TfidfVectorizer(stop_words=&#39;english&#39;,tokenizer=LemmaTokenizer()))]

for vectorizer in vectorizers:
    bayes_pipe = Pipeline([
            (vectorizer),
            (&#39;mnb&#39;, MultinomialNB())
        ])
    scores = cross_val_score(bayes_pipe, X_text_train, y_text_train,cv=5,verbose=1)
    b = bayes_pipe.fit(X_text_train, y_text_train)
    y_pred = b.predict(X_text_test)
    print(classification_report(y_text_test, y_pred, target_names=[&#39;class 0&#39;,&#39;class 1&#39;]))
    print(&#39;Cross val score for mnb classifier using {} vectorizer is {}&#39;.format(vectorizer[0],scores))
    print(&#39;Accuracy score for mnb classifier using {} vectorizer is {}&#39;.format(vectorizer[0],bayes_pipe.score(X_text_test, y_text_test)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              precision    recall  f1-score   support

     class 0       0.77      0.95      0.85       725
     class 1       0.93      0.71      0.81       725

    accuracy                           0.83      1450
   macro avg       0.85      0.83      0.83      1450
weighted avg       0.85      0.83      0.83      1450

Cross val score for mnb classifier using cvec vectorizer is [0.80114943 0.80689655 0.86321839 0.81724138 0.79770115]
Accuracy score for mnb classifier using cvec vectorizer is 0.8289655172413793


              precision    recall  f1-score   support

     class 0       0.65      0.99      0.78       725
     class 1       0.98      0.46      0.63       725

    accuracy                           0.73      1450
   macro avg       0.82      0.73      0.71      1450
weighted avg       0.82      0.73      0.71      1450

Cross val score for mnb classifier using tfvec vectorizer is [0.71149425 0.70689655 0.74712644 0.73448276 0.7045977 ]
Accuracy score for mnb classifier using tfvec vectorizer is 0.7282758620689656
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus the recall scores for multinomial NB with countvectorizer seems to provide higher recall when compared to the tfidf vectorizer.&lt;/p&gt;
&lt;p&gt;In the meantime, we create a function to encapsulate our evaluation process such that it returns only the false positive rate and true positive rate with a &lt;code&gt;sklearn&lt;/code&gt; processing pipeline.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# store predicted_proba scores for later evaluation under ROC curve
def generate_roc(pipeline):

    b = pipeline.fit(X_text_train, y_text_train)
    print(f&amp;quot;Train Score:{round(b.score(X_text_train, y_text_train),2)} / Test Score {round(b.score(X_text_test, y_text_test),2)}&amp;quot;)
    fpr, tpr, _ = roc_curve(y_text_test, b.predict_proba(X_text_test)[:,1],pos_label=1)
    
    return [fpr,tpr]

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Rewriting the CountVectorizer Naive Bayes and TF-IDF Naive Bayes into their respective pipelines:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cv_bayes_pipe = Pipeline([
            (vectorizers[0]),
            (&#39;mnb&#39;, MultinomialNB())
        ])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tfidf_bayes_pipe = Pipeline([
            (vectorizers[1]),
            (&#39;mnb&#39;, MultinomialNB())
        ])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;pipeline-for-logistic-regression-baseline&#34;&gt;Pipeline for Logistic Regression Baseline&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipe = Pipeline([
    (&#39;cvec&#39;, CountVectorizer(stop_words=&#39;english&#39;,tokenizer=LemmaTokenizer())),
    (&#39;lr&#39;, LogisticRegression(solver=&#39;saga&#39;,max_iter=300))
])
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;obtain-hyperparameters-for-our-vectorizer-and-logistic-regressor&#34;&gt;Obtain hyperparameters for our vectorizer and logistic regressor.&lt;/h4&gt;
&lt;p&gt;We can use a grid search to find the optimal hyperparameters for our pipelines:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipe_params = {
    &#39;cvec__max_features&#39;: [2500, 3000, 3500],
    &#39;cvec__ngram_range&#39;: [(1,1), (1,2)],
    &#39;lr__penalty&#39; : [&#39;elasticnet&#39;],
    &#39;lr__C&#39; : np.arange(0.1,1,0.1),
    &#39;lr__l1_ratio&#39; : np.arange(0,1.1,0.2)
}

gs = GridSearchCV(pipe, param_grid=pipe_params, cv=5,verbose=1,n_jobs=-1)
gs.fit(X_text_train, y_text_train)
print(gs.best_score_)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.9154022988505747
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gs.best_params_
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;cvec__max_features&#39;: 2500,
 &#39;cvec__ngram_range&#39;: (1, 1),
 &#39;lr__C&#39;: 0.1,
 &#39;lr__l1_ratio&#39;: 1.0,
 &#39;lr__penalty&#39;: &#39;elasticnet&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The best score for our logistic regression pipeline:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gs.best_estimator_.score(X_text_test,y_text_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.9186206896551724
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the hyperparameters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# try model on title
optimal_pipe = Pipeline([
            (&#39;cvec&#39;, CountVectorizer(tokenizer=LemmaTokenizer(),max_features=2500,ngram_range=(1,1))),
            (&#39;lr&#39;, LogisticRegression(solver=&#39;saga&#39;,max_iter=300,C=0.1,l1_ratio=1.0,penalty=&#39;elasticnet&#39;))
        ])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_title = df_title[&#39;title&#39;]
y_title = df_title[&#39;label&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;optimal_pipe.fit(X_text_train, y_text_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We try the model on our &lt;code&gt;title&lt;/code&gt; dataset to obtain the accuracy of the model to classify the subreddit from titles alone.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y_logr_pred = optimal_pipe.predict(X_text_test)
print(classification_report(y_text_test, y_logr_pred, target_names=[&#39;class 0&#39;,&#39;class 1&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              precision    recall  f1-score   support

     class 0       0.55      1.00      0.71       725
     class 1       0.99      0.18      0.31       725

    accuracy                           0.59      1450
   macro avg       0.77      0.59      0.51      1450
weighted avg       0.77      0.59      0.51      1450
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we explore the use tfidfvectorizer instead of countvectorizer to account for document similarity&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tfidf_pipe = Pipeline([
    (&#39;tfvec&#39;, TfidfVectorizer(stop_words=&#39;english&#39;,tokenizer=LemmaTokenizer())),
    (&#39;lr&#39;, LogisticRegression(solver=&#39;saga&#39;,max_iter=300))
])

tfidf_params = {
    &#39;tfvec__max_features&#39;: [2500, 3000, 3500],
    &#39;tfvec__ngram_range&#39;: [(1,1), (1,2)],
    &#39;lr__penalty&#39; : [&#39;elasticnet&#39;],
    &#39;lr__C&#39; : np.arange(0.1,1,0.1),
    &#39;lr__l1_ratio&#39; : np.arange(0,1.1,0.2)
}

gs = GridSearchCV(tfidf_pipe, param_grid=tfidf_params, cv=3,verbose=1,n_jobs=-1)
gs.fit(X_text_train, y_text_train)
print(gs.best_score_)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.9183908045977012
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It seems that tfidf vectorizer performs best with the logistic regression model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tfidf_best_pipe = Pipeline([
    (&#39;tfvec&#39;, TfidfVectorizer(max_features=3500,ngram_range=(1,1),stop_words=&#39;english&#39;,tokenizer=LemmaTokenizer())),
    (&#39;lr&#39;, LogisticRegression(solver=&#39;saga&#39;,max_iter=300,C=0.9,l1_ratio=1.0,penalty=&#39;elasticnet&#39;))
])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# test model against test text data and rest of titles
y_text_tfidf_pred = gs.best_estimator_.predict(X_text_test)
y_title_tfidf_pred = gs.best_estimator_.predict(X_title)
print(&amp;quot;Text Report (results based on test data) \n&amp;quot; + 
      classification_report(y_text_test, y_text_tfidf_pred, target_names=[&#39;class 0&#39;,&#39;class 1&#39;]))
print(&amp;quot;Titles (all titles) Report \n&amp;quot; + 
      classification_report(y_title, y_title_tfidf_pred, target_names=[&#39;class 0&#39;,&#39;class 1&#39;]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text Report (results based on test data) 
              precision    recall  f1-score   support

     class 0       0.93      0.91      0.92       725
     class 1       0.91      0.94      0.92       725

    accuracy                           0.92      1450
   macro avg       0.92      0.92      0.92      1450
weighted avg       0.92      0.92      0.92      1450

Titles (all titles) Report 
              precision    recall  f1-score   support

     class 0       0.92      0.17      0.29      2900
     class 1       0.54      0.98      0.70      2900

    accuracy                           0.58      5800
   macro avg       0.73      0.58      0.49      5800
weighted avg       0.73      0.58      0.49      5800
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While the optimised model with tfidf vectorizer performs remarkably well with high precision and recall, when used with the &lt;code&gt;titles&lt;/code&gt; dataset, we can see that that it is somewhat overfit, unable to classify the titles correctly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# look at sample predictions

pd.DataFrame(data=zip(X_text_test,y_text_test,y_text_tfidf_pred),columns=[&#39;text&#39;,&#39;actual&#39;,&#39;predicted&#39;]).head(5)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
      &lt;th&gt;actual&lt;/th&gt;
      &lt;th&gt;predicted&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;title says watched porn since got nasty furry ...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;understand bad bad read lemon fanfic main vide...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;lovely quite attractive boyfriend met girl wee...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;dated briefly three months never turned someth...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;background dating almost years good many fight...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;model-evaluation--summary&#34;&gt;Model Evaluation &amp;amp; Summary&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cv_log_roc = generate_roc(optimal_pipe)
tfidf_log_roc = generate_roc(tfidf_best_pipe)
cv_nb_roc = generate_roc(cv_bayes_pipe)
tfidf_nb_roc = generate_roc(tfidf_bayes_pipe)
tfidf_knn_roc = generate_roc(knn_best_pipe)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train Score:0.59 / Test Score 0.59
Train Score:0.93 / Test Score 0.92
Train Score:0.89 / Test Score 0.83
Train Score:0.81 / Test Score 0.73
Train Score:1.0 / Test Score 0.81
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Evaluation

roc_data ={
    &#39;cv_nb&#39; : cv_nb_roc,
    &#39;tfidf_nb_roc&#39; : tfidf_nb_roc,
    &#39;cv_log_roc&#39; : cv_log_roc,
    &#39;tfidf_log_roc&#39; : tfidf_log_roc,
    &#39;tfidf_knn_roc&#39; : tfidf_knn_roc
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#### Plot figure
plt.figure(1,figsize=(10,5))
plt.plot([0, 1], [0, 1], &#39;k--&#39;)
for key,roc in roc_data.items():
    plt.plot(roc[0], roc[1], label=key)
plt.xlabel(&#39;Sensitivity&#39;)
plt.ylabel(&#39;1 - Specificity&#39;)
plt.title(&#39;ROC curve&#39;)
plt.legend(loc=&#39;best&#39;)

plt.savefig(&amp;quot;./img/roc_curve.png&amp;quot;,dpi=300)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;output_89_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;crossvectorizer&lt;/code&gt; + &lt;code&gt;logistic regression&lt;/code&gt; model seems to perform similar to the &lt;code&gt;tfidf&lt;/code&gt; vectorizer and &lt;code&gt;logistic regression&lt;/code&gt; model. When looking at the accuracy score of all the models, the tfidf+ logistic regression model performs the best with an accuracy of 92% in terms of predicting if the selftext is either an r/confessions or r/relationships post.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
